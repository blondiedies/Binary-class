{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:44:06.244656Z",
     "start_time": "2024-08-19T14:44:04.568459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose\n",
    "import random\n"
   ],
   "id": "2a122adee9a39c20",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:44:07.015209Z",
     "start_time": "2024-08-19T14:44:07.010180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rnn = nn.LSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ],
   "id": "8dfb7e66099184ef",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:44:08.734843Z",
     "start_time": "2024-08-19T14:44:08.221808Z"
    }
   },
   "cell_type": "code",
   "source": "sample0, sr0 = librosa.load('Keystroke-Datasets/MBPWavs/0.wav')",
   "id": "bfacb9ffeaf90c7e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:44:15.212714Z",
     "start_time": "2024-08-19T14:44:15.210322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# waveform function for me to not bang my keyboard\n",
    "def disp_waveform(signal, sr=None, color='blue'):\n",
    "    plt.figure(figsize=(7,2))\n",
    "    return librosa.display.waveshow(signal, sr=sr, color=color)"
   ],
   "id": "9ad2373f64ed7deb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-19T14:44:18.812029Z",
     "start_time": "2024-08-19T14:44:18.807951Z"
    }
   },
   "source": [
    "def isolator(signal, sample_rate, n_fft, hop_length, before, after, threshold, show=False):\n",
    "    strokes = []\n",
    "    # -- signal'\n",
    "    if show:\n",
    "        disp_waveform(signal, sr=sample_rate)\n",
    "    fft = librosa.stft(signal, n_fft=n_fft, hop_length=hop_length)\n",
    "    energy = np.abs(np.sum(fft, axis=0)).astype(float)\n",
    "    # norm = np.linalg.norm(energy)\n",
    "    # energy = energy/norm\n",
    "    # -- energy'\n",
    "    if show:\n",
    "        disp_waveform(energy)\n",
    "    threshed = energy > threshold\n",
    "    # -- peaks'\n",
    "    if show:\n",
    "        disp_waveform(threshed.astype(float))\n",
    "    peaks = np.where(threshed == True)[0]\n",
    "    peak_count = len(peaks)\n",
    "    prev_end = sample_rate*0.1*(-1)\n",
    "    # '-- isolating keystrokes'\n",
    "    for i in range(peak_count):\n",
    "        this_peak = peaks[i]\n",
    "        timestamp = (this_peak*hop_length) + n_fft//2\n",
    "        if timestamp > prev_end + (0.1*sample_rate):\n",
    "            keystroke = signal[timestamp-before:timestamp+after]\n",
    "            # strokes.append(torch.tensor(keystroke)[None, :])\n",
    "            # keystroke = transform(keystroke)\n",
    "            strokes.append(keystroke)\n",
    "            if show:\n",
    "                disp_waveform(keystroke, sr=sample_rate)\n",
    "            prev_end = timestamp+after\n",
    "    return strokes"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:44:21.141748Z",
     "start_time": "2024-08-19T14:44:21.107854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Constants we actually need for the task\n",
    "MBP_AUDIO_DIR = '/Users/jorgeleon/Keystroke-Datasets/MBPWavs/'\n",
    "keys_s = '1234567890QWERTYUIOPASDFGHJKLZXCVBNM'\n",
    "# keys_s = '12'\n",
    "labels = list(keys_s)\n",
    "keys = [k + '.wav' for k in labels]\n",
    "data_dict = {'Key':[], 'File':[]}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ],
   "id": "3c33893a14fb819c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:44:24.487741Z",
     "start_time": "2024-08-19T14:44:24.484344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataset(n_fft, hop_length, before, after):\n",
    "    for i, File in enumerate(keys):\n",
    "        loc = MBP_AUDIO_DIR + File\n",
    "        samples, sr = librosa.load(loc)\n",
    "        prom = 0.06\n",
    "        step = 0.005\n",
    "        strokes = isolator(samples, sr, n_fft, hop_length, before, after, prom, False )\n",
    "        print(f'File {File} length: {len(strokes)}')\n",
    "        label = [labels[i]]*len(strokes)\n",
    "        data_dict['Key'] += label\n",
    "        data_dict['File'] += strokes\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for l in df['Key']:\n",
    "        if not l in mapper:\n",
    "            mapper[l] = counter\n",
    "            counter += 1\n",
    "    df.replace({'Key': mapper}, inplace = True)\n",
    "\n",
    "    return df"
   ],
   "id": "2fee322473e2bfe1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:44:28.862537Z",
     "start_time": "2024-08-19T14:44:28.182269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key in keys_s:\n",
    "    sample, sr = librosa.load(f'Keystroke-Datasets/MBPWavs/{key}.wav')\n",
    "    print(sr)\n",
    "    print(len(isolator(sample, sr, 1024, 225, 2400, 12000, 0.06)), end=' ')\n",
    "    "
   ],
   "id": "953e2f0556453dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "26 22050\n",
      "25 22050\n",
      "25 22050\n",
      "27 22050\n",
      "26 22050\n",
      "27 22050\n",
      "28 22050\n",
      "28 22050\n",
      "25 22050\n",
      "26 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "27 22050\n",
      "26 22050\n",
      "25 22050\n",
      "27 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "26 22050\n",
      "26 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "26 "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:44:31.984566Z",
     "start_time": "2024-08-19T14:44:31.314360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_fft = 1024\n",
    "hop_length = 225\n",
    "before = 2400\n",
    "after = 12000\n",
    "mbp_dataset = create_dataset(n_fft, hop_length, before, after)\n",
    "mbp_dataset"
   ],
   "id": "f2dd6f724446d4fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1.wav length: 25\n",
      "File 2.wav length: 25\n",
      "File 3.wav length: 25\n",
      "File 4.wav length: 25\n",
      "File 5.wav length: 25\n",
      "File 6.wav length: 26\n",
      "File 7.wav length: 25\n",
      "File 8.wav length: 25\n",
      "File 9.wav length: 27\n",
      "File 0.wav length: 26\n",
      "File Q.wav length: 27\n",
      "File W.wav length: 28\n",
      "File E.wav length: 28\n",
      "File R.wav length: 25\n",
      "File T.wav length: 26\n",
      "File Y.wav length: 25\n",
      "File U.wav length: 25\n",
      "File I.wav length: 25\n",
      "File O.wav length: 25\n",
      "File P.wav length: 25\n",
      "File A.wav length: 25\n",
      "File S.wav length: 25\n",
      "File D.wav length: 25\n",
      "File F.wav length: 27\n",
      "File G.wav length: 26\n",
      "File H.wav length: 25\n",
      "File J.wav length: 27\n",
      "File K.wav length: 25\n",
      "File L.wav length: 25\n",
      "File Z.wav length: 25\n",
      "File X.wav length: 26\n",
      "File C.wav length: 26\n",
      "File V.wav length: 25\n",
      "File B.wav length: 25\n",
      "File N.wav length: 25\n",
      "File M.wav length: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/8bpx1vc91zq76n48xvq00vkr0000gn/T/ipykernel_38603/2996179166.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace({'Key': mapper}, inplace = True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     Key                                               File\n",
       "0      0  [-0.00017975704, -0.00012727435, -9.371067e-05...\n",
       "1      0  [0.0004975861, 0.00049031794, 0.00055128767, 0...\n",
       "2      0  [0.0003178973, 0.00034715654, 0.00037197635, 0...\n",
       "3      0  [0.0026817801, 0.0026667325, 0.0026979204, 0.0...\n",
       "4      0  [0.006475482, 0.006330982, 0.0053669773, 0.003...\n",
       "..   ...                                                ...\n",
       "916   35  [-0.250816, -0.25290227, -0.2548398, -0.256657...\n",
       "917   35  [0.13746458, 0.13331993, 0.12892574, 0.1242145...\n",
       "918   35  [0.0017171799, 0.0016756048, 0.0016776036, 0.0...\n",
       "919   35  [-0.00014814897, -0.00018149195, -0.0002237717...\n",
       "920   35  [0.0002516488, 0.00018340952, 0.00015876105, 0...\n",
       "\n",
       "[921 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.00017975704, -0.00012727435, -9.371067e-05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.0004975861, 0.00049031794, 0.00055128767, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.0003178973, 0.00034715654, 0.00037197635, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.0026817801, 0.0026667325, 0.0026979204, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.006475482, 0.006330982, 0.0053669773, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>35</td>\n",
       "      <td>[-0.250816, -0.25290227, -0.2548398, -0.256657...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>35</td>\n",
       "      <td>[0.13746458, 0.13331993, 0.12892574, 0.1242145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>35</td>\n",
       "      <td>[0.0017171799, 0.0016756048, 0.0016776036, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>35</td>\n",
       "      <td>[-0.00014814897, -0.00018149195, -0.0002237717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>35</td>\n",
       "      <td>[0.0002516488, 0.00018340952, 0.00015876105, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>921 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:44:34.945909Z",
     "start_time": "2024-08-19T14:44:34.914709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio_samples = mbp_dataset['File'].values.tolist()\n",
    "labels = mbp_dataset['Key'].values.tolist()\n",
    "\n",
    "audioDataset = np.array(audio_samples, dtype = object)\n",
    "print(audio_samples[0].shape)\n",
    "mfcc = librosa.feature.mfcc(y=audio_samples[0], sr=44100) # shape: (n_mfcc, t)\n",
    "print(mfcc.shape)\n",
    "# labels = np.array(labels)"
   ],
   "id": "c89e46a8b42021c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14400,)\n",
      "(20, 29)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:44:54.724008Z",
     "start_time": "2024-08-19T14:44:54.721529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TimeShifting():\n",
    "    def __call__(self, samples):\n",
    "#       samples_shape = samples.shape\n",
    "        samples = samples.flatten()\n",
    "        \n",
    "        shift = int(len(samples) * 0.4) #Max shift (0.4)\n",
    "        random_shift = random.randint(0, shift) #Random number between 0 and 0.4*len(samples)\n",
    "        data_roll = np.roll(samples, random_shift)\n",
    "        return data_roll"
   ],
   "id": "39d2cd83eaab0466",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:44:57.031292Z",
     "start_time": "2024-08-19T14:44:57.028857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def time_shift(samples):\n",
    "    samples = samples.flatten()\n",
    "    shift = int(len(samples) * 0.4) #Max shift (0.4)\n",
    "    random_shift = random.randint(0, shift) #Random number between 0 and 0.4*len(samples)\n",
    "    data_roll = np.roll(samples, random_shift)\n",
    "    return data_roll"
   ],
   "id": "ac3598b8c7bcd285",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:44:58.794443Z",
     "start_time": "2024-08-19T14:44:58.768300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "class ToMelSpectrogram:\n",
    "    def __init__(self, audio_length=14400):\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        if len(samples) > self.audio_length:\n",
    "            samples = samples[:self.audio_length]\n",
    "        elif len(samples) < self.audio_length:\n",
    "            samples = np.pad(samples, (0, self.audio_length - len(samples)), mode='constant')\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=samples, sr=44100, n_mels=64, n_fft=1024, hop_length=225)\n",
    "        mel_spec_resized = resize(mel_spec, (64, 64), anti_aliasing=True)\n",
    "        mel_spec_resized = np.expand_dims(mel_spec_resized, axis=0)\n",
    "        return torch.tensor(mel_spec_resized)\n",
    "\n",
    "\n",
    "class ToMelSpectrogramMfcc:\n",
    "    def __init__(self, audio_length=14400):\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        if len(samples) > self.audio_length:\n",
    "            samples = samples[:self.audio_length]\n",
    "        elif len(samples) < self.audio_length:\n",
    "            samples = np.pad(samples, (0, self.audio_length - len(samples)), mode='constant')\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=samples, sr=44100, n_mels=64, n_fft=n_fft, hop_length=hop_length)\n",
    "        mel_spec = librosa.feature.mfcc(S=librosa.power_to_db(mel_spec))\n",
    "        mel_spec_resized = resize(mel_spec, (64, 64), anti_aliasing=True)\n",
    "        mel_spec_resized = np.expand_dims(mel_spec_resized, axis=0)\n",
    "\n",
    "        return torch.tensor(mel_spec_resized)\n",
    "\n",
    "\n",
    "class ToMfcc:\n",
    "    def __init__(self, audio_length=14400):\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        if len(samples) > self.audio_length:\n",
    "            samples = samples[:self.audio_length]\n",
    "        elif len(samples) < self.audio_length:\n",
    "            samples = np.pad(samples, (0, self.audio_length - len(samples)), mode='constant')\n",
    "        \n",
    "        mfcc_spec = librosa.feature.mfcc(y=samples, sr=44100)\n",
    "        mfcc_spec = np.transpose(mfcc_spec)\n",
    "        return torch.tensor(mfcc_spec)\n"
   ],
   "id": "4a429af86b3206df",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:45:00.226118Z",
     "start_time": "2024-08-19T14:45:00.223294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = Compose([ToMelSpectrogram()])\n",
    "transform_mfcc = Compose([ToMfcc()])"
   ],
   "id": "f019fc2bc0e25204",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:45:01.387587Z",
     "start_time": "2024-08-19T14:45:01.369945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio_samples_new = audio_samples.copy() # audio samples CNN\n",
    "\n",
    "for i, sample in enumerate(audio_samples):\n",
    "    audio_samples_new.append(time_shift(sample))\n",
    "    labels.append(labels[i])\n",
    "    \n",
    "# convert labels to a numpy array\n",
    "labels = np.array(labels)\n",
    "print(len(audio_samples_new))\n",
    "print(len(labels))"
   ],
   "id": "44b3b3ca9f37f4c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842\n",
      "1842\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:45:11.415831Z",
     "start_time": "2024-08-19T14:45:02.394011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audioDatasetFin, audioDatasetMfcc = [], []\n",
    "\n",
    "for i in range(len(audio_samples_new)):\n",
    "    transformed_sample = transform(audio_samples_new[i])\n",
    "    transformed_mfcc = transform_mfcc(audio_samples_new[i])\n",
    "    audioDatasetFin.append((transformed_sample, labels[i]))\n",
    "    audioDatasetMfcc.append((transformed_sample, transformed_mfcc, labels[i]))"
   ],
   "id": "be9e929216f37f07",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:45:11.436048Z",
     "start_time": "2024-08-19T14:45:11.421791Z"
    }
   },
   "cell_type": "code",
   "source": "len(audioDatasetFin)",
   "id": "94d1788d46fe597f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1842"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T14:45:11.452135Z",
     "start_time": "2024-08-19T14:45:11.441024Z"
    }
   },
   "cell_type": "code",
   "source": "audioDatasetMfcc[0][0].shape",
   "id": "cf735153a104afec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:33:02.384601Z",
     "start_time": "2024-08-19T15:33:02.377497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "class MfccLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.2, num_classes=36):\n",
    "        super(MfccLSTM, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.LazyLinear(64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        \n",
    "        # self.lstm = nn.Sequential(\n",
    "        #     nn.LSTM(input_size, hidden_size, batch_first=True),\n",
    "        #     nn.Dropout(dropout),\n",
    "        #     nn.LSTM(hidden_size, hidden_size, batch_first=True),\n",
    "        #     nn.Dropout(dropout),\n",
    "        #     nn.LazyLinear(256),\n",
    "        #     nn.Linear(256, output_size)\n",
    "        # )\n",
    "        \n",
    "        self.fc3 = nn.LazyLinear(128)\n",
    "        self.final_lstm = nn.LSTM(1, 64, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.LazyLinear(num_classes)\n",
    "    \n",
    "    def forward(self, image_input, sequence_input):\n",
    "        # must return shape (batch_size, num_classes) \n",
    "        # batch_size: right now is 16\n",
    "        # num_classes: right now is 36\n",
    "        x1 = self.conv(image_input)\n",
    "        # print(f'output of convolutional part: {x1.shape[1:]}')\n",
    "        # print(\"after conv layer1\")\n",
    "        # print('got here x1')\n",
    "        # must return shape (batch_size, output_size)\n",
    "        # output_size: should be 36\n",
    "        # # # print(sequence_input.shape)\n",
    "        out1, _ = self.lstm(sequence_input)\n",
    "        out1_dp = self.dropout(out1)\n",
    "        # print(f'output of first lstm: {out1_dp.shape[1:]}')\n",
    "        out2, _ = self.lstm2(out1_dp[:, -1, :])\n",
    "        out2_dp = self.dropout(out2)\n",
    "        # print(f'output of second lstm: {out2_dp.shape[1:]}')\n",
    "        x2 = self.fc2(self.fc1(out2_dp))\n",
    "        x3 = torch.cat((x1, x2), 1)\n",
    "        # print(f'output of concatenation: {x3.shape[1:]}')\n",
    "        # x4 = self.fc3(x3)\n",
    "        # final_out, _ = self.final_lstm(torch.unsqueeze(x4, dim=2))\n",
    "        # print(f'output of final lstm: {final_out[:, -1, :].shape[1:]}')\n",
    "        # x = self.fc(final_out[:, -1, :])\n",
    "        # print(f'output of final linear layer: {x.shape[1:]}')\n",
    "        # print()\n",
    "        x = self.fc(x3)\n",
    "        return x\n",
    "    "
   ],
   "id": "21c53f1c392a2eb2",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:33:02.936589Z",
     "start_time": "2024-08-19T15:33:02.933814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=36):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.LazyLinear(512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 14 * 14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "876758bd07bc26b7",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T15:33:03.571762Z",
     "start_time": "2024-08-19T15:33:03.565741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "def train(dataset, num_epochs, model_path, leave_one_out=False):\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "    \n",
    "    if leave_one_out:\n",
    "        train_set, val_set = dataset[:-1], [dataset[-1]]\n",
    "    else:\n",
    "        train_set, val_set = train_test_split(dataset, test_size=0.15)\n",
    "    train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=16, shuffle=True)\n",
    "    \n",
    "    # print(\"Before initialization of the model\")\n",
    "    model = MfccLSTM(input_size=20, hidden_size=32, num_classes=36, output_size=64)\n",
    "    # print(\"After initialization of the model\")\n",
    "    model = model.to(device)\n",
    "    # print(\"Model to GPU\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.BCELoss()\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        correct_train = 0  # correct training examples\n",
    "        total_train = 0 # total training examples\n",
    "        tic = time.perf_counter()\n",
    "        \n",
    "        # print(f'---- EPOCH {epoch} ----')\n",
    "        # print(\"Before entering the inner loop\")\n",
    "        for images, sequences, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, sequences)\n",
    "            # time.sleep(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_train_loss += loss.item() * images.size(0)\n",
    "\n",
    "            _, predicted_train = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted_train == labels).sum().item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        toc = time.perf_counter()\n",
    "        time_taken = toc - tic\n",
    "        \n",
    "        epoch_train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Evaluation of the model\n",
    "        model.eval()\n",
    "        total, correct = 0, 0\n",
    "        for images, sequences, labels  in val_loader:\n",
    "            images = images.to(device)\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images, sequences)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = correct / total\n",
    "        print(f'correct: {correct}, total: {total}')\n",
    "        print(f'val_accuracy: {val_accuracy:.4f}')\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} / Val Accuracy: {val_accuracy:.4f} Iter Time: {time_taken:.2f}s\")\n",
    "        \n",
    "        if epoch == num_epochs - 1 and epoch != 0:\n",
    "            plt.plot(range(epoch+1), train_accuracies, label='Training Accuracy', color=\"blue\")\n",
    "            plt.plot(range(epoch+1), val_accuracies, label='Validation Accuracy', color=\"orange\")\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title(f'Training vs Validation Accuracy')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        print()\n",
    "    torch.save(model.state_dict(), model_path)"
   ],
   "id": "9e1ab811f0d1abfe",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-19T15:33:04.365297Z"
    }
   },
   "cell_type": "code",
   "source": "train(audioDatasetMfcc, 500, \"model_multiclass_200_cnnlstm_finallstm_19_08_24.pth\")",
   "id": "7407a641de41b299",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 34, total: 277\n",
      "val_accuracy: 0.1227\n",
      "Epoch [1/500], Train Loss: 3.4612, Train Accuracy: 0.0569 / Val Accuracy: 0.1227 Iter Time: 1.34s\n",
      "\n",
      "correct: 73, total: 277\n",
      "val_accuracy: 0.2635\n",
      "Epoch [2/500], Train Loss: 2.7910, Train Accuracy: 0.1700 / Val Accuracy: 0.2635 Iter Time: 0.98s\n",
      "\n",
      "correct: 78, total: 277\n",
      "val_accuracy: 0.2816\n",
      "Epoch [3/500], Train Loss: 2.2875, Train Accuracy: 0.3131 / Val Accuracy: 0.2816 Iter Time: 0.97s\n",
      "\n",
      "correct: 104, total: 277\n",
      "val_accuracy: 0.3755\n",
      "Epoch [4/500], Train Loss: 1.9197, Train Accuracy: 0.4013 / Val Accuracy: 0.3755 Iter Time: 0.97s\n",
      "\n",
      "correct: 114, total: 277\n",
      "val_accuracy: 0.4116\n",
      "Epoch [5/500], Train Loss: 1.6562, Train Accuracy: 0.4818 / Val Accuracy: 0.4116 Iter Time: 0.96s\n",
      "\n",
      "correct: 131, total: 277\n",
      "val_accuracy: 0.4729\n",
      "Epoch [6/500], Train Loss: 1.4401, Train Accuracy: 0.5399 / Val Accuracy: 0.4729 Iter Time: 0.97s\n",
      "\n",
      "correct: 145, total: 277\n",
      "val_accuracy: 0.5235\n",
      "Epoch [7/500], Train Loss: 1.2298, Train Accuracy: 0.5917 / Val Accuracy: 0.5235 Iter Time: 0.98s\n",
      "\n",
      "correct: 157, total: 277\n",
      "val_accuracy: 0.5668\n",
      "Epoch [8/500], Train Loss: 1.1329, Train Accuracy: 0.6204 / Val Accuracy: 0.5668 Iter Time: 0.98s\n",
      "\n",
      "correct: 147, total: 277\n",
      "val_accuracy: 0.5307\n",
      "Epoch [9/500], Train Loss: 0.9125, Train Accuracy: 0.7220 / Val Accuracy: 0.5307 Iter Time: 0.97s\n",
      "\n",
      "correct: 160, total: 277\n",
      "val_accuracy: 0.5776\n",
      "Epoch [10/500], Train Loss: 0.8724, Train Accuracy: 0.7195 / Val Accuracy: 0.5776 Iter Time: 0.97s\n",
      "\n",
      "correct: 170, total: 277\n",
      "val_accuracy: 0.6137\n",
      "Epoch [11/500], Train Loss: 0.7408, Train Accuracy: 0.7668 / Val Accuracy: 0.6137 Iter Time: 0.98s\n",
      "\n",
      "correct: 167, total: 277\n",
      "val_accuracy: 0.6029\n",
      "Epoch [12/500], Train Loss: 0.6135, Train Accuracy: 0.8134 / Val Accuracy: 0.6029 Iter Time: 0.99s\n",
      "\n",
      "correct: 181, total: 277\n",
      "val_accuracy: 0.6534\n",
      "Epoch [13/500], Train Loss: 0.5604, Train Accuracy: 0.8173 / Val Accuracy: 0.6534 Iter Time: 0.99s\n",
      "\n",
      "correct: 170, total: 277\n",
      "val_accuracy: 0.6137\n",
      "Epoch [14/500], Train Loss: 0.4866, Train Accuracy: 0.8460 / Val Accuracy: 0.6137 Iter Time: 0.98s\n",
      "\n",
      "correct: 185, total: 277\n",
      "val_accuracy: 0.6679\n",
      "Epoch [15/500], Train Loss: 0.4328, Train Accuracy: 0.8639 / Val Accuracy: 0.6679 Iter Time: 0.98s\n",
      "\n",
      "correct: 188, total: 277\n",
      "val_accuracy: 0.6787\n",
      "Epoch [16/500], Train Loss: 0.4328, Train Accuracy: 0.8690 / Val Accuracy: 0.6787 Iter Time: 0.97s\n",
      "\n",
      "correct: 185, total: 277\n",
      "val_accuracy: 0.6679\n",
      "Epoch [17/500], Train Loss: 0.4096, Train Accuracy: 0.8601 / Val Accuracy: 0.6679 Iter Time: 0.97s\n",
      "\n",
      "correct: 191, total: 277\n",
      "val_accuracy: 0.6895\n",
      "Epoch [18/500], Train Loss: 0.3842, Train Accuracy: 0.8799 / Val Accuracy: 0.6895 Iter Time: 0.97s\n",
      "\n",
      "correct: 193, total: 277\n",
      "val_accuracy: 0.6968\n",
      "Epoch [19/500], Train Loss: 0.3095, Train Accuracy: 0.8946 / Val Accuracy: 0.6968 Iter Time: 0.98s\n",
      "\n",
      "correct: 195, total: 277\n",
      "val_accuracy: 0.7040\n",
      "Epoch [20/500], Train Loss: 0.2914, Train Accuracy: 0.9067 / Val Accuracy: 0.7040 Iter Time: 0.97s\n",
      "\n",
      "correct: 187, total: 277\n",
      "val_accuracy: 0.6751\n",
      "Epoch [21/500], Train Loss: 0.2611, Train Accuracy: 0.9157 / Val Accuracy: 0.6751 Iter Time: 0.97s\n",
      "\n",
      "correct: 198, total: 277\n",
      "val_accuracy: 0.7148\n",
      "Epoch [22/500], Train Loss: 0.2453, Train Accuracy: 0.9246 / Val Accuracy: 0.7148 Iter Time: 0.99s\n",
      "\n",
      "correct: 190, total: 277\n",
      "val_accuracy: 0.6859\n",
      "Epoch [23/500], Train Loss: 0.1808, Train Accuracy: 0.9431 / Val Accuracy: 0.6859 Iter Time: 0.97s\n",
      "\n",
      "correct: 197, total: 277\n",
      "val_accuracy: 0.7112\n",
      "Epoch [24/500], Train Loss: 0.2381, Train Accuracy: 0.9252 / Val Accuracy: 0.7112 Iter Time: 1.00s\n",
      "\n",
      "correct: 198, total: 277\n",
      "val_accuracy: 0.7148\n",
      "Epoch [25/500], Train Loss: 0.2575, Train Accuracy: 0.9188 / Val Accuracy: 0.7148 Iter Time: 0.97s\n",
      "\n",
      "correct: 182, total: 277\n",
      "val_accuracy: 0.6570\n",
      "Epoch [26/500], Train Loss: 0.2432, Train Accuracy: 0.9233 / Val Accuracy: 0.6570 Iter Time: 0.97s\n",
      "\n",
      "correct: 192, total: 277\n",
      "val_accuracy: 0.6931\n",
      "Epoch [27/500], Train Loss: 0.2875, Train Accuracy: 0.9246 / Val Accuracy: 0.6931 Iter Time: 1.02s\n",
      "\n",
      "correct: 201, total: 277\n",
      "val_accuracy: 0.7256\n",
      "Epoch [28/500], Train Loss: 0.2062, Train Accuracy: 0.9425 / Val Accuracy: 0.7256 Iter Time: 1.02s\n",
      "\n",
      "correct: 194, total: 277\n",
      "val_accuracy: 0.7004\n",
      "Epoch [29/500], Train Loss: 0.1587, Train Accuracy: 0.9534 / Val Accuracy: 0.7004 Iter Time: 1.07s\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "20fb3c1c29961f85",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
