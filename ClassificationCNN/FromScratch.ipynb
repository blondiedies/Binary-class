{
 "cells": [
  {
   "cell_type": "code",
   "id": "2a122adee9a39c20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T21:34:13.080484Z",
     "start_time": "2025-01-27T21:34:13.077547Z"
    }
   },
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pydub import AudioSegment, silence\n",
    "import pickle\n",
    "import pyloudnorm as pyln\n",
    "from sklearn.metrics import make_scorer\n"
   ],
   "outputs": [],
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "id": "9ad2373f64ed7deb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:46:12.359885Z",
     "start_time": "2025-01-27T19:46:12.357671Z"
    }
   },
   "source": [
    "# waveform function for me to not bang my keyboard\n",
    "def disp_waveform(signal,title='', sr=None, color='blue'):\n",
    "    plt.figure(figsize=(7,2))\n",
    "    plt.title(title)\n",
    "    librosa.display.waveshow(signal, sr=sr, color=color)"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:46:12.733733Z",
     "start_time": "2025-01-27T19:46:12.730615Z"
    }
   },
   "source": [
    "import noisereduce as nr\n",
    "\n",
    "def signum(x):\n",
    "    return 1 if x>0 else -1\n",
    "\n",
    "def isolator(signal, sample_rate, n_fft, hop_length, before, after, threshold, show=False):\n",
    "    strokes = []\n",
    "    # -- signal'\n",
    "    denoised_signal = nr.reduce_noise(signal, sr=sample_rate)\n",
    "    # if show:\n",
    "    #     disp_waveform(denoised_signal, 'signal waveform DENOISED', sr=sample_rate)\n",
    "    #     disp_waveform(signal, 'signal waveform NOISED', sr=sample_rate)\n",
    "    #     disp_waveform(denoised_signal_boosted, 'signal waveform DENOISED n BOOSTED', sr=sample_rate)\n",
    "    signal = denoised_signal\n",
    "    fft = librosa.stft(signal, n_fft=n_fft, hop_length=hop_length)\n",
    "    energy = np.abs(np.sum(fft, axis=0)).astype(float)\n",
    "    norm = np.linalg.norm(energy)\n",
    "    energy = energy/norm\n",
    "    # -- energy'\n",
    "    threshed = energy > threshold\n",
    "    # -- peaks'\n",
    "    if show:\n",
    "        # disp_waveform(threshed.astype(float), sr=sample_rate)\n",
    "        pass\n",
    "    peaks = np.where(threshed == True)[0]\n",
    "    peak_count = len(peaks)\n",
    "    prev_end = sample_rate*0.1*(-1)\n",
    "    # '-- isolating keystrokes'\n",
    "    timestamps = []\n",
    "    for i in range(peak_count):\n",
    "        this_peak = peaks[i]\n",
    "        timestamp = (this_peak*hop_length) + n_fft//2\n",
    "        if timestamp > prev_end + (0.1*sample_rate):\n",
    "            keystroke = signal[timestamp-before:timestamp+after]\n",
    "            # strokes.append(torch.tensor(keystroke)[None, :])\n",
    "            # keystroke = transform(keystroke)\n",
    "            if len(keystroke) >= before + after:\n",
    "                strokes.append(keystroke)\n",
    "                timestamps.append(timestamp)\n",
    "                if len(strokes) <= 1:\n",
    "                    # disp_waveform(keystroke, title=f'keystroke {len(strokes)}', sr=sample_rate)\n",
    "                    pass\n",
    "                prev_end = timestamp+after\n",
    "    return peaks, strokes, timestamps"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "23314fd12f6ff70f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:46:13.203096Z",
     "start_time": "2025-01-27T19:46:13.201505Z"
    }
   },
   "source": [
    "def partition_audio(samples_arr : np.array):\n",
    "    ret_samples = np.abs(samples_arr)\n",
    "    return ret_samples"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "925e1183ef02fb55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:46:13.556791Z",
     "start_time": "2025-01-27T19:46:13.551108Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "def find_key_presses(waveform, res, waveform_threshold, waveform_max, threshold_background, history_size, remove_low_power):\n",
    "    # Clear previous results\n",
    "    # res.clear()\n",
    "    # waveform_threshold = np.zeros_like(waveform)\n",
    "    # waveform_max = np.zeros_like(waveform)\n",
    "    # \n",
    "    rb_begin = 0\n",
    "    rb_average = 0.0\n",
    "    rb_samples = np.zeros(history_size)\n",
    "\n",
    "    k = history_size\n",
    "    que = deque(maxlen=k)\n",
    "\n",
    "    samples = np.abs(waveform)  # Taking absolute values like waveformAbs in C++\n",
    "    n = len(samples)\n",
    "    overall_loudness = 0\n",
    "    len_ovr_loudness = 0\n",
    "    for i in range(n):\n",
    "        ii = i - k // 2\n",
    "        if ii >= 0:\n",
    "            rb_average *= len(rb_samples)\n",
    "            rb_average -= rb_samples[rb_begin]\n",
    "            acur = samples[i]\n",
    "            rb_samples[rb_begin] = acur\n",
    "            rb_average += acur\n",
    "            rb_average /= len(rb_samples)\n",
    "            rb_begin = (rb_begin + 1) % len(rb_samples)\n",
    "        if i < k:\n",
    "            # Handling initial filling of the deque\n",
    "            while que and samples[i] >= samples[que[-1]]:\n",
    "                que.pop()\n",
    "            que.append(i)\n",
    "        else:\n",
    "            # Maintain the deque as a max-queue for the sliding window\n",
    "            while que and que[0] <= i - k:\n",
    "                que.popleft()\n",
    "\n",
    "            # same code as if i<k\n",
    "            while que and samples[i] >= samples[que[-1]]:\n",
    "                que.pop()\n",
    "            que.append(i)\n",
    "\n",
    "            itest = i - k // 2\n",
    "            if  k <= itest < n - k and que[0] == itest:\n",
    "                acur = samples[itest]\n",
    "                if acur > threshold_background * rb_average:\n",
    "                    res.append({\n",
    "                        'waveform': waveform[itest - k//6 : itest + (5*k)//6],\n",
    "                        'index': itest\n",
    "                    })\n",
    "                    quiet_part = samples[itest + (3*k)//6 : itest + (5*k)//6]\n",
    "                    len_ovr_loudness += len(quiet_part)\n",
    "                    overall_loudness += np.sum(quiet_part)\n",
    "            waveform_threshold[itest] = threshold_background * rb_average\n",
    "            waveform_max[itest] = samples[que[0]]\n",
    "\n",
    "    if remove_low_power:\n",
    "        while True:\n",
    "            old_n = len(res)\n",
    "\n",
    "            avg_power = sum(samples[kp[\"position\"]] for kp in res) / len(res)\n",
    "\n",
    "            tmp_res = res[:]\n",
    "            res.clear()\n",
    "\n",
    "            for kp in tmp_res:\n",
    "                if samples[kp[\"position\"]] > 0.3 * avg_power:\n",
    "                    res.append(kp)\n",
    "\n",
    "            if len(res) == old_n:\n",
    "                break\n",
    "                \n",
    "    if len_ovr_loudness > 0:\n",
    "        avg_loudness = overall_loudness / len_ovr_loudness\n",
    "    else:\n",
    "        avg_loudness = 0\n",
    "        \n",
    "    return {'waveform_threshold': waveform_threshold, \n",
    "            'waveform_max': waveform_max,\n",
    "            'res': res,\n",
    "            'avg_loudness': avg_loudness\n",
    "            }"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "b24a7c9ee1772887",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:46:13.991111Z",
     "start_time": "2025-01-27T19:46:13.989049Z"
    }
   },
   "source": [
    "import array\n",
    "\n",
    "def numpy_to_audiosegment(samples, sample_rate=44100, sample_width=2, channels=1):\n",
    "    # Ensure the numpy array is in the correct dtype (int16 or int32 based on sample_width)\n",
    "    if sample_width == 2:\n",
    "        samples = np.int16(samples)\n",
    "    elif sample_width == 4:\n",
    "        samples = np.int32(samples)\n",
    "    \n",
    "    # Convert numpy array to byte data\n",
    "    audio_data = array.array('h', samples)  # 'h' for 16-bit PCM audio\n",
    "    byte_data = audio_data.tobytes()\n",
    "    \n",
    "    # Create AudioSegment\n",
    "    audio_segment = AudioSegment(\n",
    "        data=byte_data,\n",
    "        sample_width=sample_width,  # 2 for 16-bit, 4 for 32-bit\n",
    "        frame_rate=sample_rate,\n",
    "        channels=channels\n",
    "    )\n",
    "    \n",
    "    return audio_segment"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "6cfc880f1fa8f48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:46:17.187196Z",
     "start_time": "2025-01-27T19:46:14.523860Z"
    }
   },
   "source": [
    "# constants\n",
    "N_FFT, HOP_LENGTH, BEFORE, AFTER = 1024, 225, 2400, 12000\n",
    "\n",
    "key_length = 8820\n",
    "for num in range(0, 5):\n",
    "    samples, sr = librosa.load(f'../MKA datasets/All Dataset/Raw Data/{num}/{num}mac.wav', sr=44100)\n",
    "    pydub_samples = AudioSegment.from_file(f'../MKA datasets/All Dataset/Raw Data/{num}/{num}mac.wav', format=\"wav\", frame_rate=44100)\n",
    "    \n",
    "    samples2, sr = librosa.load(f'../Dataset-for-Binary/base-audio/audio_{num}.wav', sr=44100)\n",
    "    pydub_samples2 = AudioSegment.from_file(f'../Dataset-for-Binary/base-audio/audio_{num}.wav', format='wav', frame_rate=44100, sample_width=4)\n",
    "    \n",
    "    silences = silence.detect_silence(pydub_samples, silence_thresh=1.01*pydub_samples.dBFS, min_silence_len=70)\n",
    "    ovr_dbms = []\n",
    "    for start_ind, final_ind in silences:\n",
    "        ovr_dbms.append(pydub_samples[start_ind:final_ind].dBFS)\n",
    "    avg_dbfs = np.average(ovr_dbms)\n",
    "    \n",
    "    # the number 2\n",
    "    silences2 = silence.detect_silence(pydub_samples2, silence_thresh=1.01*pydub_samples2.dBFS, min_silence_len=70)\n",
    "    ovr_dbms2 = []\n",
    "    for start_ind, final_ind in silences2:\n",
    "        ovr_dbms2.append(pydub_samples2[start_ind:final_ind].dBFS)\n",
    "    avg_dbfs2 = np.average(ovr_dbms2)\n",
    "    \n",
    "    threshold_vals = np.arange(0.13, 0.15, 0.002)\n",
    "    print(f'KEY {num}')\n",
    "    for i in threshold_vals:\n",
    "        print(f'i={i:.3f}')\n",
    "        print(\"AUDIO MKA\")\n",
    "        print(f'avg dBFS: {avg_dbfs:.3f}')\n",
    "        return_dic = find_key_presses(samples,[],{},{},np.abs(i*avg_dbfs), key_length, False)\n",
    "        print(f'\\tbackground_prof: {abs(i*avg_dbfs):.3f} / Number of keys: {len(return_dic[\"res\"])}')\n",
    "        print(\"AUDIO ORIGINAL\")\n",
    "        print(f'avg dBFS: {avg_dbfs2:.3f}')\n",
    "        return_dic2 = find_key_presses(samples2,[],{},{},np.abs(i*avg_dbfs2), key_length, False)\n",
    "        print(f'\\tbackground_prof: {abs(i*avg_dbfs2):.3f} / Number of keys: {len(return_dic2[\"res\"])}')\n",
    "        print()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY 0\n",
      "i=0.130\n",
      "AUDIO MKA\n",
      "avg dBFS: -47.495\n",
      "\tbackground_prof: 6.174 / Number of keys: 34\n",
      "AUDIO ORIGINAL\n",
      "avg dBFS: -30.743\n",
      "\tbackground_prof: 3.997 / Number of keys: 31\n",
      "\n",
      "i=0.132\n",
      "AUDIO MKA\n",
      "avg dBFS: -47.495\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 31\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAUDIO MKA\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mavg dBFS: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_dbfs\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 31\u001B[0m return_dic \u001B[38;5;241m=\u001B[39m find_key_presses(samples,[],{},{},np\u001B[38;5;241m.\u001B[39mabs(i\u001B[38;5;241m*\u001B[39mavg_dbfs), key_length, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mbackground_prof: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mabs\u001B[39m(i\u001B[38;5;241m*\u001B[39mavg_dbfs)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m / Number of keys: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(return_dic[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mres\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAUDIO ORIGINAL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[43], line 44\u001B[0m, in \u001B[0;36mfind_key_presses\u001B[0;34m(waveform, res, waveform_threshold, waveform_max, threshold_background, history_size, remove_low_power)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m que \u001B[38;5;129;01mand\u001B[39;00m samples[i] \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m samples[que[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]]:\n\u001B[1;32m     43\u001B[0m     que\u001B[38;5;241m.\u001B[39mpop()\n\u001B[0;32m---> 44\u001B[0m que\u001B[38;5;241m.\u001B[39mappend(i)\n\u001B[1;32m     46\u001B[0m itest \u001B[38;5;241m=\u001B[39m i \u001B[38;5;241m-\u001B[39m k \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m  k \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m itest \u001B[38;5;241m<\u001B[39m n \u001B[38;5;241m-\u001B[39m k \u001B[38;5;129;01mand\u001B[39;00m que[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m itest:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "c3cd39ac1fe20468",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:46:19.212367Z",
     "start_time": "2025-01-27T19:46:19.209782Z"
    }
   },
   "source": [
    "from detecta import detect_peaks\n",
    "import scipy.signal as signal\n",
    "\n",
    "def count_peaks(samples, key_length=14400, show=True):\n",
    "    # meter = pyln.Meter(44100)  # Create BS.1770 meter\n",
    "    # loudness = meter.integrated_loudness(samples)\n",
    "    # target_loudness = -25.0\n",
    "    # samples = pyln.normalize.loudness(samples, loudness, target_loudness\n",
    "    threshold = np.percentile(samples, 96.6)\n",
    "    # final_samples = pyln.normalize.peak(samples, 0.75)\n",
    "    indexes = detect_peaks(samples[int(key_length/3): -int(key_length/3)], show=show, mpd=key_length - key_length/3, mph=threshold)\n",
    "    return len(indexes)\n",
    "\n",
    "def isolator_new(file_path, sr, key_length=14400, k=0.15):\n",
    "    pydub_samples = AudioSegment.from_file(file_path, format=\"wav\", frame_rate=sr)\n",
    "    silences = silence.detect_silence(pydub_samples, silence_thresh=1.01*pydub_samples.dBFS, min_silence_len=50)\n",
    "    ovr_dbms = []\n",
    "    for start_ind, final_ind in silences:\n",
    "        ovr_dbms.append(pydub_samples[start_ind:final_ind].dBFS)\n",
    "    avg_dbfs = np.average(ovr_dbms)\n",
    "    samples, sr = librosa.load(file_path, sr=44100)\n",
    "    samples = nr.reduce_noise(samples, sr=44100)\n",
    "    return_dic = find_key_presses(samples,[],{},{},np.abs(k*avg_dbfs), key_length, False)\n",
    "    return return_dic"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:46:20.063045Z",
     "start_time": "2025-01-27T19:46:20.059529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataset_viejo(n_fft, hop_length, before, after, keys, audio_dir, curr_labels, prom=0.2391, original=True, key_length=14400):\n",
    "    data_dict = {'Key':[], 'File':[]}\n",
    "    base_step = 0.01\n",
    "    for i, File in enumerate(keys):\n",
    "        curr_step = base_step\n",
    "        loc = audio_dir + File\n",
    "        samples, sr = librosa.load(loc)\n",
    "        # samples = nr.reduce_noise(samples, sr=44100)\n",
    "        show = (File[6 if original else 0] == '0')\n",
    "        peaks_count = count_peaks(samples, key_length, False)\n",
    "        strokes = isolator(samples, sr, n_fft, hop_length, before, after, prom, show)[1]\n",
    "        num_keys = len(strokes)\n",
    "        count = 0\n",
    "        k = prom\n",
    "        prev_k = prom\n",
    "        print(f'num_keys: {num_keys} // peaks_count: {peaks_count} // prom: {prom}')\n",
    "        while num_keys != peaks_count:\n",
    "            if num_keys > peaks_count:\n",
    "                if count > 0 and prev_k == k + curr_step:\n",
    "                    curr_step /= 2\n",
    "                elif count > 0:\n",
    "                    curr_step += (curr_step / 2)\n",
    "                prev_k = k\n",
    "                k += curr_step\n",
    "            else:\n",
    "                if count > 0 and prev_k == k - curr_step:\n",
    "                    curr_step /= 2\n",
    "                elif count > 0:\n",
    "                    curr_step += (curr_step / 2)\n",
    "                prev_k = k\n",
    "                k += -curr_step\n",
    "            strokes = isolator(samples, sr, n_fft, hop_length, before, after, k, show)[1]\n",
    "            num_keys = len(strokes)\n",
    "            # print(f'actual k: {k:.3f} // num strokes: {num_keys}')\n",
    "            # time.sleep(1)\n",
    "            count += 1\n",
    "        \n",
    "        print(f'{File}. Len strokes: {len(strokes)}')\n",
    "        if show:\n",
    "            print(f'Length strokes: {len(strokes)}')\n",
    "        label = [curr_labels[i]]*len(strokes)\n",
    "        data_dict['Key'] += label\n",
    "        data_dict['File'] += strokes\n",
    "        \n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for l in df['Key']:\n",
    "        if not l in mapper:\n",
    "            mapper[l] = counter\n",
    "            counter += 1\n",
    "    df.replace({'Key': mapper}, inplace = True)\n",
    "\n",
    "    return df"
   ],
   "id": "fddfae8f4d5b9034",
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "23a14d8cd7e215a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:46:20.513228Z",
     "start_time": "2025-01-27T19:46:20.509641Z"
    }
   },
   "source": [
    "def create_dataset(keys, initial_k, key_length=8820):\n",
    "    data_dict = {'Key':[], 'File':[]}\n",
    "    base_step = 0.01\n",
    "    # file_path_function = lambda currkey, keyb: f'../MKA datasets/All Dataset/Raw Data/{currkey}/{currkey}{keyb}.wav'\n",
    "    for keyb in ['mac']:\n",
    "        for i, key in enumerate(keys):\n",
    "            curr_key = key\n",
    "            curr_step = base_step\n",
    "            # if key.isalpha and not key.isalnum(): # if is a string\n",
    "            #     curr_key = key.lower()\n",
    "            # file_path = f'../MKA datasets/All Dataset/Raw Data/{curr_key}/{curr_key}{keyb}.wav'\n",
    "            file_path = f'../Dataset-for-Binary/base-audio/audio_{curr_key}.wav'\n",
    "            samples, sr = librosa.load(file_path, sr=44100)\n",
    "        \n",
    "            # peaks_count\n",
    "            if key == \"W\":\n",
    "                show_val = True\n",
    "            else:\n",
    "                show_val = False\n",
    "            peaks_count = count_peaks(samples, key_length, show_val)\n",
    "            \n",
    "            # isolator\n",
    "            k = initial_k \n",
    "            curr_array = isolator_new(file_path, sr, key_length, k)['res']\n",
    "            strokes = [curr['waveform'] for curr in curr_array]\n",
    "            num_keys = len(strokes)\n",
    "            prev_k = k \n",
    "            count = 0\n",
    "            while num_keys != peaks_count:\n",
    "                if num_keys > peaks_count:\n",
    "                    if count > 0 and prev_k == k + curr_step:\n",
    "                        curr_step /= 2\n",
    "                    elif count > 0:\n",
    "                        curr_step += (curr_step / 2)\n",
    "                    prev_k = k\n",
    "                    k += curr_step\n",
    "                else:\n",
    "                    if count > 0 and prev_k == k - curr_step:\n",
    "                        curr_step /= 2\n",
    "                    elif count > 0:\n",
    "                        curr_step += (curr_step / 2)\n",
    "                    prev_k = k\n",
    "                    k += -curr_step\n",
    "                curr_arr = isolator_new(file_path, sr, key_length, k)['res']\n",
    "                strokes = [arr['waveform'] for arr in curr_arr]\n",
    "                num_keys = len(strokes)\n",
    "                count += 1\n",
    "                # print(f'k={k:.4f}\\tnum_keys={num_keys}')\n",
    "            print(f'key {key} {keyb} final k={k:.4f}\\tnum_keys={num_keys}\\tpeaks={peaks_count}')\n",
    "            print()\n",
    "            \n",
    "            # now get the actual keys file\n",
    "            label = [keys[i]]*num_keys\n",
    "            data_dict['Key'] += label\n",
    "            data_dict['File'] += strokes\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for l in df['Key']:\n",
    "        if not l in mapper:\n",
    "            mapper[l] = counter\n",
    "            counter += 1\n",
    "    df.replace({'Key': mapper}, inplace = True)\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:46:21.255133Z",
     "start_time": "2025-01-27T19:46:21.253455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ],
   "id": "fb1e97647d26f096",
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "c165936fc0ca1752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:46:45.333300Z",
     "start_time": "2025-01-27T19:46:22.091285Z"
    }
   },
   "source": [
    "# currently used keys\n",
    "curr_keys = list('1234567890QWERTYUIOPASDFGHJKLZXCVBNM')\n",
    "# curr_keys.append(\"space\")\n",
    "N_FFT, HOP_LENGTH, BEFORE, AFTER = 1024, 225, 2400, 12000\n",
    "MBP_AUDIO_DIR, labels, audiostr = ('/Users/jorgeleon/Binary-class/Dataset-for-Binary/base-audio/', list('1234567890QWERTYUIOPASDFGHJKLZXCVBNM'), 'audio_')\n",
    "# MBP_AUDIO_DIR, audiostr = '/Users/jorgeleon/Binary-class/MKA datasets/Mac/Raw data/', ''\n",
    "keys = [audiostr + k + '.wav' for k in labels]\n",
    "# key_length = 9200\n",
    "key_length=14400\n",
    "BEFORE = int(key_length / 6)\n",
    "AFTER = int(5 * (key_length / 6))\n",
    "# print(BEFORE, AFTER)\n",
    "# Create the final dataset\n",
    "mbp_dataset = create_dataset_viejo(N_FFT, HOP_LENGTH, BEFORE, AFTER, keys, MBP_AUDIO_DIR, labels, prom=0.2391,original=False, key_length=key_length)\n",
    "# original_dataset = create_dataset(curr_keys, initial_k=0.5, key_length=14400)\n",
    "# print(original_dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_1.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_2.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_3.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_4.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_5.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_6.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_7.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_8.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_9.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_0.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_Q.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_W.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_E.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_R.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_T.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_Y.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_U.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_I.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_O.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_P.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_A.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_S.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_D.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_F.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_G.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_H.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_J.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_K.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_L.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_Z.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_X.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_C.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_V.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_B.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_N.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_M.wav. Len strokes: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/8bpx1vc91zq76n48xvq00vkr0000gn/T/ipykernel_53517/2417281330.py:54: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace({'Key': mapper}, inplace = True)\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T14:37:29.549927Z",
     "start_time": "2025-01-16T14:37:29.494859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read curr_dataset back from the file\n",
    "# Write curr_dataset to a file to avoid running this shit over & over again\n",
    "with open('mbp_dataset_latest.pkl', 'wb') as f:\n",
    "    pickle.dump(mbp_dataset, f)\n",
    "\n",
    "# with open('mbp_dataset_31-10-24.pkl', 'rb') as f:\n",
    "#     mbp_dataset = pickle.load(f)"
   ],
   "id": "d52aca795d8f78b5",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:46:50.868875Z",
     "start_time": "2025-01-27T19:46:50.862899Z"
    }
   },
   "cell_type": "code",
   "source": "mbp_dataset",
   "id": "dfef6976f967c80c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Key                                               File\n",
       "0      0  [-1.9765257e-12, -7.5266117e-13, -3.2800477e-1...\n",
       "1      0  [3.1104875e-07, 2.8876556e-07, 4.8392434e-07, ...\n",
       "2      0  [1.5262138e-05, 1.8472152e-05, 2.0566145e-05, ...\n",
       "3      0  [0.0005208879, 0.0005132711, 0.0005245662, 0.0...\n",
       "4      0  [0.0026841052, 0.0026423365, 0.002146231, 0.00...\n",
       "..   ...                                                ...\n",
       "895   35  [-3.1930215e-14, 6.141843e-14, 3.7286046e-13, ...\n",
       "896   35  [-5.356133e-09, -6.045961e-09, -7.822481e-09, ...\n",
       "897   35  [-1.1476328e-08, -2.8126092e-08, -4.074221e-08...\n",
       "898   35  [-3.3517617e-13, 5.6366576e-13, 8.1714155e-13,...\n",
       "899   35  [-9.953198e-09, -9.345554e-09, 1.640801e-08, 5...\n",
       "\n",
       "[900 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-1.9765257e-12, -7.5266117e-13, -3.2800477e-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[3.1104875e-07, 2.8876556e-07, 4.8392434e-07, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.5262138e-05, 1.8472152e-05, 2.0566145e-05, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.0005208879, 0.0005132711, 0.0005245662, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.0026841052, 0.0026423365, 0.002146231, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>35</td>\n",
       "      <td>[-3.1930215e-14, 6.141843e-14, 3.7286046e-13, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>35</td>\n",
       "      <td>[-5.356133e-09, -6.045961e-09, -7.822481e-09, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>35</td>\n",
       "      <td>[-1.1476328e-08, -2.8126092e-08, -4.074221e-08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>35</td>\n",
       "      <td>[-3.3517617e-13, 5.6366576e-13, 8.1714155e-13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>35</td>\n",
       "      <td>[-9.953198e-09, -9.345554e-09, 1.640801e-08, 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "8a7918b824b24998",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:46:59.162863Z",
     "start_time": "2025-01-27T19:46:59.160709Z"
    }
   },
   "source": "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:47:01.003037Z",
     "start_time": "2025-01-27T19:47:00.999880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataset(n_fft, hop_length, before, after, keys, audio_dir, curr_labels, prom=0.2391, original=True):\n",
    "    data_dict = {'Key':[], 'File':[]}\n",
    "    for i, File in enumerate(keys):\n",
    "        loc = audio_dir + File\n",
    "        samples, sr = librosa.load(loc)\n",
    "        show = (File[6 if original else 0] == '0')\n",
    "        strokes = isolator(samples, sr, n_fft, hop_length, before, after, prom, show)\n",
    "        if show:\n",
    "            print(f'Length strokes: {len(strokes)}')\n",
    "        label = [curr_labels[i]]*len(strokes)\n",
    "        data_dict['Key'] += label\n",
    "        data_dict['File'] += strokes\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for l in df['Key']:\n",
    "        if not l in mapper:\n",
    "            mapper[l] = counter\n",
    "            counter += 1\n",
    "    df.replace({'Key': mapper}, inplace = True)\n",
    "\n",
    "    return df"
   ],
   "id": "2fee322473e2bfe1",
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "2b8b07040da6027e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:47:02.702254Z",
     "start_time": "2025-01-27T19:47:02.699926Z"
    }
   },
   "source": [
    "import audiosegment\n",
    "\n",
    "def get_audio_length(audio_path):\n",
    "    audio = audiosegment.from_file(audio_path)\n",
    "    return audio.duration_seconds\n",
    "\n",
    "def convert_to_ms(t):\n",
    "    return round(t*1000)\n",
    "\n",
    "def get_audio_length_average(audio_path, keys):\n",
    "    lengths = []\n",
    "    for i, File in enumerate(keys):\n",
    "        loc = audio_path + File\n",
    "        length = get_audio_length(loc)\n",
    "        print(f'File {loc} length: {length:2f}\\n')\n",
    "        lengths.append(length)\n",
    "    average = np.mean(lengths)\n",
    "    return convert_to_ms(average)"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "id": "ac3598b8c7bcd285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:47:04.637442Z",
     "start_time": "2025-01-27T19:47:04.634797Z"
    }
   },
   "source": [
    "\n",
    "def time_shift(samples):\n",
    "    samples = samples.flatten()\n",
    "    shift = int(len(samples) * 0.4) #Max shift (0.4)\n",
    "    random_shift = random.randint(0, shift) #Random number between 0 and 0.4*len(samples)\n",
    "    data_roll = np.roll(samples, random_shift)\n",
    "    return data_roll\n",
    "\n",
    "def masking(samples):\n",
    "    num_mask = 2\n",
    "    freq_masking_max_percentage=0.10\n",
    "    time_masking_max_percentage=0.10\n",
    "    spec = samples\n",
    "    mean_value = spec.mean()\n",
    "    for i in range(num_mask):\n",
    "        all_frames_num, all_freqs_num = spec.shape[1], spec.shape[1] \n",
    "        freq_percentage = random.uniform(0.0, freq_masking_max_percentage)\n",
    "\n",
    "        num_freqs_to_mask = int(freq_percentage * all_freqs_num)\n",
    "        f0 = np.random.uniform(low=0.0, high=all_freqs_num - num_freqs_to_mask)\n",
    "        f0 = int(f0)\n",
    "        spec[:, f0:f0 + num_freqs_to_mask] = mean_value\n",
    "\n",
    "        time_percentage = random.uniform(0.0, time_masking_max_percentage)\n",
    "\n",
    "        num_frames_to_mask = int(time_percentage * all_frames_num)\n",
    "        t0 = np.random.uniform(low=0.0, high=all_frames_num - num_frames_to_mask)\n",
    "        t0 = int(t0)\n",
    "        spec[t0:t0 + num_frames_to_mask, :] = mean_value\n",
    "    return spec"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "id": "4a429af86b3206df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T02:31:55.873484Z",
     "start_time": "2025-01-28T02:31:55.864254Z"
    }
   },
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "class ToMelSpectrogram:\n",
    "    def __init__(self, audio_length=14400):\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        if len(samples) > self.audio_length:\n",
    "            samples = samples[:self.audio_length]\n",
    "        elif len(samples) < self.audio_length:\n",
    "            samples = np.pad(samples, (0, self.audio_length - len(samples)), mode='constant')\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=samples, sr=44100, n_mels=64, n_fft=1024, hop_length=225)\n",
    "        mel_spec_resized = resize(mel_spec, (64, 64), anti_aliasing=True)\n",
    "        mel_spec_resized = np.expand_dims(mel_spec_resized, axis=0)\n",
    "        return mel_spec_resized\n",
    "\n",
    "\n",
    "class ToMelSpectrogramMfcc:\n",
    "    def __init__(self, audio_length=14400):\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        if len(samples) > self.audio_length:\n",
    "            samples = samples[:self.audio_length]\n",
    "        elif len(samples) < self.audio_length:\n",
    "            samples = np.pad(samples, (0, self.audio_length - len(samples)), mode='constant')\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=samples, sr=44100, n_mels=64, n_fft=n_fft, hop_length=hop_length)\n",
    "        mel_spec = librosa.feature.mfcc(S=librosa.power_to_db(mel_spec))\n",
    "        mel_spec_resized = resize(mel_spec, (64, 64), anti_aliasing=True)\n",
    "        # mel_spec_resized = np.expand_dims(mel_spec_resized, axis=0)\n",
    "\n",
    "        return torch.tensor(mel_spec_resized)\n",
    "\n",
    "\n",
    "class ToMfcc:\n",
    "    def __init__(self, audio_length=14400):\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        if len(samples) > self.audio_length:\n",
    "            samples = samples[:self.audio_length]\n",
    "        elif len(samples) < self.audio_length:\n",
    "            samples = np.pad(samples, (0, self.audio_length - len(samples)), mode='constant')\n",
    "        \n",
    "        mfcc_spec = librosa.feature.mfcc(y=samples, sr=44100)\n",
    "        mfcc_spec = np.transpose(mfcc_spec)\n",
    "        \n",
    "        return torch.tensor(mfcc_spec)\n"
   ],
   "outputs": [],
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "id": "f019fc2bc0e25204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T23:46:48.078352Z",
     "start_time": "2025-01-27T23:46:48.075781Z"
    }
   },
   "source": [
    "transform = Compose([ToMelSpectrogram(key_length)])\n",
    "transform_mfcc = Compose([ToMfcc(key_length)])"
   ],
   "outputs": [],
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "id": "44b3b3ca9f37f4c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:47:10.642029Z",
     "start_time": "2025-01-27T19:47:10.621541Z"
    }
   },
   "source": [
    "audio_samples = mbp_dataset['File'].values.tolist()\n",
    "labels = mbp_dataset['Key'].values.tolist()\n",
    "\n",
    "audio_samples_no_masking = audio_samples.copy()\n",
    "labels_no_masking = labels.copy()\n",
    "audio_samples_new = audio_samples.copy() # audio samples CNN\n",
    "print(len(audio_samples))\n",
    "\n",
    "print(type(audio_samples[0]))\n",
    "\n",
    "for i, sample in enumerate(audio_samples):\n",
    "    audio_samples_new.append(time_shift(sample))\n",
    "    labels.append(labels[i])\n",
    "\n",
    "# convert labels to a numpy array\n",
    "labels = np.array(labels)\n",
    "print(len(audio_samples_new))\n",
    "print(len(labels))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "<class 'numpy.ndarray'>\n",
      "1800\n",
      "1800\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "id": "4a0f8d9f11e77431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:25:04.148669Z",
     "start_time": "2025-01-27T14:24:57.118887Z"
    }
   },
   "source": [
    "audioDatasetFin, audioDatasetFinMasking, audioDatasetMfcc, audioDatasetMfccMasking = [], [], [], []\n",
    "\n",
    "for i in range(len(audio_samples_new)):\n",
    "    transformed_sample = transform(audio_samples_new[i])\n",
    "    transformed_mfcc = transform_mfcc(audio_samples_new[i])\n",
    "    \n",
    "    # CoAtNet part\n",
    "    audioDatasetFin.append((transformed_sample, labels[i]))\n",
    "    audioDatasetFinMasking.append((masking(transformed_sample), labels[i]))\n",
    "    \n",
    "    # masking part\n",
    "    audioDatasetMfcc.append((transformed_sample, transformed_mfcc, labels[i]))\n",
    "    audioDatasetMfccMasking.append((masking(transformed_sample), transformed_mfcc, labels[i]))\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "f0b21201209caf5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T23:47:18.570737Z",
     "start_time": "2025-01-27T23:47:18.409908Z"
    }
   },
   "source": [
    "from specAugment import spec_augment_pytorch\n",
    "\n",
    "audio, sr = librosa.load('../Dataset-for-Binary/base-audio/audio_0.wav')\n",
    "i=1\n",
    "\n",
    "mel_spectrogram = librosa.feature.melspectrogram(y=audio_samples[i], sr=44100, n_mels=64, n_fft=1024, hop_length=225, fmax=22050)\n",
    "mel_spectrogram = resize(mel_spectrogram, (64, 64), anti_aliasing=True) \n",
    "shape = mel_spectrogram.shape\n",
    "print(mel_spectrogram.shape)\n",
    "mel_spectrogram = np.reshape(mel_spectrogram, (-1, shape[0], shape[1]))\n",
    "print(mel_spectrogram.shape)\n",
    "mel_spectrogram = torch.from_numpy(mel_spectrogram)\n",
    "\n",
    "spec_augment_pytorch.visualization_spectrogram(mel_spectrogram=mel_spectrogram,\n",
    "                                                      title=\"Raw Mel Spectrogram\")\n",
    "print(labels[i])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n",
      "(1, 64, 64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "For X (65) and Y (2) with flat shading, A should have shape (1, 64, 3) or (1, 64, 4) or (1, 64) or (64,), not (1, 64, 64)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[112], line 14\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(mel_spectrogram\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     12\u001B[0m mel_spectrogram \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(mel_spectrogram)\n\u001B[0;32m---> 14\u001B[0m spec_augment_pytorch\u001B[38;5;241m.\u001B[39mvisualization_spectrogram(mel_spectrogram\u001B[38;5;241m=\u001B[39mmel_spectrogram,\n\u001B[1;32m     15\u001B[0m                                                       title\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRaw Mel Spectrogram\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(labels[i])\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/specAugment/spec_augment_pytorch.py:125\u001B[0m, in \u001B[0;36mvisualization_spectrogram\u001B[0;34m(mel_spectrogram, title)\u001B[0m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# Show mel-spectrogram using librosa's specshow.\u001B[39;00m\n\u001B[1;32m    124\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m4\u001B[39m))\n\u001B[0;32m--> 125\u001B[0m librosa\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mspecshow(librosa\u001B[38;5;241m.\u001B[39mpower_to_db(mel_spectrogram, ref\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mmax), y_axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmel\u001B[39m\u001B[38;5;124m'\u001B[39m, fmax\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8000\u001B[39m, x_axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    126\u001B[0m \u001B[38;5;66;03m# plt.colorbar(format='%+2.0f dB')\u001B[39;00m\n\u001B[1;32m    127\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(title)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/librosa/display.py:1208\u001B[0m, in \u001B[0;36mspecshow\u001B[0;34m(data, x_coords, y_coords, x_axis, y_axis, sr, hop_length, n_fft, win_length, fmin, fmax, tuning, bins_per_octave, key, Sa, mela, thaat, auto_aspect, htk, unicode, intervals, unison, ax, **kwargs)\u001B[0m\n\u001B[1;32m   1204\u001B[0m x_coords \u001B[38;5;241m=\u001B[39m __mesh_coords(x_axis, x_coords, data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mall_params)\n\u001B[1;32m   1206\u001B[0m axes \u001B[38;5;241m=\u001B[39m __check_axes(ax)\n\u001B[0;32m-> 1208\u001B[0m out \u001B[38;5;241m=\u001B[39m axes\u001B[38;5;241m.\u001B[39mpcolormesh(x_coords, y_coords, data, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1210\u001B[0m __set_current_image(ax, out)\n\u001B[1;32m   1212\u001B[0m \u001B[38;5;66;03m# Set up axis scaling\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/__init__.py:1465\u001B[0m, in \u001B[0;36m_preprocess_data.<locals>.inner\u001B[0;34m(ax, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1462\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m   1463\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(ax, \u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1464\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1465\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(ax, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mmap\u001B[39m(sanitize_sequence, args), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1467\u001B[0m     bound \u001B[38;5;241m=\u001B[39m new_sig\u001B[38;5;241m.\u001B[39mbind(ax, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1468\u001B[0m     auto_label \u001B[38;5;241m=\u001B[39m (bound\u001B[38;5;241m.\u001B[39marguments\u001B[38;5;241m.\u001B[39mget(label_namer)\n\u001B[1;32m   1469\u001B[0m                   \u001B[38;5;129;01mor\u001B[39;00m bound\u001B[38;5;241m.\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mget(label_namer))\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:6298\u001B[0m, in \u001B[0;36mAxes.pcolormesh\u001B[0;34m(self, alpha, norm, cmap, vmin, vmax, shading, antialiased, *args, **kwargs)\u001B[0m\n\u001B[1;32m   6294\u001B[0m coords \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mstack([X, Y], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m   6296\u001B[0m kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msnap\u001B[39m\u001B[38;5;124m'\u001B[39m, mpl\u001B[38;5;241m.\u001B[39mrcParams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpcolormesh.snap\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m-> 6298\u001B[0m collection \u001B[38;5;241m=\u001B[39m mcoll\u001B[38;5;241m.\u001B[39mQuadMesh(\n\u001B[1;32m   6299\u001B[0m     coords, antialiased\u001B[38;5;241m=\u001B[39mantialiased, shading\u001B[38;5;241m=\u001B[39mshading,\n\u001B[1;32m   6300\u001B[0m     array\u001B[38;5;241m=\u001B[39mC, cmap\u001B[38;5;241m=\u001B[39mcmap, norm\u001B[38;5;241m=\u001B[39mnorm, alpha\u001B[38;5;241m=\u001B[39malpha, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   6301\u001B[0m collection\u001B[38;5;241m.\u001B[39m_scale_norm(norm, vmin, vmax)\n\u001B[1;32m   6303\u001B[0m coords \u001B[38;5;241m=\u001B[39m coords\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# flatten the grid structure; keep x, y\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/collections.py:2164\u001B[0m, in \u001B[0;36mQuadMesh.__init__\u001B[0;34m(self, coordinates, antialiased, shading, **kwargs)\u001B[0m\n\u001B[1;32m   2162\u001B[0m kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpickradius\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m   2163\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(coordinates\u001B[38;5;241m=\u001B[39mcoordinates, shading\u001B[38;5;241m=\u001B[39mshading)\n\u001B[0;32m-> 2164\u001B[0m Collection\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2166\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_antialiased \u001B[38;5;241m=\u001B[39m antialiased\n\u001B[1;32m   2167\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bbox \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mBbox\u001B[38;5;241m.\u001B[39munit()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/collections.py:203\u001B[0m, in \u001B[0;36mCollection.__init__\u001B[0;34m(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, offset_transform, norm, cmap, pickradius, hatch, urls, zorder, **kwargs)\u001B[0m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_offset_transform \u001B[38;5;241m=\u001B[39m offset_transform\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_path_effects \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 203\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_internal_update(kwargs)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_paths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/artist.py:1219\u001B[0m, in \u001B[0;36mArtist._internal_update\u001B[0;34m(self, kwargs)\u001B[0m\n\u001B[1;32m   1212\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_internal_update\u001B[39m(\u001B[38;5;28mself\u001B[39m, kwargs):\n\u001B[1;32m   1213\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1214\u001B[0m \u001B[38;5;124;03m    Update artist properties without prenormalizing them, but generating\u001B[39;00m\n\u001B[1;32m   1215\u001B[0m \u001B[38;5;124;03m    errors as if calling `set`.\u001B[39;00m\n\u001B[1;32m   1216\u001B[0m \n\u001B[1;32m   1217\u001B[0m \u001B[38;5;124;03m    The lack of prenormalization is to maintain backcompatibility.\u001B[39;00m\n\u001B[1;32m   1218\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_props(\n\u001B[1;32m   1220\u001B[0m         kwargs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{cls.__name__}\u001B[39;00m\u001B[38;5;124m.set() got an unexpected keyword argument \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1221\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{prop_name!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/artist.py:1195\u001B[0m, in \u001B[0;36mArtist._update_props\u001B[0;34m(self, props, errfmt)\u001B[0m\n\u001B[1;32m   1192\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(func):\n\u001B[1;32m   1193\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[1;32m   1194\u001B[0m                     errfmt\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m), prop_name\u001B[38;5;241m=\u001B[39mk))\n\u001B[0;32m-> 1195\u001B[0m             ret\u001B[38;5;241m.\u001B[39mappend(func(v))\n\u001B[1;32m   1196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret:\n\u001B[1;32m   1197\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpchanged()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/collections.py:2034\u001B[0m, in \u001B[0;36m_MeshData.set_array\u001B[0;34m(self, A)\u001B[0m\n\u001B[1;32m   2032\u001B[0m     shape \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mshape(A)\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m shape \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ok_shapes:\n\u001B[0;32m-> 2034\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2035\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFor X (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwidth\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) and Y (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mheight\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shading\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2036\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshading, A should have shape \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2037\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28mstr\u001B[39m,\u001B[38;5;250m \u001B[39mok_shapes))\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, not \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mA\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2038\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mset_array(A)\n",
      "\u001B[0;31mValueError\u001B[0m: For X (65) and Y (2) with flat shading, A should have shape (1, 64, 3) or (1, 64, 4) or (1, 64) or (64,), not (1, 64, 64)"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "id": "a2fe8d5f61e30540",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:03:53.638753Z",
     "start_time": "2025-01-27T14:03:53.636232Z"
    }
   },
   "source": [
    "# check for lengths of datasets\n",
    "len(audioDatasetMfcc), len(audioDatasetMfcc + audioDatasetMfccMasking), len(audioDatasetFin), len(audioDatasetFin + audioDatasetFinMasking)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 3600, 1800, 3600)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 101
  },
  {
   "cell_type": "code",
   "id": "21c53f1c392a2eb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T19:50:27.621337Z",
     "start_time": "2025-01-27T19:50:27.607035Z"
    }
   },
   "source": [
    "import time\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class MfccLSTM(nn.Module, BaseEstimator):\n",
    "    def __init__(self, batch_size=16, num_epochs=500, patience=120):\n",
    "        super(MfccLSTM, self).__init__()        \n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        hidden_size = 32\n",
    "        input_size = 20\n",
    "        dropout = 0.2 \n",
    "        num_classes = 36\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.LazyLinear(64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.LazyLinear(128)\n",
    "        self.final_lstm = nn.LSTM(1, 128, batch_first=True, proj_size=64)\n",
    "        \n",
    "        self.fc = nn.LazyLinear(num_classes)\n",
    "    \n",
    "    def forward(self, images, sequences):\n",
    "        # must return shape (batch_size, num_classes) \n",
    "        # batch_size: right now is 16\n",
    "        # num_classes: right now is 36\n",
    "        x1 = self.conv(images)\n",
    "        # print(f'input of first lstm: {sequences.shape[1:]}')\n",
    "        out1, _ = self.lstm(sequences)\n",
    "        out1_dp = self.dropout(out1)\n",
    "        # print(f'output of first lstm: {out1_dp.shape[1:]}')\n",
    "        # print(f'input of second lstm: {out1_dp[:, -1, :].shape[1:]}')\n",
    "        out2, _ = self.lstm2(out1_dp[:, -1, :])\n",
    "        out2_dp = self.dropout(out2)\n",
    "        # print(f'output of second lstm: {out2_dp.shape[1:]}')\n",
    "        x2 = self.fc2(self.fc1(out2_dp))\n",
    "        x3 = torch.cat((x1, x2), 1)\n",
    "        # print(f'output of concatenation: {x3.shape[1:]}')\n",
    "        # x4 = self.fc3(x3)\n",
    "        # # print(f'input final lstm: {x4[:,-1,:].shape[1:]}')\n",
    "        # print(f'x4.shape: {x4.shape[1:]}')\n",
    "        # x_final = self.final_lstm(x4)\n",
    "        # # x = self.fc(final_out[:, -1, :])\n",
    "        x = self.fc(x3)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._optimizer = optim.Adam(self.parameters(), lr=5e-4)\n",
    "        # same training method but now inside the class\n",
    "        model = self.to(device)\n",
    "        \n",
    "        # loss criterion\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # # concatenate so it has the same shape as before\n",
    "        # dataset = np.concatenate((X, y), axis=1)\n",
    "         # concatenate so it has the same shape as before\n",
    "        dataset = [(X[i], y[i]) for i in range(len(X))]\n",
    "        train_set, val_set = train_test_split(dataset, test_size=0.005)\n",
    "        train_loader = DataLoader(train_set, batch_size=self.batch_size)\n",
    "        val_loader = DataLoader(val_set, batch_size=self.batch_size)\n",
    "        \n",
    "        best_val_acc, epochs_no_imp = 0, 0\n",
    "        train_accuracies, val_accuracies = [], []\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            model.train()\n",
    "            epoch_train_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            tic = time.perf_counter()\n",
    "            \n",
    "            for (images, sequences), labels in train_loader:\n",
    "                images = images.to(device)\n",
    "                sequences = sequences.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                self._optimizer.zero_grad()\n",
    "        \n",
    "                # converting labels to Long to avoid error \"not implemented for Int\"\n",
    "                labels = labels.long()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images, sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "                _, predicted_train = torch.max(outputs.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted_train == labels).sum().item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                self._optimizer.step()\n",
    "            \n",
    "            toc = time.perf_counter()\n",
    "            time_taken = toc - tic\n",
    "            \n",
    "            epoch_train_loss /= len(train_loader.dataset)\n",
    "            train_accuracy = correct_train / total_train\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            # Evaluation of the model\n",
    "            model.eval()\n",
    "            total, correct = 0, 0\n",
    "            \n",
    "            for (images, sequences), labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                sequences = sequences.to(device)\n",
    "                labels = labels.to(device)\n",
    "        \n",
    "                outputs = model(images, sequences)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            #\n",
    "            val_accuracy = correct / total\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            if (epoch + 1) % 1 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{self.num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}, Iter Time: {time_taken:.2f}s\")\n",
    "                \n",
    "            if val_accuracy > best_val_acc:\n",
    "                best_val_acc = val_accuracy\n",
    "                epochs_no_imp = 0\n",
    "                best_model_state = model.state_dict()  # Save the best model\n",
    "            else:\n",
    "                epochs_no_imp += 1\n",
    "            if epochs_no_imp >= self.patience:\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "                model.load_state_dict(best_model_state)  # Load the best model\n",
    "                break\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        argnames=[\"images\", \"sequences\"]\n",
    "        fin_dict = {}\n",
    "        # create the list with each of the ith range tuples\n",
    "        for i in range(len(X[0])-1):\n",
    "            fin_dict[argnames[i]] = [torch.tensor(t[i]) for t in dataset]\n",
    "\n",
    "        # torch.stack each one of the lists\n",
    "        for key in fin_dict.keys():\n",
    "            fin_dict[key] = torch.stack(fin_dict[key]).to(device)\n",
    "        \n",
    "        images = [tup[0] for tup in X]\n",
    "        sequences = [tup[1] for tup in X]\n",
    "        images_torch, sequences_torch = torch.tensor(np.array(images)).to(device), torch.tensor(np.array(sequences)).to(device)\n",
    "        # model specifying\n",
    "        model = self.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(images_torch, sequences_torch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        pred = []\n",
    "        # phrase = predicted.tolist()\n",
    "        # for i in range(len(phrase)):\n",
    "        #     pred.append(self.keys[phrase[i]])\n",
    "        # \n",
    "        # pred_df = pd.DataFrame(pred)\n",
    "        # return np.squeeze(pred_df.to_numpy().T)\n",
    "        return predicted.tolist()"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "id": "817ee4d2ff56f788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T02:32:13.354107Z",
     "start_time": "2025-01-28T02:32:13.344620Z"
    }
   },
   "source": [
    "from coatnet import CoAtNet as CoAtNetImp\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CyclicLR\n",
    "\n",
    "num_blocks = [2, 2, 3, 5, 2]            # L\n",
    "channels = [64, 96, 192, 384, 768]      # D\n",
    "\n",
    "class CoAtNet(nn.Module, BaseEstimator):\n",
    "    def __init__(self, lr=1e-5, num_epochs=500, patience=50, keys='1234567890QWERTYUIOPASDFGHJKLZXCVBNM'):\n",
    "        super(CoAtNet, self).__init__()    \n",
    "        self.keys = keys\n",
    "        self.model = CoAtNetImp((64, 64), 1, num_blocks, channels, num_classes=len(self.keys))\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def fit(self, X, y, model_name, batch_size=16, lr=5e-6, random_state=42):\n",
    "        # concatenate so it has the same shape as before\n",
    "        dataset = [(X[i], y[i]) for i in range(np.array(X).shape[0])]\n",
    "        # dataset = np.concatenate((X, y), axis=1)\n",
    "        train_set, val_set = train_test_split(dataset, test_size=0.03, random_state=random_state, shuffle=True)\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(SEED)\n",
    "        train_loader, val_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, generator=g), DataLoader(val_set, batch_size=batch_size, shuffle=True, generator=g)\n",
    "\n",
    "        # Initialize model, optimizer, and loss function\n",
    "        self._optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "        # Add learning rate scheduler\n",
    "        # Add OneCycleLR scheduler which often works well with Adam\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "            self._optimizer,\n",
    "            gamma=0.97  # Multiply LR by 0.95 each epoch\n",
    "        )\n",
    "        model = self.model.to(device)\n",
    "        \n",
    "        # loss criterion\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_acc, epochs_no_imp = 0, 0\n",
    "        train_accuracies, val_accuracies = [], []\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            model.train()\n",
    "            epoch_train_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            tic = time.perf_counter()\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                augmented_images = spec_augment_pytorch.spec_augment(mel_spectrogram=images)\n",
    "                images = augmented_images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                self._optimizer.zero_grad()\n",
    "        \n",
    "                # converting labels to Long to avoid error \"not implemented for Int\"\n",
    "                labels = labels.long()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "                _, predicted_train = torch.max(outputs.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted_train == labels).sum().item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Add gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                                            \n",
    "                self._optimizer.step()\n",
    "                \n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            # scheduler.step()\n",
    "            toc = time.perf_counter()\n",
    "            time_taken = toc - tic\n",
    "            \n",
    "            epoch_train_loss /= len(train_loader.dataset)\n",
    "            train_accuracy = correct_train / total_train\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            # Evaluation of the model\n",
    "            model.eval()\n",
    "            total, correct = 0, 0\n",
    "            \n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "        \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            val_accuracy = correct / total\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            if (epoch + 1) % 1 == 0 or epoch == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{self.num_epochs}], Train Loss: {epoch_train_loss:.3f}, Train Accuracy: {train_accuracy:.3f}, Val Accuracy: {val_accuracy:.3f} LR: {scheduler.get_last_lr()[0]:.7f} Iter Time: {time_taken:.2f}s\")\n",
    "            if val_accuracy > best_val_acc:\n",
    "                best_val_acc = val_accuracy\n",
    "                epochs_no_imp = 0\n",
    "                best_model_state = model.state_dict()  # Save the best model\n",
    "            else:\n",
    "                epochs_no_imp += 1\n",
    "            if epochs_no_imp >= self.patience:\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "                model.load_state_dict(best_model_state)  # Load the best model\n",
    "                break\n",
    "            \n",
    "        torch.save(self.model.state_dict(), f'models/{model_name}.pth')\n",
    "        #     # Plot accuracy curves\n",
    "        # plt.plot(range(1, self.num_epochs+1), train_accuracies, label='Training Accuracy')\n",
    "        # plt.plot(range(1, self.num_epochs+1), val_accuracies, label='Validation Accuracy')\n",
    "        # plt.xlabel('Epoch')\n",
    "        # plt.ylabel('Accuracy')\n",
    "        # plt.title('Accuracy vs Epoch')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        argnames=[\"x\"]\n",
    "        fin_dict = {}\n",
    "        # create the list with each of the ith range tuples\n",
    "        # print(range(len(X[0])-1))\n",
    "        # for i in range(len(X[0])-1):\n",
    "        #     fin_dict[argnames[i]] = [t[i] for t in dataset]\n",
    "        #     \n",
    "        # # torch.stack each one of the lists\n",
    "        # for key in fin_dict.keys():\n",
    "        #     fin_dict[key] = torch.stack(fin_dict[key]).to(device)\n",
    "        \n",
    "        X = torch.tensor(np.array(X)).to(device)\n",
    "        \n",
    "        # model specifying\n",
    "        model = self.model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        pred = []\n",
    "        # phrase = predicted.tolist()\n",
    "        # for i in range(len(phrase)):\n",
    "        #     pred.append(self.keys[phrase[i]])\n",
    "        # \n",
    "        # pred_df = pd.DataFrame(pred)\n",
    "        # return np.squeeze(pred_df.to_numpy().T)\n",
    "        return predicted.tolist()"
   ],
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T02:32:15.318829Z",
     "start_time": "2025-01-28T02:32:15.317024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def getIndCurrKeys(ind: int):\n",
    "    return curr_keys[ind]"
   ],
   "id": "44ed2a512289bde3",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T02:32:15.596320Z",
     "start_time": "2025-01-28T02:32:15.588069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ],
   "id": "c6e08104a3dc0684",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117e04d70>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T02:40:13.440119Z",
     "start_time": "2025-01-28T02:40:13.434027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from specAugment.sparse_image_warp_zcaceres import sparse_image_warp\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "\n",
    "def time_warp(spec, W=5):\n",
    "    num_rows = spec.shape[0]\n",
    "    spec_len = spec.shape[1]\n",
    "    y = num_rows // 2\n",
    "    horizontal_line_at_ctr = spec[:, y, :]\n",
    "    assert horizontal_line_at_ctr.shape[1] == spec_len\n",
    "\n",
    "    point_to_warp = torch.Tensor(horizontal_line_at_ctr[0, random.randrange(W, spec_len - W)])\n",
    "    assert isinstance(point_to_warp, torch.Tensor)\n",
    "\n",
    "    # Uniform distribution from (0,W) with chance to be up to W negative\n",
    "    dist_to_warp = random.randrange(-W, W)\n",
    "    src_pts, dest_pts = torch.tensor([[[y, point_to_warp]]]), torch.tensor([[[y, point_to_warp + dist_to_warp]]])\n",
    "    warped_spectro, dense_flows = sparse_image_warp(spec, src_pts, dest_pts)\n",
    "    return warped_spectro.squeeze(3)\n",
    "\n",
    "\n",
    "def spec_augment(mel_spectrogram, time_warping_para=80, frequency_masking_para=27,\n",
    "                 time_masking_para=100, frequency_mask_num=1, time_mask_num=1):\n",
    "    \"\"\"Spec augmentation Calculation Function.\n",
    "\n",
    "    'SpecAugment' have 3 steps for audio data augmentation.\n",
    "    first step is time warping using Tensorflow's image_sparse_warp function.\n",
    "    Second step is frequency masking, last step is time masking.\n",
    "\n",
    "    # Arguments:\n",
    "      mel_spectrogram(numpy array): audio file path of you want to warping and masking.\n",
    "      time_warping_para(float): Augmentation parameter, \"time warp parameter W\".\n",
    "        If none, default = 80 for LibriSpeech.\n",
    "      frequency_masking_para(float): Augmentation parameter, \"frequency mask parameter F\"\n",
    "        If none, default = 100 for LibriSpeech.\n",
    "      time_masking_para(float): Augmentation parameter, \"time mask parameter T\"\n",
    "        If none, default = 27 for LibriSpeech.\n",
    "      frequency_mask_num(float): number of frequency masking lines, \"m_F\".\n",
    "        If none, default = 1 for LibriSpeech.\n",
    "      time_mask_num(float): number of time masking lines, \"m_T\".\n",
    "        If none, default = 1 for LibriSpeech.\n",
    "\n",
    "    # Returns\n",
    "      mel_spectrogram(numpy array): warped and masked mel spectrogram.\n",
    "    \"\"\"\n",
    "    v = mel_spectrogram.shape[0]\n",
    "    tau = mel_spectrogram.shape[1]\n",
    " \n",
    "    # Step 1 : Time warping (TO DO...)\n",
    "    # warped_mel_spectrogram = time_warp(mel_spectrogram)\n",
    "    warped_mel_spectrogram = mel_spectrogram\n",
    "\n",
    "    # Step 2 : Frequency masking\n",
    "    for i in range(frequency_mask_num):\n",
    "        f = np.random.uniform(low=0.0, high=frequency_masking_para)\n",
    "        f = int(f)\n",
    "        f0 = random.randint(0, v - f)\n",
    "        warped_mel_spectrogram[f0:f0 + f, :] = 0\n",
    "\n",
    "    # Step 3 : Time masking\n",
    "    for i in range(time_mask_num):\n",
    "        t = np.random.uniform(low=0.0, high=time_masking_para)\n",
    "        t = int(t)\n",
    "        t0 = random.randint(0, tau - t)\n",
    "        warped_mel_spectrogram[:, t0:t0 + t] = 0\n",
    "\n",
    "    return warped_mel_spectrogram\n",
    "\n",
    "\n",
    "def visualization_spectrogram(mel_spectrogram, title):\n",
    "    \"\"\"visualizing result of SpecAugment\n",
    "\n",
    "    # Arguments:\n",
    "      mel_spectrogram(ndarray): mel_spectrogram to visualize.\n",
    "      title(String): plot figure's title\n",
    "    \"\"\"\n",
    "    # Show mel-spectrogram using librosa's specshow.\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(librosa.power_to_db(mel_spectrogram, ref=np.max), y_axis='mel', fmax=8000, x_axis='time')\n",
    "    # plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "d263379d3882d7a4",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T02:40:18.131826Z",
     "start_time": "2025-01-28T02:40:18.027269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dataset = audio_samples\n",
    "dataset_labels = labels_no_masking\n",
    "train_set, test_set, labels_train_set, labels_test_set = train_test_split(dataset, dataset_labels, test_size=0.1, random_state=SEED, shuffle=True)\n",
    "final_train_set = []\n",
    "coatnet = True\n",
    "\n",
    "if coatnet:\n",
    "    model = CoAtNet(lr=1e-4, keys=curr_keys)\n",
    "    for i in range(len(train_set)):\n",
    "        transformed_sample = transform(train_set[i])\n",
    "        spec=transformed_sample\n",
    "        print(spec.shape)\n",
    "        print(f'V: {transformed_sample.shape[0]}')\n",
    "        print(f'tau: {transformed_sample.shape[1]}')\n",
    "        num_rows = spec.shape[0]\n",
    "        spec_len = spec.shape[1]\n",
    "        y = num_rows // 2\n",
    "        horizontal_line_at_ctr = spec[:,y]\n",
    "        print(f'len(horizontal_line_at_ctr): {len(horizontal_line_at_ctr)}')\n",
    "        print(f'spec_len: {spec_len}')\n",
    "        augmented_spectrogram = spec_augment(mel_spectrogram=transformed_sample)\n",
    "        final_train_set.append((augmented_spectrogram, labels_train_set[i]))\n",
    "        # final_train_set.append((transformed_sample, labels_train_set[i]))\n",
    "        # transformed_sample_ts = transform(time_shift(train_set[i]))\n",
    "        # append to final train set\n",
    "        # final_train_set.append((transformed_sample, labels_train_set[i]))\n",
    "        # final_train_set.append((transformed_sample_ts, labels_train_set[i]))\n",
    "        # final_train_set.append((masking(transformed_sample), labels_train_set[i]))\n",
    "        # final_train_set.append((masking(transformed_sample), labels_train_set[i]))\n",
    "        # final_train_set.append((masking(transformed_sample_ts), labels_train_set[i]))\n",
    "        # final_train_set.append((masking(transformed_sample_ts), labels_train_set[i]))\n",
    "    #   Copy final train set to iterate over it\n",
    "    X_train = [t[0] for t in final_train_set]\n",
    "    y_train = [t[1] for t in final_train_set]\n",
    "    print(f'LEN FINAL TRAIN SET: {len(final_train_set)}')\n",
    "else:\n",
    "    model = MfccLSTM()\n",
    "    for i in range(len(train_set)):\n",
    "        transformed_mfcc = transform_mfcc(train_set[i])\n",
    "        transformed_sample = transform(train_set[i])\n",
    "        final_train_set.append((transformed_sample, transformed_mfcc, labels_train_set[i]))\n",
    "        final_train_set.append((masking(transformed_sample), transformed_mfcc, labels_train_set[i]))\n",
    "    X_train = [(t[0],t[1]) for t in final_train_set]\n",
    "    y_train = [t[2] for t in final_train_set]\n",
    "\n",
    "param_grid = {\n",
    "    'patience': [75],\n",
    "    'lr': [1e-4, 1e-5],\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted', zero_division=0.0),\n",
    "    'precision_weighted': make_scorer(precision_score, average='weighted', zero_division=0.0),\n",
    "    'recall_weighted': make_scorer(recall_score, average='weighted', zero_division=0.0),\n",
    "}\n",
    "\n",
    "# grid_search = GridSearchCV(CoAtNet(), param_grid, cv=6, scoring=scoring, refit=False, verbose=3)\n",
    "# fit_params = {\"model_name\": 'grid_search_model_22-01-25', \"batch_size\": 16}\n",
    "# grid_search.fit(X_train, y_train, **fit_params)\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "formatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M\")\n",
    "print(\"Formatted Date and Time:\", formatted_datetime)\n",
    "batch_size = 16\n",
    "lr = 1e-6\n",
    "model_name = f'model_{batch_size}_{lr}_{formatted_datetime}'\n",
    "model.fit(X_train, y_train, model_name, batch_size, lr)\n",
    "\n",
    "# # # # Load the existing checkpoint\n",
    "# # checkpoint = torch.load(\"models/13-01-24.pth\", weights_only=True)\n",
    "# #\n",
    "# # # Rename the keys\n",
    "# # new_checkpoint = {}\n",
    "# # for k, v in checkpoint.items():\n",
    "# #     new_checkpoint[f\"model.{k}\"] = v\n",
    "# #\n",
    "# # model.load_state_dict(new_checkpoint, strict=True)\n",
    "# # model.load_state_dict(torch.load(\"models/13-01-24.pth\", weights_only=True), strict=True)\n",
    "#\n",
    "final_test_set = list(map(transform, test_set))\n",
    "print(final_test_set[0].shape)\n",
    "prediction = model.predict(final_test_set)\n",
    "np_prediction = np.array(prediction)\n",
    "accuracy = accuracy_score(labels_test_set, np_prediction)\n",
    "print(f'Final Accuracy: {accuracy:.3f}')\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(labels_test_set, np_prediction, average='weighted')\n",
    "recall = recall_score(labels_test_set, np_prediction, average='weighted')\n",
    "f1 = f1_score(labels_test_set, np_prediction, average='weighted')\n",
    "\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')"
   ],
   "id": "582f0155d3d48bef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64, 64)\n",
      "V: 1\n",
      "tau: 64\n",
      "len(horizontal_line_at_ctr): 1\n",
      "spec_len: 64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty range for randrange() (0, -23, -23)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[122], line 23\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlen(horizontal_line_at_ctr): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(horizontal_line_at_ctr)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspec_len: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mspec_len\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 23\u001B[0m     augmented_spectrogram \u001B[38;5;241m=\u001B[39m spec_augment(mel_spectrogram\u001B[38;5;241m=\u001B[39mtransformed_sample)\n\u001B[1;32m     24\u001B[0m     final_train_set\u001B[38;5;241m.\u001B[39mappend((augmented_spectrogram, labels_train_set[i]))\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;66;03m# final_train_set.append((transformed_sample, labels_train_set[i]))\u001B[39;00m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;66;03m# transformed_sample_ts = transform(time_shift(train_set[i]))\u001B[39;00m\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;66;03m# append to final train set\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;66;03m# final_train_set.append((masking(transformed_sample_ts), labels_train_set[i]))\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m#   Copy final train set to iterate over it\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[121], line 61\u001B[0m, in \u001B[0;36mspec_augment\u001B[0;34m(mel_spectrogram, time_warping_para, frequency_masking_para, time_masking_para, frequency_mask_num, time_mask_num)\u001B[0m\n\u001B[1;32m     59\u001B[0m     f \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39muniform(low\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m, high\u001B[38;5;241m=\u001B[39mfrequency_masking_para)\n\u001B[1;32m     60\u001B[0m     f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(f)\n\u001B[0;32m---> 61\u001B[0m     f0 \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m, v \u001B[38;5;241m-\u001B[39m f)\n\u001B[1;32m     62\u001B[0m     warped_mel_spectrogram[f0:f0 \u001B[38;5;241m+\u001B[39m f, :] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# Step 3 : Time masking\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/random.py:362\u001B[0m, in \u001B[0;36mRandom.randint\u001B[0;34m(self, a, b)\u001B[0m\n\u001B[1;32m    358\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrandint\u001B[39m(\u001B[38;5;28mself\u001B[39m, a, b):\n\u001B[1;32m    359\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return random integer in range [a, b], including both end points.\u001B[39;00m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 362\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandrange(a, b\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/random.py:345\u001B[0m, in \u001B[0;36mRandom.randrange\u001B[0;34m(self, start, stop, step)\u001B[0m\n\u001B[1;32m    343\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m width \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    344\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m istart \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_randbelow(width)\n\u001B[0;32m--> 345\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mempty range for randrange() (\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (istart, istop, width))\n\u001B[1;32m    347\u001B[0m \u001B[38;5;66;03m# Non-unit step argument supplied.\u001B[39;00m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m istep \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[0;31mValueError\u001B[0m: empty range for randrange() (0, -23, -23)"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T11:46:54.155487Z",
     "start_time": "2025-01-27T11:46:53.981599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    " \n",
    "sorted_df = cv_results_df.sort_values(by=['rank_test_accuracy'])\n",
    "\n",
    "for ind, row in sorted_df.iterrows():\n",
    "    print(f'Rank: {row[\"rank_test_accuracy\"]}')\n",
    "    print(f'Params: {row[\"params\"]}')\n",
    "    print(f'Test accuracy: {row[\"mean_test_accuracy\"]:.3f}', end=\" / \")\n",
    "    print(f'F1 Weighted: {row[\"mean_test_f1_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Recall Weighted: {row[\"mean_test_recall_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Precision Weighted: {row[\"mean_test_precision_weighted\"]:.3f}', end=\"\\n\\n\")"
   ],
   "id": "7abd3d77de634e98",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m cv_results_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(grid_search\u001B[38;5;241m.\u001B[39mcv_results_)\n\u001B[1;32m      3\u001B[0m sorted_df \u001B[38;5;241m=\u001B[39m cv_results_df\u001B[38;5;241m.\u001B[39msort_values(by\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrank_test_accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ind, row \u001B[38;5;129;01min\u001B[39;00m sorted_df\u001B[38;5;241m.\u001B[39miterrows():\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:26:34.422348Z",
     "start_time": "2025-01-24T21:26:33.032075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_test_set = list(map(transform, test_set))\n",
    "print(final_test_set[0].shape)\n",
    "prediction = model.predict(final_test_set)\n",
    "np_prediction = np.array(prediction)\n",
    "accuracy = accuracy_score(labels_test_set, np_prediction)\n",
    "print(f'Final Accuracy: {accuracy:.3f}')\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(labels_test_set, np_prediction, average='weighted')\n",
    "recall = recall_score(labels_test_set, np_prediction, average='weighted')\n",
    "f1 = f1_score(labels_test_set, np_prediction, average='weighted')\n",
    "\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')"
   ],
   "id": "6a6a1addf467f062",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64])\n",
      "Final Accuracy: 0.000\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1 Score: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7ef7b7b8c02a010e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "df452b6574d7ca53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_set, test_set, labels_train_set, labels_test_set",
   "id": "cdb0b9b7fca4bd2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T03:06:15.014457Z",
     "start_time": "2025-01-20T03:06:11.152854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wave\n",
    "\n",
    "output_wav_file = 'new_output_19-01-24.wav'\n",
    "\n",
    "while True:\n",
    "    word = input(\"\\nIntroduce la palabra:\")\n",
    "    if word == 'exit':\n",
    "        break\n",
    "    word = word.upper()\n",
    "    curr_word, curr_raw_word, curr_labels = [], [], []\n",
    "\n",
    "    for letter in word:\n",
    "        letter_index = curr_keys.index(letter)\n",
    "        # Convert numpy array to list\n",
    "        labels_test_set_list = labels_test_set.tolist()\n",
    "        \n",
    "        # Get the index of the first occurrence\n",
    "        try:\n",
    "            final_index = labels_test_set_list.index(letter_index)\n",
    "            curr_word.append(transform(test_set[final_index]))\n",
    "            curr_raw_word.append(test_set[final_index])\n",
    "            random_array = np.random.uniform(1e-10, 1e-09, 300)\n",
    "            curr_raw_word.append(random_array)\n",
    "        except ValueError:\n",
    "            print(f'letter {letter} not found')\n",
    "            final_index = None  # Handle the case when the value is not found\n",
    "    \n",
    "    final_audio = np.concatenate(curr_raw_word, axis=0)\n",
    "    print(np.mean(final_audio))\n",
    "    \n",
    "    with wave.open(output_wav_file, 'w') as wav_file:\n",
    "        n_channels = 2\n",
    "        sampwidth = 4\n",
    "        sample_rate = 44100\n",
    "        \n",
    "        wav_file.setnchannels(2)\n",
    "        wav_file.setsampwidth(4)\n",
    "        wav_file.setframerate(sample_rate)\n",
    "        \n",
    "        wav_file.writeframes(final_audio)\n",
    "        # for letter in curr_word:\n",
    "        #     # wav_file.writeframesraw(letter.numpy())\n",
    "        #     ww\n",
    "            # print(f'curr_letter length: {len(letter)} / num of frames of file: {wav_file.getnframes()}')"
   ],
   "id": "d0d3579db4fa2ab3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.3600332760636145e-06\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "e",
   "id": "57deb2613ace7a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:27:40.628310Z",
     "start_time": "2025-01-21T02:27:38.565707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "print(sorted(labels_test_set))\n",
    "while True:\n",
    "    word = input(\"\\nIntroduce la palabra:\")\n",
    "    if word == 'exit':\n",
    "        break\n",
    "    word = word.upper()\n",
    "    curr_word, curr_labels = [], []\n",
    "\n",
    "    for letter in word:\n",
    "        letter_index = curr_keys.index(letter)\n",
    "        # Convert numpy array to list\n",
    "        labels_test_set_list = labels_test_set.tolist()\n",
    "        \n",
    "        # Get the index of the first occurrence\n",
    "        try:\n",
    "            final_index = labels_test_set_list.index(letter_index)\n",
    "        except ValueError:\n",
    "            print(f'letter {letter} not found')\n",
    "            final_index = None  # Handle the case when the value is not found\n",
    "        # append to curr_word the value in that index of X_test\n",
    "        curr_word.append(transform(test_set[final_index]))\n",
    "        curr_labels.append(labels_test_set[final_index])\n",
    "    print(\"curr_word[0].shape\")\n",
    "    print(curr_word[0].shape)\n",
    "    model.eval()\n",
    "    prediction = model.predict(curr_word)\n",
    "    prediction_list = list(map(getIndCurrKeys, prediction)) \n",
    "    print(f'prediction: {prediction_list}')\n",
    "    print(f'real labels: {list(map(getIndCurrKeys, curr_labels))}')\n",
    "    \n",
    "    response = ollama.chat(model='spanishSpellchecker', messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': ''.join(prediction_list)\n",
    "      },\n",
    "    ])\n",
    "    print(response['message']['content'])\n",
    "    # time.sleep(3)"
   ],
   "id": "9557487afa5294ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 27]\n",
      "letter H not found\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 22\u001B[0m\n\u001B[1;32m     20\u001B[0m         final_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# Handle the case when the value is not found\u001B[39;00m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;66;03m# append to curr_word the value in that index of X_test\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m     curr_word\u001B[38;5;241m.\u001B[39mappend(transform(test_set[final_index]))\n\u001B[1;32m     23\u001B[0m     curr_labels\u001B[38;5;241m.\u001B[39mappend(labels_test_set[final_index])\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcurr_word[0].shape\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not NoneType"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T00:48:16.181135Z",
     "start_time": "2024-11-04T00:48:12.503783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "type(dataset)\n",
    "import ollama\n",
    "\n",
    "samples, sr = librosa.load('harmony.wav')\n",
    "samples = nr.reduce_noise(samples, sr=44100)\n",
    "peaks_count = count_peaks(samples, key_length, True)\n",
    "N_FFT, HOP_LENGTH, BEFORE, AFTER, prom = 1024, 225, 2400, 12000, 0.2391\n",
    "strokes = isolator(samples, sr, N_FFT, HOP_LENGTH, BEFORE, AFTER, prom, False)[1]\n",
    "num_keys = len(strokes)\n",
    "count = 0\n",
    "k = prom\n",
    "prev_k = prom\n",
    "curr_step = 0.01\n",
    "print(f'num_keys: {num_keys} // peaks_count: {peaks_count} // prom: {prom}')\n",
    "while num_keys != peaks_count:\n",
    "    if num_keys > peaks_count:\n",
    "        if count > 0 and prev_k == k + curr_step:\n",
    "            curr_step /= 2\n",
    "        elif count > 0:\n",
    "            curr_step += (curr_step / 2)\n",
    "        prev_k = k\n",
    "        k += curr_step\n",
    "    else:\n",
    "        if count > 0 and prev_k == k - curr_step:\n",
    "            curr_step /= 2\n",
    "        elif count > 0:\n",
    "            curr_step += (curr_step / 2)\n",
    "        prev_k = k\n",
    "        k += -curr_step\n",
    "    strokes = isolator(samples, sr, N_FFT, HOP_LENGTH, BEFORE, AFTER, k, True)[1]\n",
    "    num_keys = len(strokes)\n",
    "    # print(f'actual k: {k:.7f} // num strokes: {num_keys}')\n",
    "    # time.sleep(1)\n",
    "    count += 1\n",
    "\n",
    "model.eval()\n",
    "final_word = []\n",
    "\n",
    "for i in range(len(strokes)):\n",
    "    transformed_sample = transform(strokes[i])\n",
    "    final_word.append(transformed_sample)\n",
    "\n",
    "print(len(final_word))\n",
    "prediction = model.predict(final_word)\n",
    "prediction_list = list(map(getIndCurrKeys, prediction)) \n",
    "print(f'prediction: {prediction_list}')\n",
    "\n",
    "response = ollama.chat(model='spellchecker', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': ''.join(prediction_list)\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ],
   "id": "11882f601dfa1178",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_keys: 1 // peaks_count: 7 // prom: 0.2391\n",
      "7\n",
      "prediction: ['G', 'G', 'G', 'G', 'G', 'G', 'G']\n",
      "Five G's are good.\n",
      "Five G's are great.\n",
      "Five G's are grand.\n",
      "Five G's are game.\n",
      "Five G's are gay.\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T21:07:32.488912Z",
     "start_time": "2024-10-31T16:55:30.063428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "N_SPLITS = 5\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True)\n",
    "fold_results = []\n",
    "accuracies, recalls, precisions, f1_scores = [], [], [], []\n",
    "\n",
    "dataset = audio_samples_new\n",
    "coatnet = True\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)): \n",
    "    print(f'Fold {fold+1}/{N_SPLITS}')\n",
    "    train_set = Subset(dataset, train_idx)\n",
    "    labels_train_set = Subset(labels, train_idx)\n",
    "    test_set = Subset(dataset, val_idx)\n",
    "    labels_test_set = Subset(labels, val_idx)\n",
    "    final_train_set = []\n",
    "    if coatnet:\n",
    "        model = CoAtNet(keys=curr_keys)\n",
    "        # # for i in range(len(train_set)):\n",
    "        #     transformed_sample = transform(train_set[i])\n",
    "        #     final_train_set.append((transformed_sample, labels_train_set[i]))\n",
    "        #     final_train_set.append((masking(transformed_sample), labels_train_set[i]))\n",
    "        #     X_train = [t[0] for t in final_train_set]\n",
    "        #     y_train = [t[1] for t in final_train_set]\n",
    "        for i in range(len(train_set)):\n",
    "            transformed_sample = transform(dataset[i])\n",
    "            final_train_set.append((transformed_sample, labels_train_set[i]))\n",
    "            final_train_set.append((masking(transformed_sample), labels_train_set[i]))\n",
    "        X_train = [t[0] for t in final_train_set]\n",
    "        y_train = [t[1] for t in final_train_set]\n",
    "        print(len(final_train_set))\n",
    "    else:\n",
    "        model = MfccLSTM()\n",
    "        for i in range(len(train_set)):\n",
    "            transformed_mfcc = transform_mfcc(train_set[i])\n",
    "            transformed_sample = transform(train_set[i])\n",
    "            final_train_set.append((transformed_sample, transformed_mfcc, labels_train_set[i]))\n",
    "            final_train_set.append((masking(transformed_sample), transformed_mfcc, labels_train_set[i]))\n",
    "            X_train = [(t[0],t[1]) for t in final_train_set]\n",
    "            y_train = [t[2] for t in final_train_set]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    final_test_set = []\n",
    "    test_set = Subset(dataset, val_idx)\n",
    "    labels_test_set = Subset(labels, val_idx)\n",
    "    for i in range(len(test_set)):\n",
    "        transformed_sample = transform(test_set[i])\n",
    "        final_test_set.append((transformed_sample, labels_test_set[i]))\n",
    "    X_test = [t[0] for t in final_test_set]\n",
    "    y_test = [t[1] for t in final_test_set]\n",
    "    \n",
    "    word = 'abnormalization'\n",
    "    word = word.upper()\n",
    "    curr_word = []\n",
    "    curr_labels = []\n",
    "    for letter in word:\n",
    "        letter_index = curr_keys.index(letter)\n",
    "        # find first index that is equal to letter index in y_test\n",
    "        final_index = y_test.index(letter_index)\n",
    "        # append to curr_word the value in that index of X_test\n",
    "        curr_word.append(X_test[final_index])\n",
    "        curr_labels.append(y_test[final_index])\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    prediction = model.predict(curr_word)\n",
    "    print(f'prediction: {list(map(getIndCurrKeys, prediction))}')\n",
    "    print(f'real labels: {list(map(getIndCurrKeys, curr_labels))}')\n",
    "    \n",
    "    if coatnet:\n",
    "        final_test_set = []\n",
    "        for i in range(len(test_set)):\n",
    "            transformed_sample = transform(test_set[i])\n",
    "            final_test_set.append((transformed_sample, labels_test_set[i]))\n",
    "        X_test = [t[0] for t in final_test_set]\n",
    "        y_test = [t[1] for t in final_test_set]\n",
    "        prediction = model.predict(X_test)\n",
    "    else: \n",
    "        final_test_set = []\n",
    "        for i in range(len(test_set)):\n",
    "            transformed_mfcc = transform_mfcc(test_set[i])\n",
    "            transformed_sample = transform(test_set[i])\n",
    "            final_test_set.append((transformed_sample, transformed_mfcc, labels_test_set[i]))\n",
    "        X_test = [(t[0],t[1]) for t in final_test_set]\n",
    "        y_test = [t[2] for t in final_test_set]\n",
    "        prediction = model.predict(X_test)\n",
    "    \n",
    "    print(f'prediction: {prediction[:20]}')\n",
    "    print(f'true labels: {y_test[:20]}')\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, prediction)\n",
    "    precision = precision_score(y_test, prediction, average='macro')  # For binary classification\n",
    "    recall = recall_score(y_test, prediction, average='macro')\n",
    "    f1 = f1_score(y_test, prediction, average='macro')\n",
    "    \n",
    "    print(f'Fold {fold+1} FINAL RESULTS: ')\n",
    "    print(f'accuracy: {accuracy:.3f} // precision: {precision:.3f} // recall: {recall:.3f} // f1: {f1:.3f}\\n')\n",
    "    accuracies.append(accuracy)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "print(f'\\nAverage accuracy: {np.mean(accuracies):.3f}')\n",
    "print(f'Average precision: {np.mean(precisions):.3f}')\n",
    "print(f'Average recall: {np.mean(recalls):.3f}')\n",
    "print(f'Average f1: {np.mean(f1_scores):.3f}')"
   ],
   "id": "461576e21a0e7175",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "2880\n",
      "Epoch [1/700], Train Loss: 3.5566, Train Accuracy: 0.0512, Val Accuracy: 0.0833, Iter Time: 8.80s\n",
      "Epoch [2/700], Train Loss: 3.1773, Train Accuracy: 0.1615, Val Accuracy: 0.1250, Iter Time: 8.66s\n",
      "Epoch [3/700], Train Loss: 2.8303, Train Accuracy: 0.2800, Val Accuracy: 0.2083, Iter Time: 8.69s\n",
      "Epoch [4/700], Train Loss: 2.4016, Train Accuracy: 0.4057, Val Accuracy: 0.2986, Iter Time: 8.70s\n",
      "Epoch [5/700], Train Loss: 1.9332, Train Accuracy: 0.5344, Val Accuracy: 0.3264, Iter Time: 8.71s\n",
      "Epoch [6/700], Train Loss: 1.4823, Train Accuracy: 0.6520, Val Accuracy: 0.3542, Iter Time: 8.74s\n",
      "Epoch [7/700], Train Loss: 1.0722, Train Accuracy: 0.7745, Val Accuracy: 0.3819, Iter Time: 8.77s\n",
      "Epoch [8/700], Train Loss: 0.7293, Train Accuracy: 0.8735, Val Accuracy: 0.4097, Iter Time: 8.77s\n",
      "Epoch [9/700], Train Loss: 0.4836, Train Accuracy: 0.9306, Val Accuracy: 0.4375, Iter Time: 8.90s\n",
      "Epoch [10/700], Train Loss: 0.3017, Train Accuracy: 0.9686, Val Accuracy: 0.4583, Iter Time: 8.84s\n",
      "Epoch [11/700], Train Loss: 0.1997, Train Accuracy: 0.9817, Val Accuracy: 0.4653, Iter Time: 8.63s\n",
      "Epoch [12/700], Train Loss: 0.1416, Train Accuracy: 0.9872, Val Accuracy: 0.5000, Iter Time: 8.65s\n",
      "Epoch [13/700], Train Loss: 0.1059, Train Accuracy: 0.9868, Val Accuracy: 0.4931, Iter Time: 8.66s\n",
      "Epoch [14/700], Train Loss: 0.0923, Train Accuracy: 0.9883, Val Accuracy: 0.5000, Iter Time: 8.66s\n",
      "Epoch [15/700], Train Loss: 0.0582, Train Accuracy: 0.9923, Val Accuracy: 0.5139, Iter Time: 8.69s\n",
      "Epoch [16/700], Train Loss: 0.0475, Train Accuracy: 0.9952, Val Accuracy: 0.5139, Iter Time: 8.73s\n",
      "Epoch [17/700], Train Loss: 0.0556, Train Accuracy: 0.9905, Val Accuracy: 0.5278, Iter Time: 8.79s\n",
      "Epoch [18/700], Train Loss: 0.0406, Train Accuracy: 0.9960, Val Accuracy: 0.5139, Iter Time: 8.73s\n",
      "Epoch [19/700], Train Loss: 0.0617, Train Accuracy: 0.9890, Val Accuracy: 0.4444, Iter Time: 8.77s\n",
      "Epoch [20/700], Train Loss: 0.0560, Train Accuracy: 0.9898, Val Accuracy: 0.5208, Iter Time: 8.80s\n",
      "Epoch [21/700], Train Loss: 0.0368, Train Accuracy: 0.9931, Val Accuracy: 0.5278, Iter Time: 8.80s\n",
      "Epoch [22/700], Train Loss: 0.0253, Train Accuracy: 0.9952, Val Accuracy: 0.5556, Iter Time: 8.87s\n",
      "Epoch [23/700], Train Loss: 0.0399, Train Accuracy: 0.9923, Val Accuracy: 0.5139, Iter Time: 8.71s\n",
      "Epoch [24/700], Train Loss: 0.0794, Train Accuracy: 0.9817, Val Accuracy: 0.4931, Iter Time: 8.65s\n",
      "Epoch [25/700], Train Loss: 0.1427, Train Accuracy: 0.9635, Val Accuracy: 0.4931, Iter Time: 8.66s\n",
      "Epoch [26/700], Train Loss: 0.1840, Train Accuracy: 0.9466, Val Accuracy: 0.4514, Iter Time: 8.71s\n",
      "Epoch [27/700], Train Loss: 0.0748, Train Accuracy: 0.9806, Val Accuracy: 0.5139, Iter Time: 8.74s\n",
      "Epoch [28/700], Train Loss: 0.0285, Train Accuracy: 0.9956, Val Accuracy: 0.5625, Iter Time: 8.87s\n",
      "Epoch [29/700], Train Loss: 0.0378, Train Accuracy: 0.9898, Val Accuracy: 0.5417, Iter Time: 8.77s\n",
      "Epoch [30/700], Train Loss: 0.0561, Train Accuracy: 0.9883, Val Accuracy: 0.5208, Iter Time: 8.76s\n",
      "Epoch [31/700], Train Loss: 0.0465, Train Accuracy: 0.9872, Val Accuracy: 0.4722, Iter Time: 8.76s\n",
      "Epoch [32/700], Train Loss: 0.0800, Train Accuracy: 0.9773, Val Accuracy: 0.5139, Iter Time: 8.78s\n",
      "Epoch [33/700], Train Loss: 0.0913, Train Accuracy: 0.9766, Val Accuracy: 0.5625, Iter Time: 8.83s\n",
      "Epoch [34/700], Train Loss: 0.1701, Train Accuracy: 0.9463, Val Accuracy: 0.4792, Iter Time: 8.83s\n",
      "Epoch [35/700], Train Loss: 0.1250, Train Accuracy: 0.9627, Val Accuracy: 0.5278, Iter Time: 8.81s\n",
      "Epoch [36/700], Train Loss: 0.0939, Train Accuracy: 0.9730, Val Accuracy: 0.5625, Iter Time: 8.67s\n",
      "Epoch [37/700], Train Loss: 0.1062, Train Accuracy: 0.9704, Val Accuracy: 0.5556, Iter Time: 8.69s\n",
      "Epoch [38/700], Train Loss: 0.1258, Train Accuracy: 0.9642, Val Accuracy: 0.5417, Iter Time: 8.71s\n",
      "Epoch [39/700], Train Loss: 0.0656, Train Accuracy: 0.9770, Val Accuracy: 0.6250, Iter Time: 8.71s\n",
      "Epoch [40/700], Train Loss: 0.0837, Train Accuracy: 0.9781, Val Accuracy: 0.5208, Iter Time: 8.71s\n",
      "Epoch [41/700], Train Loss: 0.0915, Train Accuracy: 0.9744, Val Accuracy: 0.5069, Iter Time: 8.78s\n",
      "Epoch [42/700], Train Loss: 0.1157, Train Accuracy: 0.9653, Val Accuracy: 0.5486, Iter Time: 8.78s\n",
      "Epoch [43/700], Train Loss: 0.1740, Train Accuracy: 0.9485, Val Accuracy: 0.5972, Iter Time: 8.80s\n",
      "Epoch [44/700], Train Loss: 0.1938, Train Accuracy: 0.9466, Val Accuracy: 0.6111, Iter Time: 8.80s\n",
      "Epoch [45/700], Train Loss: 0.1084, Train Accuracy: 0.9671, Val Accuracy: 0.6111, Iter Time: 8.81s\n",
      "Epoch [46/700], Train Loss: 0.0748, Train Accuracy: 0.9773, Val Accuracy: 0.5694, Iter Time: 8.82s\n",
      "Epoch [47/700], Train Loss: 0.0625, Train Accuracy: 0.9795, Val Accuracy: 0.5694, Iter Time: 8.84s\n",
      "Epoch [48/700], Train Loss: 0.1090, Train Accuracy: 0.9700, Val Accuracy: 0.6111, Iter Time: 8.65s\n",
      "Epoch [49/700], Train Loss: 0.1008, Train Accuracy: 0.9697, Val Accuracy: 0.5833, Iter Time: 8.66s\n",
      "Epoch [50/700], Train Loss: 0.1549, Train Accuracy: 0.9514, Val Accuracy: 0.5556, Iter Time: 8.70s\n",
      "Epoch [51/700], Train Loss: 0.2681, Train Accuracy: 0.9178, Val Accuracy: 0.5139, Iter Time: 8.77s\n",
      "Epoch [52/700], Train Loss: 0.1890, Train Accuracy: 0.9415, Val Accuracy: 0.4792, Iter Time: 8.71s\n",
      "Epoch [53/700], Train Loss: 0.1429, Train Accuracy: 0.9543, Val Accuracy: 0.5139, Iter Time: 8.75s\n",
      "Epoch [54/700], Train Loss: 0.1249, Train Accuracy: 0.9631, Val Accuracy: 0.5278, Iter Time: 8.77s\n",
      "Epoch [55/700], Train Loss: 0.1087, Train Accuracy: 0.9653, Val Accuracy: 0.5833, Iter Time: 8.82s\n",
      "Epoch [56/700], Train Loss: 0.1636, Train Accuracy: 0.9507, Val Accuracy: 0.5000, Iter Time: 8.82s\n",
      "Epoch [57/700], Train Loss: 0.1090, Train Accuracy: 0.9693, Val Accuracy: 0.6042, Iter Time: 8.80s\n",
      "Epoch [58/700], Train Loss: 0.0919, Train Accuracy: 0.9708, Val Accuracy: 0.5486, Iter Time: 8.88s\n",
      "Epoch [59/700], Train Loss: 0.2338, Train Accuracy: 0.9338, Val Accuracy: 0.5972, Iter Time: 8.86s\n",
      "Epoch [60/700], Train Loss: 0.1588, Train Accuracy: 0.9536, Val Accuracy: 0.5556, Iter Time: 8.72s\n",
      "Epoch [61/700], Train Loss: 0.1513, Train Accuracy: 0.9558, Val Accuracy: 0.5347, Iter Time: 8.67s\n",
      "Epoch [62/700], Train Loss: 0.1606, Train Accuracy: 0.9539, Val Accuracy: 0.5972, Iter Time: 8.71s\n",
      "Epoch [63/700], Train Loss: 0.2313, Train Accuracy: 0.9338, Val Accuracy: 0.5208, Iter Time: 8.78s\n",
      "Epoch [64/700], Train Loss: 0.1701, Train Accuracy: 0.9448, Val Accuracy: 0.5069, Iter Time: 9.81s\n",
      "Epoch [65/700], Train Loss: 0.1435, Train Accuracy: 0.9529, Val Accuracy: 0.5972, Iter Time: 8.84s\n",
      "Epoch [66/700], Train Loss: 0.1657, Train Accuracy: 0.9547, Val Accuracy: 0.5556, Iter Time: 8.38s\n",
      "Epoch [67/700], Train Loss: 0.1286, Train Accuracy: 0.9642, Val Accuracy: 0.5903, Iter Time: 8.40s\n",
      "Epoch [68/700], Train Loss: 0.1620, Train Accuracy: 0.9510, Val Accuracy: 0.4514, Iter Time: 8.50s\n",
      "Epoch [69/700], Train Loss: 0.2618, Train Accuracy: 0.9214, Val Accuracy: 0.5069, Iter Time: 9.30s\n",
      "Epoch [70/700], Train Loss: 0.2020, Train Accuracy: 0.9404, Val Accuracy: 0.5903, Iter Time: 8.87s\n",
      "Epoch [71/700], Train Loss: 0.2925, Train Accuracy: 0.9152, Val Accuracy: 0.5764, Iter Time: 8.47s\n",
      "Epoch [72/700], Train Loss: 0.1428, Train Accuracy: 0.9591, Val Accuracy: 0.6042, Iter Time: 8.32s\n",
      "Epoch [73/700], Train Loss: 0.0802, Train Accuracy: 0.9781, Val Accuracy: 0.6181, Iter Time: 8.93s\n",
      "Epoch [74/700], Train Loss: 0.0929, Train Accuracy: 0.9755, Val Accuracy: 0.6181, Iter Time: 8.88s\n",
      "Epoch [75/700], Train Loss: 0.2559, Train Accuracy: 0.9295, Val Accuracy: 0.5556, Iter Time: 8.63s\n",
      "Epoch [76/700], Train Loss: 0.3136, Train Accuracy: 0.9108, Val Accuracy: 0.5625, Iter Time: 8.63s\n",
      "Epoch [77/700], Train Loss: 0.2093, Train Accuracy: 0.9382, Val Accuracy: 0.6111, Iter Time: 8.67s\n",
      "Epoch [78/700], Train Loss: 0.2342, Train Accuracy: 0.9258, Val Accuracy: 0.5972, Iter Time: 9.10s\n",
      "Epoch [79/700], Train Loss: 0.2616, Train Accuracy: 0.9251, Val Accuracy: 0.5903, Iter Time: 8.77s\n",
      "Epoch [80/700], Train Loss: 0.1821, Train Accuracy: 0.9474, Val Accuracy: 0.6736, Iter Time: 8.76s\n",
      "Epoch [81/700], Train Loss: 0.1771, Train Accuracy: 0.9499, Val Accuracy: 0.6181, Iter Time: 8.79s\n",
      "Epoch [82/700], Train Loss: 0.1894, Train Accuracy: 0.9481, Val Accuracy: 0.6319, Iter Time: 8.82s\n",
      "Epoch [83/700], Train Loss: 0.2540, Train Accuracy: 0.9269, Val Accuracy: 0.5208, Iter Time: 8.78s\n",
      "Epoch [84/700], Train Loss: 0.2851, Train Accuracy: 0.9137, Val Accuracy: 0.6111, Iter Time: 8.74s\n",
      "Epoch [85/700], Train Loss: 0.2584, Train Accuracy: 0.9258, Val Accuracy: 0.5486, Iter Time: 8.65s\n",
      "Epoch [86/700], Train Loss: 0.2575, Train Accuracy: 0.9273, Val Accuracy: 0.5972, Iter Time: 8.62s\n",
      "Epoch [87/700], Train Loss: 0.1559, Train Accuracy: 0.9554, Val Accuracy: 0.6042, Iter Time: 8.64s\n",
      "Epoch [88/700], Train Loss: 0.1484, Train Accuracy: 0.9671, Val Accuracy: 0.6042, Iter Time: 8.65s\n",
      "Epoch [89/700], Train Loss: 0.1595, Train Accuracy: 0.9605, Val Accuracy: 0.6667, Iter Time: 8.71s\n",
      "Epoch [90/700], Train Loss: 0.1804, Train Accuracy: 0.9485, Val Accuracy: 0.6181, Iter Time: 8.71s\n",
      "Epoch [91/700], Train Loss: 0.2852, Train Accuracy: 0.9229, Val Accuracy: 0.6181, Iter Time: 8.76s\n",
      "Epoch [92/700], Train Loss: 0.2947, Train Accuracy: 0.9196, Val Accuracy: 0.5417, Iter Time: 8.77s\n",
      "Epoch [93/700], Train Loss: 0.3006, Train Accuracy: 0.9090, Val Accuracy: 0.5764, Iter Time: 8.79s\n",
      "Epoch [94/700], Train Loss: 0.2475, Train Accuracy: 0.9276, Val Accuracy: 0.5347, Iter Time: 8.82s\n",
      "Epoch [95/700], Train Loss: 0.2000, Train Accuracy: 0.9521, Val Accuracy: 0.6319, Iter Time: 8.85s\n",
      "Epoch [96/700], Train Loss: 0.2122, Train Accuracy: 0.9415, Val Accuracy: 0.5833, Iter Time: 8.82s\n",
      "Epoch [97/700], Train Loss: 0.2355, Train Accuracy: 0.9317, Val Accuracy: 0.6389, Iter Time: 8.66s\n",
      "Epoch [98/700], Train Loss: 0.2256, Train Accuracy: 0.9371, Val Accuracy: 0.6319, Iter Time: 8.68s\n",
      "Epoch [99/700], Train Loss: 0.1997, Train Accuracy: 0.9452, Val Accuracy: 0.6111, Iter Time: 8.65s\n",
      "Epoch [100/700], Train Loss: 0.1930, Train Accuracy: 0.9448, Val Accuracy: 0.6111, Iter Time: 8.66s\n",
      "Epoch [101/700], Train Loss: 0.3623, Train Accuracy: 0.8999, Val Accuracy: 0.5764, Iter Time: 8.69s\n",
      "Epoch [102/700], Train Loss: 0.3407, Train Accuracy: 0.9010, Val Accuracy: 0.5833, Iter Time: 8.71s\n",
      "Epoch [103/700], Train Loss: 0.1547, Train Accuracy: 0.9565, Val Accuracy: 0.6181, Iter Time: 8.76s\n",
      "Epoch [104/700], Train Loss: 0.1982, Train Accuracy: 0.9510, Val Accuracy: 0.6806, Iter Time: 8.77s\n",
      "Epoch [105/700], Train Loss: 0.2387, Train Accuracy: 0.9393, Val Accuracy: 0.5764, Iter Time: 8.87s\n",
      "Epoch [106/700], Train Loss: 0.2535, Train Accuracy: 0.9342, Val Accuracy: 0.5764, Iter Time: 8.79s\n",
      "Epoch [107/700], Train Loss: 0.2246, Train Accuracy: 0.9364, Val Accuracy: 0.5694, Iter Time: 8.85s\n",
      "Epoch [108/700], Train Loss: 0.1524, Train Accuracy: 0.9605, Val Accuracy: 0.6042, Iter Time: 8.86s\n",
      "Epoch [109/700], Train Loss: 0.2713, Train Accuracy: 0.9324, Val Accuracy: 0.6111, Iter Time: 8.72s\n",
      "Epoch [110/700], Train Loss: 0.2571, Train Accuracy: 0.9309, Val Accuracy: 0.5625, Iter Time: 8.65s\n",
      "Epoch [111/700], Train Loss: 0.3034, Train Accuracy: 0.9141, Val Accuracy: 0.6597, Iter Time: 8.66s\n",
      "Epoch [112/700], Train Loss: 0.2399, Train Accuracy: 0.9313, Val Accuracy: 0.5278, Iter Time: 8.69s\n",
      "Epoch [113/700], Train Loss: 0.3625, Train Accuracy: 0.8991, Val Accuracy: 0.5625, Iter Time: 8.74s\n",
      "Epoch [114/700], Train Loss: 0.3068, Train Accuracy: 0.9211, Val Accuracy: 0.6667, Iter Time: 8.74s\n",
      "Epoch [115/700], Train Loss: 0.2457, Train Accuracy: 0.9317, Val Accuracy: 0.6181, Iter Time: 8.93s\n",
      "Epoch [116/700], Train Loss: 0.1430, Train Accuracy: 0.9561, Val Accuracy: 0.6806, Iter Time: 9.30s\n",
      "Epoch [117/700], Train Loss: 0.1648, Train Accuracy: 0.9496, Val Accuracy: 0.6042, Iter Time: 8.56s\n",
      "Epoch [118/700], Train Loss: 0.2032, Train Accuracy: 0.9430, Val Accuracy: 0.6528, Iter Time: 8.59s\n",
      "Epoch [119/700], Train Loss: 0.2874, Train Accuracy: 0.9240, Val Accuracy: 0.6042, Iter Time: 8.67s\n",
      "Epoch [120/700], Train Loss: 0.3066, Train Accuracy: 0.9170, Val Accuracy: 0.5694, Iter Time: 8.78s\n",
      "Epoch [121/700], Train Loss: 0.3501, Train Accuracy: 0.9013, Val Accuracy: 0.5903, Iter Time: 8.73s\n",
      "Epoch [122/700], Train Loss: 0.2784, Train Accuracy: 0.9141, Val Accuracy: 0.6042, Iter Time: 8.62s\n",
      "Epoch [123/700], Train Loss: 0.2544, Train Accuracy: 0.9240, Val Accuracy: 0.6181, Iter Time: 8.66s\n",
      "Epoch [124/700], Train Loss: 0.2404, Train Accuracy: 0.9346, Val Accuracy: 0.6667, Iter Time: 8.67s\n",
      "Epoch [125/700], Train Loss: 0.1825, Train Accuracy: 0.9503, Val Accuracy: 0.6458, Iter Time: 8.74s\n",
      "Epoch [126/700], Train Loss: 0.2179, Train Accuracy: 0.9492, Val Accuracy: 0.7083, Iter Time: 8.70s\n",
      "Epoch [127/700], Train Loss: 0.2134, Train Accuracy: 0.9408, Val Accuracy: 0.6528, Iter Time: 8.22s\n",
      "Epoch [128/700], Train Loss: 0.2922, Train Accuracy: 0.9258, Val Accuracy: 0.5833, Iter Time: 8.37s\n",
      "Epoch [129/700], Train Loss: 0.3106, Train Accuracy: 0.9203, Val Accuracy: 0.6250, Iter Time: 8.45s\n",
      "Epoch [130/700], Train Loss: 0.2092, Train Accuracy: 0.9477, Val Accuracy: 0.6875, Iter Time: 9.10s\n",
      "Epoch [131/700], Train Loss: 0.2242, Train Accuracy: 0.9448, Val Accuracy: 0.6181, Iter Time: 8.69s\n",
      "Epoch [132/700], Train Loss: 0.2733, Train Accuracy: 0.9295, Val Accuracy: 0.6667, Iter Time: 8.67s\n",
      "Epoch [133/700], Train Loss: 0.2435, Train Accuracy: 0.9386, Val Accuracy: 0.6875, Iter Time: 8.68s\n",
      "Epoch [134/700], Train Loss: 0.3116, Train Accuracy: 0.9159, Val Accuracy: 0.6250, Iter Time: 8.59s\n",
      "Epoch [135/700], Train Loss: 0.2955, Train Accuracy: 0.9200, Val Accuracy: 0.6875, Iter Time: 8.66s\n",
      "Epoch [136/700], Train Loss: 0.2206, Train Accuracy: 0.9466, Val Accuracy: 0.7153, Iter Time: 8.69s\n",
      "Epoch [137/700], Train Loss: 0.2250, Train Accuracy: 0.9401, Val Accuracy: 0.7361, Iter Time: 8.71s\n",
      "Epoch [138/700], Train Loss: 0.2251, Train Accuracy: 0.9452, Val Accuracy: 0.6389, Iter Time: 8.76s\n",
      "Epoch [139/700], Train Loss: 0.2354, Train Accuracy: 0.9353, Val Accuracy: 0.6319, Iter Time: 8.79s\n",
      "Epoch [140/700], Train Loss: 0.2637, Train Accuracy: 0.9284, Val Accuracy: 0.6250, Iter Time: 8.80s\n",
      "Epoch [141/700], Train Loss: 0.3150, Train Accuracy: 0.9148, Val Accuracy: 0.6181, Iter Time: 8.98s\n",
      "Epoch [142/700], Train Loss: 0.3277, Train Accuracy: 0.9185, Val Accuracy: 0.6111, Iter Time: 9.01s\n",
      "Epoch [143/700], Train Loss: 0.2868, Train Accuracy: 0.9247, Val Accuracy: 0.7083, Iter Time: 8.86s\n",
      "Epoch [144/700], Train Loss: 0.2236, Train Accuracy: 0.9437, Val Accuracy: 0.6111, Iter Time: 8.84s\n",
      "Epoch [145/700], Train Loss: 0.2174, Train Accuracy: 0.9419, Val Accuracy: 0.6250, Iter Time: 8.91s\n",
      "Epoch [146/700], Train Loss: 0.2998, Train Accuracy: 0.9159, Val Accuracy: 0.6667, Iter Time: 8.62s\n",
      "Epoch [147/700], Train Loss: 0.2752, Train Accuracy: 0.9309, Val Accuracy: 0.6250, Iter Time: 8.63s\n",
      "Epoch [148/700], Train Loss: 0.2926, Train Accuracy: 0.9262, Val Accuracy: 0.6944, Iter Time: 8.62s\n",
      "Epoch [149/700], Train Loss: 0.1871, Train Accuracy: 0.9492, Val Accuracy: 0.6319, Iter Time: 8.65s\n",
      "Epoch [150/700], Train Loss: 0.2097, Train Accuracy: 0.9452, Val Accuracy: 0.6181, Iter Time: 8.69s\n",
      "Epoch [151/700], Train Loss: 0.2341, Train Accuracy: 0.9481, Val Accuracy: 0.6389, Iter Time: 8.68s\n",
      "Epoch [152/700], Train Loss: 0.3397, Train Accuracy: 0.9123, Val Accuracy: 0.5486, Iter Time: 8.73s\n",
      "Epoch [153/700], Train Loss: 0.4056, Train Accuracy: 0.8900, Val Accuracy: 0.6528, Iter Time: 8.74s\n",
      "Epoch [154/700], Train Loss: 0.2741, Train Accuracy: 0.9280, Val Accuracy: 0.6806, Iter Time: 8.76s\n",
      "Epoch [155/700], Train Loss: 0.2863, Train Accuracy: 0.9211, Val Accuracy: 0.6944, Iter Time: 8.75s\n",
      "Epoch [156/700], Train Loss: 0.2738, Train Accuracy: 0.9349, Val Accuracy: 0.7222, Iter Time: 8.80s\n",
      "Epoch [157/700], Train Loss: 0.1605, Train Accuracy: 0.9532, Val Accuracy: 0.7222, Iter Time: 8.88s\n",
      "Epoch [158/700], Train Loss: 0.1670, Train Accuracy: 0.9561, Val Accuracy: 0.7500, Iter Time: 8.72s\n",
      "Epoch [159/700], Train Loss: 0.1421, Train Accuracy: 0.9675, Val Accuracy: 0.6875, Iter Time: 8.72s\n",
      "Epoch [160/700], Train Loss: 0.1992, Train Accuracy: 0.9529, Val Accuracy: 0.6875, Iter Time: 8.65s\n",
      "Epoch [161/700], Train Loss: 0.2602, Train Accuracy: 0.9342, Val Accuracy: 0.7014, Iter Time: 8.68s\n",
      "Epoch [162/700], Train Loss: 0.2999, Train Accuracy: 0.9170, Val Accuracy: 0.6319, Iter Time: 8.66s\n",
      "Epoch [163/700], Train Loss: 0.3226, Train Accuracy: 0.9203, Val Accuracy: 0.6736, Iter Time: 8.68s\n",
      "Epoch [164/700], Train Loss: 0.3140, Train Accuracy: 0.9240, Val Accuracy: 0.7014, Iter Time: 8.70s\n",
      "Epoch [165/700], Train Loss: 0.2925, Train Accuracy: 0.9243, Val Accuracy: 0.6875, Iter Time: 8.74s\n",
      "Epoch [166/700], Train Loss: 0.4294, Train Accuracy: 0.8893, Val Accuracy: 0.6736, Iter Time: 8.75s\n",
      "Epoch [167/700], Train Loss: 0.2799, Train Accuracy: 0.9298, Val Accuracy: 0.7083, Iter Time: 8.76s\n",
      "Epoch [168/700], Train Loss: 0.2339, Train Accuracy: 0.9423, Val Accuracy: 0.6806, Iter Time: 8.79s\n",
      "Epoch [169/700], Train Loss: 0.1680, Train Accuracy: 0.9572, Val Accuracy: 0.7153, Iter Time: 8.83s\n",
      "Epoch [170/700], Train Loss: 0.2406, Train Accuracy: 0.9433, Val Accuracy: 0.7431, Iter Time: 8.79s\n",
      "Epoch [171/700], Train Loss: 0.1901, Train Accuracy: 0.9492, Val Accuracy: 0.7083, Iter Time: 8.68s\n",
      "Epoch [172/700], Train Loss: 0.2015, Train Accuracy: 0.9503, Val Accuracy: 0.6875, Iter Time: 8.66s\n",
      "Epoch [173/700], Train Loss: 0.2527, Train Accuracy: 0.9382, Val Accuracy: 0.6389, Iter Time: 8.67s\n",
      "Epoch [174/700], Train Loss: 0.3474, Train Accuracy: 0.9211, Val Accuracy: 0.6806, Iter Time: 8.68s\n",
      "Epoch [175/700], Train Loss: 0.2420, Train Accuracy: 0.9455, Val Accuracy: 0.7431, Iter Time: 8.70s\n",
      "Epoch [176/700], Train Loss: 0.1793, Train Accuracy: 0.9576, Val Accuracy: 0.7014, Iter Time: 8.74s\n",
      "Epoch [177/700], Train Loss: 0.3246, Train Accuracy: 0.9243, Val Accuracy: 0.6597, Iter Time: 8.92s\n",
      "Epoch [178/700], Train Loss: 0.2169, Train Accuracy: 0.9488, Val Accuracy: 0.7292, Iter Time: 9.39s\n",
      "Epoch [179/700], Train Loss: 0.2294, Train Accuracy: 0.9448, Val Accuracy: 0.7431, Iter Time: 9.55s\n",
      "Epoch [180/700], Train Loss: 0.3564, Train Accuracy: 0.9097, Val Accuracy: 0.6528, Iter Time: 8.55s\n",
      "Epoch [181/700], Train Loss: 0.2821, Train Accuracy: 0.9295, Val Accuracy: 0.7153, Iter Time: 8.61s\n",
      "Epoch [182/700], Train Loss: 0.2320, Train Accuracy: 0.9455, Val Accuracy: 0.7569, Iter Time: 8.75s\n",
      "Epoch [183/700], Train Loss: 0.2135, Train Accuracy: 0.9525, Val Accuracy: 0.7083, Iter Time: 8.61s\n",
      "Epoch [184/700], Train Loss: 0.2554, Train Accuracy: 0.9390, Val Accuracy: 0.7431, Iter Time: 8.63s\n",
      "Epoch [185/700], Train Loss: 0.2223, Train Accuracy: 0.9466, Val Accuracy: 0.7431, Iter Time: 8.62s\n",
      "Epoch [186/700], Train Loss: 0.1629, Train Accuracy: 0.9656, Val Accuracy: 0.7153, Iter Time: 8.67s\n",
      "Epoch [187/700], Train Loss: 0.2239, Train Accuracy: 0.9412, Val Accuracy: 0.7222, Iter Time: 8.68s\n",
      "Epoch [188/700], Train Loss: 0.2883, Train Accuracy: 0.9324, Val Accuracy: 0.6597, Iter Time: 8.70s\n",
      "Epoch [189/700], Train Loss: 0.2314, Train Accuracy: 0.9419, Val Accuracy: 0.6806, Iter Time: 8.72s\n",
      "Epoch [190/700], Train Loss: 0.2492, Train Accuracy: 0.9437, Val Accuracy: 0.6806, Iter Time: 8.78s\n",
      "Epoch [191/700], Train Loss: 0.2574, Train Accuracy: 0.9397, Val Accuracy: 0.6667, Iter Time: 8.86s\n",
      "Epoch [192/700], Train Loss: 0.2295, Train Accuracy: 0.9499, Val Accuracy: 0.7431, Iter Time: 8.80s\n",
      "Epoch [193/700], Train Loss: 0.2096, Train Accuracy: 0.9550, Val Accuracy: 0.7500, Iter Time: 8.77s\n",
      "Epoch [194/700], Train Loss: 0.2179, Train Accuracy: 0.9426, Val Accuracy: 0.7500, Iter Time: 8.84s\n",
      "Epoch [195/700], Train Loss: 0.1612, Train Accuracy: 0.9550, Val Accuracy: 0.7222, Iter Time: 8.76s\n",
      "Epoch [196/700], Train Loss: 0.2681, Train Accuracy: 0.9455, Val Accuracy: 0.7153, Iter Time: 8.64s\n",
      "Epoch [197/700], Train Loss: 0.2651, Train Accuracy: 0.9404, Val Accuracy: 0.6528, Iter Time: 8.67s\n",
      "Epoch [198/700], Train Loss: 0.2921, Train Accuracy: 0.9327, Val Accuracy: 0.6736, Iter Time: 8.70s\n",
      "Epoch [199/700], Train Loss: 0.2825, Train Accuracy: 0.9364, Val Accuracy: 0.6111, Iter Time: 8.72s\n",
      "Epoch [200/700], Train Loss: 0.2079, Train Accuracy: 0.9518, Val Accuracy: 0.7292, Iter Time: 8.74s\n",
      "Epoch [201/700], Train Loss: 0.1300, Train Accuracy: 0.9715, Val Accuracy: 0.7639, Iter Time: 8.74s\n",
      "Epoch [202/700], Train Loss: 0.1731, Train Accuracy: 0.9583, Val Accuracy: 0.7639, Iter Time: 8.76s\n",
      "Epoch [203/700], Train Loss: 0.1764, Train Accuracy: 0.9613, Val Accuracy: 0.7431, Iter Time: 8.78s\n",
      "Epoch [204/700], Train Loss: 0.2238, Train Accuracy: 0.9477, Val Accuracy: 0.7292, Iter Time: 8.78s\n",
      "Epoch [205/700], Train Loss: 0.2993, Train Accuracy: 0.9360, Val Accuracy: 0.6528, Iter Time: 8.84s\n",
      "Epoch [206/700], Train Loss: 0.3187, Train Accuracy: 0.9203, Val Accuracy: 0.7222, Iter Time: 8.85s\n",
      "Epoch [207/700], Train Loss: 0.2404, Train Accuracy: 0.9393, Val Accuracy: 0.7153, Iter Time: 8.80s\n",
      "Epoch [208/700], Train Loss: 0.2593, Train Accuracy: 0.9423, Val Accuracy: 0.7153, Iter Time: 8.66s\n",
      "Epoch [209/700], Train Loss: 0.2717, Train Accuracy: 0.9353, Val Accuracy: 0.7292, Iter Time: 8.67s\n",
      "Epoch [210/700], Train Loss: 0.1357, Train Accuracy: 0.9686, Val Accuracy: 0.7847, Iter Time: 8.71s\n",
      "Epoch [211/700], Train Loss: 0.1458, Train Accuracy: 0.9656, Val Accuracy: 0.7847, Iter Time: 8.73s\n",
      "Epoch [212/700], Train Loss: 0.1937, Train Accuracy: 0.9554, Val Accuracy: 0.7639, Iter Time: 8.78s\n",
      "Epoch [213/700], Train Loss: 0.1540, Train Accuracy: 0.9664, Val Accuracy: 0.7847, Iter Time: 8.81s\n",
      "Epoch [214/700], Train Loss: 0.2203, Train Accuracy: 0.9525, Val Accuracy: 0.7083, Iter Time: 8.76s\n",
      "Epoch [215/700], Train Loss: 0.2945, Train Accuracy: 0.9298, Val Accuracy: 0.6667, Iter Time: 9.00s\n",
      "Epoch [216/700], Train Loss: 0.2412, Train Accuracy: 0.9393, Val Accuracy: 0.7500, Iter Time: 9.04s\n",
      "Epoch [217/700], Train Loss: 0.2665, Train Accuracy: 0.9382, Val Accuracy: 0.7778, Iter Time: 9.75s\n",
      "Epoch [218/700], Train Loss: 0.1275, Train Accuracy: 0.9730, Val Accuracy: 0.8056, Iter Time: 8.61s\n",
      "Epoch [219/700], Train Loss: 0.1171, Train Accuracy: 0.9759, Val Accuracy: 0.8056, Iter Time: 10.05s\n",
      "Epoch [220/700], Train Loss: 0.1020, Train Accuracy: 0.9788, Val Accuracy: 0.7847, Iter Time: 8.35s\n",
      "Epoch [221/700], Train Loss: 0.2650, Train Accuracy: 0.9452, Val Accuracy: 0.7500, Iter Time: 8.45s\n",
      "Epoch [222/700], Train Loss: 0.1874, Train Accuracy: 0.9543, Val Accuracy: 0.7639, Iter Time: 8.78s\n",
      "Epoch [223/700], Train Loss: 0.2571, Train Accuracy: 0.9433, Val Accuracy: 0.6597, Iter Time: 8.90s\n",
      "Epoch [224/700], Train Loss: 0.2569, Train Accuracy: 0.9415, Val Accuracy: 0.7083, Iter Time: 9.40s\n",
      "Epoch [225/700], Train Loss: 0.2169, Train Accuracy: 0.9452, Val Accuracy: 0.7778, Iter Time: 8.94s\n",
      "Epoch [226/700], Train Loss: 0.2640, Train Accuracy: 0.9452, Val Accuracy: 0.7708, Iter Time: 8.71s\n",
      "Epoch [227/700], Train Loss: 0.1512, Train Accuracy: 0.9660, Val Accuracy: 0.7917, Iter Time: 8.70s\n",
      "Epoch [228/700], Train Loss: 0.1278, Train Accuracy: 0.9733, Val Accuracy: 0.7500, Iter Time: 8.87s\n",
      "Epoch [229/700], Train Loss: 0.1333, Train Accuracy: 0.9704, Val Accuracy: 0.7569, Iter Time: 8.91s\n",
      "Epoch [230/700], Train Loss: 0.0944, Train Accuracy: 0.9792, Val Accuracy: 0.7778, Iter Time: 9.01s\n",
      "Epoch [231/700], Train Loss: 0.1620, Train Accuracy: 0.9627, Val Accuracy: 0.7778, Iter Time: 9.35s\n",
      "Epoch [232/700], Train Loss: 0.3436, Train Accuracy: 0.9236, Val Accuracy: 0.7361, Iter Time: 9.13s\n",
      "Epoch [233/700], Train Loss: 0.4327, Train Accuracy: 0.9046, Val Accuracy: 0.6944, Iter Time: 8.70s\n",
      "Epoch [234/700], Train Loss: 0.2663, Train Accuracy: 0.9423, Val Accuracy: 0.7917, Iter Time: 8.67s\n",
      "Epoch [235/700], Train Loss: 0.2248, Train Accuracy: 0.9529, Val Accuracy: 0.7708, Iter Time: 8.73s\n",
      "Epoch [236/700], Train Loss: 0.1429, Train Accuracy: 0.9708, Val Accuracy: 0.7708, Iter Time: 8.76s\n",
      "Epoch [237/700], Train Loss: 0.0526, Train Accuracy: 0.9857, Val Accuracy: 0.8403, Iter Time: 8.74s\n",
      "Epoch [238/700], Train Loss: 0.0927, Train Accuracy: 0.9832, Val Accuracy: 0.7847, Iter Time: 8.69s\n",
      "Epoch [239/700], Train Loss: 0.0812, Train Accuracy: 0.9814, Val Accuracy: 0.8056, Iter Time: 8.69s\n",
      "Epoch [240/700], Train Loss: 0.0404, Train Accuracy: 0.9898, Val Accuracy: 0.8056, Iter Time: 8.95s\n",
      "Epoch [241/700], Train Loss: 0.0486, Train Accuracy: 0.9912, Val Accuracy: 0.8681, Iter Time: 8.82s\n",
      "Epoch [242/700], Train Loss: 0.0667, Train Accuracy: 0.9890, Val Accuracy: 0.8194, Iter Time: 8.80s\n",
      "Epoch [243/700], Train Loss: 0.2925, Train Accuracy: 0.9368, Val Accuracy: 0.6736, Iter Time: 8.81s\n",
      "Epoch [244/700], Train Loss: 0.5629, Train Accuracy: 0.8812, Val Accuracy: 0.6944, Iter Time: 8.71s\n",
      "Epoch [245/700], Train Loss: 0.5116, Train Accuracy: 0.8966, Val Accuracy: 0.6597, Iter Time: 8.62s\n",
      "Epoch [246/700], Train Loss: 0.2287, Train Accuracy: 0.9474, Val Accuracy: 0.7986, Iter Time: 8.78s\n",
      "Epoch [247/700], Train Loss: 0.1367, Train Accuracy: 0.9697, Val Accuracy: 0.7639, Iter Time: 8.80s\n",
      "Epoch [248/700], Train Loss: 0.1283, Train Accuracy: 0.9700, Val Accuracy: 0.7917, Iter Time: 8.69s\n",
      "Epoch [249/700], Train Loss: 0.1702, Train Accuracy: 0.9631, Val Accuracy: 0.7639, Iter Time: 8.73s\n",
      "Epoch [250/700], Train Loss: 0.0789, Train Accuracy: 0.9839, Val Accuracy: 0.7778, Iter Time: 8.72s\n",
      "Epoch [251/700], Train Loss: 0.0840, Train Accuracy: 0.9861, Val Accuracy: 0.7917, Iter Time: 8.76s\n",
      "Epoch [252/700], Train Loss: 0.0629, Train Accuracy: 0.9850, Val Accuracy: 0.7986, Iter Time: 8.77s\n",
      "Epoch [253/700], Train Loss: 0.0442, Train Accuracy: 0.9934, Val Accuracy: 0.8472, Iter Time: 8.76s\n",
      "Epoch [254/700], Train Loss: 0.0428, Train Accuracy: 0.9905, Val Accuracy: 0.8403, Iter Time: 8.80s\n",
      "Epoch [255/700], Train Loss: 0.1584, Train Accuracy: 0.9642, Val Accuracy: 0.6458, Iter Time: 8.87s\n",
      "Epoch [256/700], Train Loss: 0.5106, Train Accuracy: 0.9002, Val Accuracy: 0.5903, Iter Time: 8.85s\n",
      "Epoch [257/700], Train Loss: 0.8275, Train Accuracy: 0.8359, Val Accuracy: 0.7083, Iter Time: 8.64s\n",
      "Epoch [258/700], Train Loss: 0.2518, Train Accuracy: 0.9441, Val Accuracy: 0.7917, Iter Time: 8.66s\n",
      "Epoch [259/700], Train Loss: 0.1445, Train Accuracy: 0.9722, Val Accuracy: 0.7500, Iter Time: 8.76s\n",
      "Epoch [260/700], Train Loss: 0.0783, Train Accuracy: 0.9795, Val Accuracy: 0.8542, Iter Time: 8.73s\n",
      "Epoch [261/700], Train Loss: 0.0272, Train Accuracy: 0.9952, Val Accuracy: 0.8264, Iter Time: 8.71s\n",
      "Epoch [262/700], Train Loss: 0.0111, Train Accuracy: 0.9982, Val Accuracy: 0.8611, Iter Time: 8.74s\n",
      "Epoch [263/700], Train Loss: 0.0009, Train Accuracy: 0.9996, Val Accuracy: 0.8681, Iter Time: 8.75s\n",
      "Epoch [264/700], Train Loss: 0.0030, Train Accuracy: 0.9993, Val Accuracy: 0.8472, Iter Time: 8.73s\n",
      "Epoch [265/700], Train Loss: 0.0042, Train Accuracy: 0.9996, Val Accuracy: 0.8611, Iter Time: 8.95s\n",
      "Epoch [266/700], Train Loss: 0.0020, Train Accuracy: 0.9996, Val Accuracy: 0.8611, Iter Time: 8.80s\n",
      "Epoch [267/700], Train Loss: 0.0182, Train Accuracy: 0.9982, Val Accuracy: 0.8264, Iter Time: 9.06s\n",
      "Epoch [268/700], Train Loss: 0.1322, Train Accuracy: 0.9810, Val Accuracy: 0.7778, Iter Time: 8.90s\n",
      "Epoch [269/700], Train Loss: 0.9723, Train Accuracy: 0.8041, Val Accuracy: 0.6389, Iter Time: 8.80s\n",
      "Epoch [270/700], Train Loss: 0.5669, Train Accuracy: 0.8655, Val Accuracy: 0.7986, Iter Time: 8.65s\n",
      "Epoch [271/700], Train Loss: 0.2231, Train Accuracy: 0.9532, Val Accuracy: 0.7917, Iter Time: 8.72s\n",
      "Epoch [272/700], Train Loss: 0.0531, Train Accuracy: 0.9854, Val Accuracy: 0.8056, Iter Time: 8.93s\n",
      "Epoch [273/700], Train Loss: 0.0737, Train Accuracy: 0.9868, Val Accuracy: 0.8125, Iter Time: 8.88s\n",
      "Epoch [274/700], Train Loss: 0.0409, Train Accuracy: 0.9916, Val Accuracy: 0.8264, Iter Time: 8.86s\n",
      "Epoch [275/700], Train Loss: 0.0249, Train Accuracy: 0.9963, Val Accuracy: 0.8194, Iter Time: 9.00s\n",
      "Epoch [276/700], Train Loss: 0.0037, Train Accuracy: 0.9985, Val Accuracy: 0.8403, Iter Time: 9.25s\n",
      "Epoch [277/700], Train Loss: 0.0016, Train Accuracy: 0.9993, Val Accuracy: 0.8333, Iter Time: 9.16s\n",
      "Epoch [278/700], Train Loss: 0.0066, Train Accuracy: 0.9996, Val Accuracy: 0.8472, Iter Time: 9.01s\n",
      "Epoch [279/700], Train Loss: 0.0025, Train Accuracy: 0.9996, Val Accuracy: 0.8333, Iter Time: 9.01s\n",
      "Epoch [280/700], Train Loss: 0.0009, Train Accuracy: 0.9996, Val Accuracy: 0.8472, Iter Time: 8.99s\n",
      "Epoch [281/700], Train Loss: 0.0048, Train Accuracy: 0.9996, Val Accuracy: 0.8611, Iter Time: 8.93s\n",
      "Epoch [282/700], Train Loss: 0.0024, Train Accuracy: 0.9996, Val Accuracy: 0.8611, Iter Time: 8.72s\n",
      "Epoch [283/700], Train Loss: 0.0011, Train Accuracy: 0.9996, Val Accuracy: 0.8611, Iter Time: 8.92s\n",
      "Epoch [284/700], Train Loss: 0.0011, Train Accuracy: 0.9996, Val Accuracy: 0.8681, Iter Time: 8.81s\n",
      "Epoch [285/700], Train Loss: 0.0010, Train Accuracy: 0.9996, Val Accuracy: 0.8681, Iter Time: 8.91s\n",
      "Epoch [286/700], Train Loss: 0.0012, Train Accuracy: 0.9996, Val Accuracy: 0.8611, Iter Time: 8.79s\n",
      "Epoch [287/700], Train Loss: 0.0009, Train Accuracy: 0.9996, Val Accuracy: 0.8611, Iter Time: 8.79s\n",
      "Epoch [288/700], Train Loss: 0.0010, Train Accuracy: 0.9996, Val Accuracy: 0.8681, Iter Time: 8.79s\n",
      "Epoch [289/700], Train Loss: 0.0009, Train Accuracy: 0.9996, Val Accuracy: 0.8611, Iter Time: 8.87s\n",
      "Epoch [290/700], Train Loss: 0.0016, Train Accuracy: 0.9996, Val Accuracy: 0.8611, Iter Time: 8.84s\n",
      "Epoch [291/700], Train Loss: 0.0014, Train Accuracy: 0.9996, Val Accuracy: 0.8472, Iter Time: 8.88s\n",
      "Early stopping after 291 epochs\n",
      "prediction: ['H', '5', '7', 'D', 'U', '7', 'H', 'M', 'S', '1', 'H', 'O', 'S', 'D', '7']\n",
      "real labels: ['A', 'B', 'N', 'O', 'R', 'M', 'A', 'L', 'I', 'Z', 'A', 'T', 'I', 'O', 'N']\n",
      "prediction: [0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 4, 4, 4, 5, 5, 5, 5, 5]\n",
      "true labels: [0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4]\n",
      "Fold 1 FINAL RESULTS: \n",
      "accuracy: 0.028 // precision: 0.028 // recall: 0.053 // f1: 0.031\n",
      "\n",
      "Fold 2/5\n",
      "2880\n",
      "Epoch [1/700], Train Loss: 3.6076, Train Accuracy: 0.0439, Val Accuracy: 0.0417, Iter Time: 10.22s\n",
      "Epoch [2/700], Train Loss: 3.1733, Train Accuracy: 0.1575, Val Accuracy: 0.1250, Iter Time: 8.83s\n",
      "Epoch [3/700], Train Loss: 2.7446, Train Accuracy: 0.3074, Val Accuracy: 0.1944, Iter Time: 8.69s\n",
      "Epoch [4/700], Train Loss: 2.2516, Train Accuracy: 0.4419, Val Accuracy: 0.2986, Iter Time: 8.67s\n",
      "Epoch [5/700], Train Loss: 1.7547, Train Accuracy: 0.5782, Val Accuracy: 0.3264, Iter Time: 8.70s\n",
      "Epoch [6/700], Train Loss: 1.2872, Train Accuracy: 0.7153, Val Accuracy: 0.3750, Iter Time: 8.74s\n",
      "Epoch [7/700], Train Loss: 0.8773, Train Accuracy: 0.8282, Val Accuracy: 0.3819, Iter Time: 8.77s\n",
      "Epoch [8/700], Train Loss: 0.5604, Train Accuracy: 0.9207, Val Accuracy: 0.4306, Iter Time: 8.72s\n",
      "Epoch [9/700], Train Loss: 0.3499, Train Accuracy: 0.9667, Val Accuracy: 0.4514, Iter Time: 8.80s\n",
      "Epoch [10/700], Train Loss: 0.2288, Train Accuracy: 0.9843, Val Accuracy: 0.4097, Iter Time: 8.78s\n",
      "Epoch [11/700], Train Loss: 0.1587, Train Accuracy: 0.9868, Val Accuracy: 0.4306, Iter Time: 8.78s\n",
      "Epoch [12/700], Train Loss: 0.1058, Train Accuracy: 0.9912, Val Accuracy: 0.4722, Iter Time: 8.76s\n",
      "Epoch [13/700], Train Loss: 0.0679, Train Accuracy: 0.9942, Val Accuracy: 0.4514, Iter Time: 8.81s\n",
      "Epoch [14/700], Train Loss: 0.0597, Train Accuracy: 0.9949, Val Accuracy: 0.4583, Iter Time: 8.84s\n",
      "Epoch [15/700], Train Loss: 0.0442, Train Accuracy: 0.9945, Val Accuracy: 0.5000, Iter Time: 8.66s\n",
      "Epoch [16/700], Train Loss: 0.0407, Train Accuracy: 0.9938, Val Accuracy: 0.4722, Iter Time: 8.66s\n",
      "Epoch [17/700], Train Loss: 0.0348, Train Accuracy: 0.9967, Val Accuracy: 0.4653, Iter Time: 9.06s\n",
      "Epoch [18/700], Train Loss: 0.0421, Train Accuracy: 0.9942, Val Accuracy: 0.4792, Iter Time: 8.80s\n",
      "Epoch [19/700], Train Loss: 0.0474, Train Accuracy: 0.9923, Val Accuracy: 0.5208, Iter Time: 8.69s\n",
      "Epoch [20/700], Train Loss: 0.0472, Train Accuracy: 0.9927, Val Accuracy: 0.5417, Iter Time: 8.70s\n",
      "Epoch [21/700], Train Loss: 0.0569, Train Accuracy: 0.9890, Val Accuracy: 0.4167, Iter Time: 8.93s\n",
      "Epoch [22/700], Train Loss: 0.1541, Train Accuracy: 0.9561, Val Accuracy: 0.5069, Iter Time: 8.79s\n",
      "Epoch [23/700], Train Loss: 0.0770, Train Accuracy: 0.9825, Val Accuracy: 0.4861, Iter Time: 9.00s\n",
      "Epoch [24/700], Train Loss: 0.0406, Train Accuracy: 0.9949, Val Accuracy: 0.5208, Iter Time: 8.81s\n",
      "Epoch [25/700], Train Loss: 0.0278, Train Accuracy: 0.9952, Val Accuracy: 0.4861, Iter Time: 8.96s\n",
      "Epoch [26/700], Train Loss: 0.0269, Train Accuracy: 0.9942, Val Accuracy: 0.5278, Iter Time: 8.85s\n",
      "Epoch [27/700], Train Loss: 0.0403, Train Accuracy: 0.9916, Val Accuracy: 0.4861, Iter Time: 8.73s\n",
      "Epoch [28/700], Train Loss: 0.0411, Train Accuracy: 0.9901, Val Accuracy: 0.4861, Iter Time: 8.62s\n",
      "Epoch [29/700], Train Loss: 0.1244, Train Accuracy: 0.9686, Val Accuracy: 0.4375, Iter Time: 8.65s\n",
      "Epoch [30/700], Train Loss: 0.1568, Train Accuracy: 0.9576, Val Accuracy: 0.5139, Iter Time: 8.67s\n",
      "Epoch [31/700], Train Loss: 0.1556, Train Accuracy: 0.9514, Val Accuracy: 0.4375, Iter Time: 8.66s\n",
      "Epoch [32/700], Train Loss: 0.0998, Train Accuracy: 0.9722, Val Accuracy: 0.4653, Iter Time: 8.69s\n",
      "Epoch [33/700], Train Loss: 0.0796, Train Accuracy: 0.9843, Val Accuracy: 0.5417, Iter Time: 8.71s\n",
      "Epoch [34/700], Train Loss: 0.0218, Train Accuracy: 0.9949, Val Accuracy: 0.5694, Iter Time: 8.77s\n",
      "Epoch [35/700], Train Loss: 0.0262, Train Accuracy: 0.9963, Val Accuracy: 0.5556, Iter Time: 8.76s\n",
      "Epoch [36/700], Train Loss: 0.0248, Train Accuracy: 0.9960, Val Accuracy: 0.5694, Iter Time: 8.76s\n",
      "Epoch [37/700], Train Loss: 0.0268, Train Accuracy: 0.9956, Val Accuracy: 0.5486, Iter Time: 8.89s\n",
      "Epoch [38/700], Train Loss: 0.0413, Train Accuracy: 0.9898, Val Accuracy: 0.5139, Iter Time: 9.37s\n",
      "Epoch [39/700], Train Loss: 0.4012, Train Accuracy: 0.8856, Val Accuracy: 0.4028, Iter Time: 8.86s\n",
      "Epoch [40/700], Train Loss: 0.4158, Train Accuracy: 0.8739, Val Accuracy: 0.5833, Iter Time: 8.67s\n",
      "Epoch [41/700], Train Loss: 0.0976, Train Accuracy: 0.9733, Val Accuracy: 0.4722, Iter Time: 8.67s\n",
      "Epoch [42/700], Train Loss: 0.0374, Train Accuracy: 0.9905, Val Accuracy: 0.5764, Iter Time: 8.65s\n",
      "Epoch [43/700], Train Loss: 0.0201, Train Accuracy: 0.9952, Val Accuracy: 0.5347, Iter Time: 8.67s\n",
      "Epoch [44/700], Train Loss: 0.0354, Train Accuracy: 0.9916, Val Accuracy: 0.5833, Iter Time: 8.97s\n",
      "Epoch [45/700], Train Loss: 0.0369, Train Accuracy: 0.9909, Val Accuracy: 0.5903, Iter Time: 8.86s\n",
      "Epoch [46/700], Train Loss: 0.0494, Train Accuracy: 0.9857, Val Accuracy: 0.5903, Iter Time: 8.97s\n",
      "Epoch [47/700], Train Loss: 0.1721, Train Accuracy: 0.9550, Val Accuracy: 0.5208, Iter Time: 8.94s\n",
      "Epoch [48/700], Train Loss: 0.4463, Train Accuracy: 0.8604, Val Accuracy: 0.4722, Iter Time: 9.07s\n",
      "Epoch [49/700], Train Loss: 0.2742, Train Accuracy: 0.9141, Val Accuracy: 0.5069, Iter Time: 8.89s\n",
      "Epoch [50/700], Train Loss: 0.1245, Train Accuracy: 0.9620, Val Accuracy: 0.5764, Iter Time: 8.94s\n",
      "Epoch [51/700], Train Loss: 0.0899, Train Accuracy: 0.9711, Val Accuracy: 0.5486, Iter Time: 8.83s\n",
      "Epoch [52/700], Train Loss: 0.0655, Train Accuracy: 0.9828, Val Accuracy: 0.5208, Iter Time: 8.60s\n",
      "Epoch [53/700], Train Loss: 0.1085, Train Accuracy: 0.9740, Val Accuracy: 0.5486, Iter Time: 8.66s\n",
      "Epoch [54/700], Train Loss: 0.1082, Train Accuracy: 0.9671, Val Accuracy: 0.5764, Iter Time: 8.66s\n",
      "Epoch [55/700], Train Loss: 0.1250, Train Accuracy: 0.9627, Val Accuracy: 0.5347, Iter Time: 8.69s\n",
      "Epoch [56/700], Train Loss: 0.1537, Train Accuracy: 0.9499, Val Accuracy: 0.4861, Iter Time: 8.69s\n",
      "Epoch [57/700], Train Loss: 0.1537, Train Accuracy: 0.9561, Val Accuracy: 0.5486, Iter Time: 8.69s\n",
      "Epoch [58/700], Train Loss: 0.1900, Train Accuracy: 0.9459, Val Accuracy: 0.5764, Iter Time: 8.74s\n",
      "Epoch [59/700], Train Loss: 0.2257, Train Accuracy: 0.9342, Val Accuracy: 0.4722, Iter Time: 8.88s\n",
      "Epoch [60/700], Train Loss: 0.1802, Train Accuracy: 0.9477, Val Accuracy: 0.5000, Iter Time: 8.76s\n",
      "Epoch [61/700], Train Loss: 0.1083, Train Accuracy: 0.9689, Val Accuracy: 0.5694, Iter Time: 8.82s\n",
      "Epoch [62/700], Train Loss: 0.1703, Train Accuracy: 0.9529, Val Accuracy: 0.5139, Iter Time: 8.80s\n",
      "Epoch [63/700], Train Loss: 0.1241, Train Accuracy: 0.9602, Val Accuracy: 0.5833, Iter Time: 8.87s\n",
      "Epoch [64/700], Train Loss: 0.1609, Train Accuracy: 0.9539, Val Accuracy: 0.5694, Iter Time: 8.86s\n",
      "Epoch [65/700], Train Loss: 0.2147, Train Accuracy: 0.9353, Val Accuracy: 0.5486, Iter Time: 9.18s\n",
      "Epoch [66/700], Train Loss: 0.2474, Train Accuracy: 0.9236, Val Accuracy: 0.5764, Iter Time: 8.73s\n",
      "Epoch [67/700], Train Loss: 0.1229, Train Accuracy: 0.9616, Val Accuracy: 0.5903, Iter Time: 8.71s\n",
      "Epoch [68/700], Train Loss: 0.0848, Train Accuracy: 0.9781, Val Accuracy: 0.6111, Iter Time: 8.70s\n",
      "Epoch [69/700], Train Loss: 0.0762, Train Accuracy: 0.9773, Val Accuracy: 0.5556, Iter Time: 8.74s\n",
      "Epoch [70/700], Train Loss: 0.1937, Train Accuracy: 0.9430, Val Accuracy: 0.5347, Iter Time: 8.77s\n",
      "Epoch [71/700], Train Loss: 0.3437, Train Accuracy: 0.9013, Val Accuracy: 0.5625, Iter Time: 9.10s\n",
      "Epoch [72/700], Train Loss: 0.2851, Train Accuracy: 0.9170, Val Accuracy: 0.6389, Iter Time: 8.98s\n",
      "Epoch [73/700], Train Loss: 0.2252, Train Accuracy: 0.9317, Val Accuracy: 0.5417, Iter Time: 8.86s\n",
      "Epoch [74/700], Train Loss: 0.2025, Train Accuracy: 0.9455, Val Accuracy: 0.6181, Iter Time: 8.89s\n",
      "Epoch [75/700], Train Loss: 0.0948, Train Accuracy: 0.9722, Val Accuracy: 0.5694, Iter Time: 8.98s\n",
      "Epoch [76/700], Train Loss: 0.1129, Train Accuracy: 0.9711, Val Accuracy: 0.6042, Iter Time: 8.87s\n",
      "Epoch [77/700], Train Loss: 0.1268, Train Accuracy: 0.9667, Val Accuracy: 0.6667, Iter Time: 8.71s\n",
      "Epoch [78/700], Train Loss: 0.1666, Train Accuracy: 0.9576, Val Accuracy: 0.5556, Iter Time: 8.69s\n",
      "Epoch [79/700], Train Loss: 0.3015, Train Accuracy: 0.9148, Val Accuracy: 0.5694, Iter Time: 8.85s\n",
      "Epoch [80/700], Train Loss: 0.2440, Train Accuracy: 0.9291, Val Accuracy: 0.5208, Iter Time: 8.87s\n",
      "Epoch [81/700], Train Loss: 0.2698, Train Accuracy: 0.9196, Val Accuracy: 0.5903, Iter Time: 9.05s\n",
      "Epoch [82/700], Train Loss: 0.1607, Train Accuracy: 0.9496, Val Accuracy: 0.5625, Iter Time: 9.54s\n",
      "Epoch [83/700], Train Loss: 0.1720, Train Accuracy: 0.9518, Val Accuracy: 0.5972, Iter Time: 9.24s\n",
      "Epoch [84/700], Train Loss: 0.1639, Train Accuracy: 0.9470, Val Accuracy: 0.5556, Iter Time: 9.58s\n",
      "Epoch [85/700], Train Loss: 0.2508, Train Accuracy: 0.9309, Val Accuracy: 0.6042, Iter Time: 9.20s\n",
      "Epoch [86/700], Train Loss: 0.2367, Train Accuracy: 0.9386, Val Accuracy: 0.6389, Iter Time: 9.09s\n",
      "Epoch [87/700], Train Loss: 0.2128, Train Accuracy: 0.9408, Val Accuracy: 0.6736, Iter Time: 9.29s\n",
      "Epoch [88/700], Train Loss: 0.2045, Train Accuracy: 0.9444, Val Accuracy: 0.5694, Iter Time: 9.16s\n",
      "Epoch [89/700], Train Loss: 0.1678, Train Accuracy: 0.9521, Val Accuracy: 0.6111, Iter Time: 8.82s\n",
      "Epoch [90/700], Train Loss: 0.3020, Train Accuracy: 0.9211, Val Accuracy: 0.6250, Iter Time: 9.32s\n",
      "Epoch [91/700], Train Loss: 0.1645, Train Accuracy: 0.9510, Val Accuracy: 0.5694, Iter Time: 9.08s\n",
      "Epoch [92/700], Train Loss: 0.2959, Train Accuracy: 0.9148, Val Accuracy: 0.6111, Iter Time: 9.01s\n",
      "Epoch [93/700], Train Loss: 0.2290, Train Accuracy: 0.9349, Val Accuracy: 0.6111, Iter Time: 9.06s\n",
      "Epoch [94/700], Train Loss: 0.1983, Train Accuracy: 0.9492, Val Accuracy: 0.5417, Iter Time: 9.11s\n",
      "Epoch [95/700], Train Loss: 0.1721, Train Accuracy: 0.9514, Val Accuracy: 0.5139, Iter Time: 8.81s\n",
      "Epoch [96/700], Train Loss: 0.2092, Train Accuracy: 0.9401, Val Accuracy: 0.5347, Iter Time: 9.19s\n",
      "Epoch [97/700], Train Loss: 0.1972, Train Accuracy: 0.9455, Val Accuracy: 0.6458, Iter Time: 9.49s\n",
      "Epoch [98/700], Train Loss: 0.2064, Train Accuracy: 0.9448, Val Accuracy: 0.5972, Iter Time: 9.15s\n",
      "Epoch [99/700], Train Loss: 0.2627, Train Accuracy: 0.9247, Val Accuracy: 0.5347, Iter Time: 9.30s\n",
      "Epoch [100/700], Train Loss: 0.2400, Train Accuracy: 0.9346, Val Accuracy: 0.6181, Iter Time: 9.50s\n",
      "Epoch [101/700], Train Loss: 0.2880, Train Accuracy: 0.9174, Val Accuracy: 0.5972, Iter Time: 9.02s\n",
      "Epoch [102/700], Train Loss: 0.2361, Train Accuracy: 0.9423, Val Accuracy: 0.6111, Iter Time: 8.66s\n",
      "Epoch [103/700], Train Loss: 0.2322, Train Accuracy: 0.9360, Val Accuracy: 0.6667, Iter Time: 9.11s\n",
      "Epoch [104/700], Train Loss: 0.2147, Train Accuracy: 0.9463, Val Accuracy: 0.6875, Iter Time: 9.39s\n",
      "Epoch [105/700], Train Loss: 0.1406, Train Accuracy: 0.9583, Val Accuracy: 0.6597, Iter Time: 9.00s\n",
      "Epoch [106/700], Train Loss: 0.1680, Train Accuracy: 0.9510, Val Accuracy: 0.6111, Iter Time: 8.70s\n",
      "Epoch [107/700], Train Loss: 0.2398, Train Accuracy: 0.9393, Val Accuracy: 0.6250, Iter Time: 8.73s\n",
      "Epoch [108/700], Train Loss: 0.2846, Train Accuracy: 0.9273, Val Accuracy: 0.4931, Iter Time: 8.73s\n",
      "Epoch [109/700], Train Loss: 0.2401, Train Accuracy: 0.9287, Val Accuracy: 0.5972, Iter Time: 8.76s\n",
      "Epoch [110/700], Train Loss: 0.2966, Train Accuracy: 0.9200, Val Accuracy: 0.5625, Iter Time: 8.78s\n",
      "Epoch [111/700], Train Loss: 0.1848, Train Accuracy: 0.9521, Val Accuracy: 0.6875, Iter Time: 9.03s\n",
      "Epoch [112/700], Train Loss: 0.2086, Train Accuracy: 0.9415, Val Accuracy: 0.5903, Iter Time: 8.89s\n",
      "Epoch [113/700], Train Loss: 0.2242, Train Accuracy: 0.9419, Val Accuracy: 0.6528, Iter Time: 8.76s\n",
      "Epoch [114/700], Train Loss: 0.2371, Train Accuracy: 0.9335, Val Accuracy: 0.5903, Iter Time: 8.98s\n",
      "Epoch [115/700], Train Loss: 0.2461, Train Accuracy: 0.9236, Val Accuracy: 0.5764, Iter Time: 9.31s\n",
      "Epoch [116/700], Train Loss: 0.3262, Train Accuracy: 0.9090, Val Accuracy: 0.5625, Iter Time: 8.96s\n",
      "Epoch [117/700], Train Loss: 0.2860, Train Accuracy: 0.9178, Val Accuracy: 0.5694, Iter Time: 9.04s\n",
      "Epoch [118/700], Train Loss: 0.2285, Train Accuracy: 0.9404, Val Accuracy: 0.5833, Iter Time: 8.95s\n",
      "Epoch [119/700], Train Loss: 0.2010, Train Accuracy: 0.9441, Val Accuracy: 0.6944, Iter Time: 8.82s\n",
      "Epoch [120/700], Train Loss: 0.2164, Train Accuracy: 0.9448, Val Accuracy: 0.6458, Iter Time: 9.00s\n",
      "Epoch [121/700], Train Loss: 0.2089, Train Accuracy: 0.9382, Val Accuracy: 0.6458, Iter Time: 8.90s\n",
      "Epoch [122/700], Train Loss: 0.2603, Train Accuracy: 0.9360, Val Accuracy: 0.6389, Iter Time: 8.84s\n",
      "Epoch [123/700], Train Loss: 0.1726, Train Accuracy: 0.9539, Val Accuracy: 0.6736, Iter Time: 8.83s\n",
      "Epoch [124/700], Train Loss: 0.1955, Train Accuracy: 0.9514, Val Accuracy: 0.6389, Iter Time: 8.90s\n",
      "Epoch [125/700], Train Loss: 0.2481, Train Accuracy: 0.9320, Val Accuracy: 0.6042, Iter Time: 8.81s\n",
      "Epoch [126/700], Train Loss: 0.3065, Train Accuracy: 0.9137, Val Accuracy: 0.5972, Iter Time: 8.65s\n",
      "Epoch [127/700], Train Loss: 0.3018, Train Accuracy: 0.9214, Val Accuracy: 0.6667, Iter Time: 8.64s\n",
      "Epoch [128/700], Train Loss: 0.3324, Train Accuracy: 0.9152, Val Accuracy: 0.6250, Iter Time: 8.80s\n",
      "Epoch [129/700], Train Loss: 0.2099, Train Accuracy: 0.9470, Val Accuracy: 0.6806, Iter Time: 8.82s\n",
      "Epoch [130/700], Train Loss: 0.2176, Train Accuracy: 0.9492, Val Accuracy: 0.5625, Iter Time: 8.80s\n",
      "Epoch [131/700], Train Loss: 0.2112, Train Accuracy: 0.9455, Val Accuracy: 0.6806, Iter Time: 8.91s\n",
      "Epoch [132/700], Train Loss: 0.2045, Train Accuracy: 0.9444, Val Accuracy: 0.6597, Iter Time: 8.99s\n",
      "Epoch [133/700], Train Loss: 0.2374, Train Accuracy: 0.9397, Val Accuracy: 0.6667, Iter Time: 8.85s\n",
      "Epoch [134/700], Train Loss: 0.2599, Train Accuracy: 0.9298, Val Accuracy: 0.6042, Iter Time: 8.88s\n",
      "Epoch [135/700], Train Loss: 0.2453, Train Accuracy: 0.9360, Val Accuracy: 0.6250, Iter Time: 8.90s\n",
      "Epoch [136/700], Train Loss: 0.2485, Train Accuracy: 0.9386, Val Accuracy: 0.6319, Iter Time: 9.05s\n",
      "Epoch [137/700], Train Loss: 0.1373, Train Accuracy: 0.9678, Val Accuracy: 0.6944, Iter Time: 8.95s\n",
      "Epoch [138/700], Train Loss: 0.2068, Train Accuracy: 0.9518, Val Accuracy: 0.6528, Iter Time: 8.96s\n",
      "Epoch [139/700], Train Loss: 0.3444, Train Accuracy: 0.9068, Val Accuracy: 0.6667, Iter Time: 8.87s\n",
      "Epoch [140/700], Train Loss: 0.2647, Train Accuracy: 0.9280, Val Accuracy: 0.6736, Iter Time: 9.66s\n",
      "Epoch [141/700], Train Loss: 0.3449, Train Accuracy: 0.9200, Val Accuracy: 0.6667, Iter Time: 9.26s\n",
      "Epoch [142/700], Train Loss: 0.2451, Train Accuracy: 0.9379, Val Accuracy: 0.6389, Iter Time: 8.84s\n",
      "Epoch [143/700], Train Loss: 0.2648, Train Accuracy: 0.9287, Val Accuracy: 0.6528, Iter Time: 8.89s\n",
      "Epoch [144/700], Train Loss: 0.2419, Train Accuracy: 0.9382, Val Accuracy: 0.6597, Iter Time: 8.98s\n",
      "Epoch [145/700], Train Loss: 0.2597, Train Accuracy: 0.9335, Val Accuracy: 0.7083, Iter Time: 8.94s\n",
      "Epoch [146/700], Train Loss: 0.2561, Train Accuracy: 0.9331, Val Accuracy: 0.6597, Iter Time: 8.98s\n",
      "Epoch [147/700], Train Loss: 0.2514, Train Accuracy: 0.9430, Val Accuracy: 0.7014, Iter Time: 8.99s\n",
      "Epoch [148/700], Train Loss: 0.1653, Train Accuracy: 0.9594, Val Accuracy: 0.6250, Iter Time: 9.00s\n",
      "Epoch [149/700], Train Loss: 0.1999, Train Accuracy: 0.9503, Val Accuracy: 0.6250, Iter Time: 9.22s\n",
      "Epoch [150/700], Train Loss: 0.2791, Train Accuracy: 0.9309, Val Accuracy: 0.5972, Iter Time: 9.30s\n",
      "Epoch [151/700], Train Loss: 0.2651, Train Accuracy: 0.9353, Val Accuracy: 0.6667, Iter Time: 9.02s\n",
      "Epoch [152/700], Train Loss: 0.2630, Train Accuracy: 0.9309, Val Accuracy: 0.5903, Iter Time: 9.03s\n",
      "Epoch [153/700], Train Loss: 0.2143, Train Accuracy: 0.9452, Val Accuracy: 0.6597, Iter Time: 9.02s\n",
      "Epoch [154/700], Train Loss: 0.2591, Train Accuracy: 0.9386, Val Accuracy: 0.6875, Iter Time: 8.86s\n",
      "Epoch [155/700], Train Loss: 0.2779, Train Accuracy: 0.9291, Val Accuracy: 0.6806, Iter Time: 8.88s\n",
      "Epoch [156/700], Train Loss: 0.3177, Train Accuracy: 0.9174, Val Accuracy: 0.6319, Iter Time: 8.90s\n",
      "Epoch [157/700], Train Loss: 0.2720, Train Accuracy: 0.9324, Val Accuracy: 0.6458, Iter Time: 8.92s\n",
      "Epoch [158/700], Train Loss: 0.3257, Train Accuracy: 0.9101, Val Accuracy: 0.6944, Iter Time: 8.95s\n",
      "Epoch [159/700], Train Loss: 0.2232, Train Accuracy: 0.9448, Val Accuracy: 0.7153, Iter Time: 8.97s\n",
      "Epoch [160/700], Train Loss: 0.2041, Train Accuracy: 0.9470, Val Accuracy: 0.6458, Iter Time: 9.07s\n",
      "Epoch [161/700], Train Loss: 0.1942, Train Accuracy: 0.9539, Val Accuracy: 0.7292, Iter Time: 9.17s\n",
      "Epoch [162/700], Train Loss: 0.1958, Train Accuracy: 0.9518, Val Accuracy: 0.6597, Iter Time: 8.92s\n",
      "Epoch [163/700], Train Loss: 0.2989, Train Accuracy: 0.9295, Val Accuracy: 0.7083, Iter Time: 8.78s\n",
      "Epoch [164/700], Train Loss: 0.1764, Train Accuracy: 0.9572, Val Accuracy: 0.6736, Iter Time: 8.82s\n",
      "Epoch [165/700], Train Loss: 0.2957, Train Accuracy: 0.9287, Val Accuracy: 0.6319, Iter Time: 8.80s\n",
      "Epoch [166/700], Train Loss: 0.3420, Train Accuracy: 0.9123, Val Accuracy: 0.6597, Iter Time: 8.83s\n",
      "Epoch [167/700], Train Loss: 0.2257, Train Accuracy: 0.9415, Val Accuracy: 0.6667, Iter Time: 8.84s\n",
      "Epoch [168/700], Train Loss: 0.2397, Train Accuracy: 0.9466, Val Accuracy: 0.7083, Iter Time: 8.90s\n",
      "Epoch [169/700], Train Loss: 0.1638, Train Accuracy: 0.9598, Val Accuracy: 0.7292, Iter Time: 8.84s\n",
      "Epoch [170/700], Train Loss: 0.2461, Train Accuracy: 0.9437, Val Accuracy: 0.7014, Iter Time: 8.96s\n",
      "Epoch [171/700], Train Loss: 0.1894, Train Accuracy: 0.9565, Val Accuracy: 0.6736, Iter Time: 8.89s\n",
      "Epoch [172/700], Train Loss: 0.2229, Train Accuracy: 0.9459, Val Accuracy: 0.6458, Iter Time: 8.91s\n",
      "Epoch [173/700], Train Loss: 0.2143, Train Accuracy: 0.9485, Val Accuracy: 0.7222, Iter Time: 9.00s\n",
      "Epoch [174/700], Train Loss: 0.2747, Train Accuracy: 0.9401, Val Accuracy: 0.7431, Iter Time: 8.99s\n",
      "Epoch [175/700], Train Loss: 0.3801, Train Accuracy: 0.9013, Val Accuracy: 0.6736, Iter Time: 8.84s\n",
      "Epoch [176/700], Train Loss: 0.2574, Train Accuracy: 0.9393, Val Accuracy: 0.6736, Iter Time: 8.79s\n",
      "Epoch [177/700], Train Loss: 0.2025, Train Accuracy: 0.9529, Val Accuracy: 0.7361, Iter Time: 8.76s\n",
      "Epoch [178/700], Train Loss: 0.1605, Train Accuracy: 0.9613, Val Accuracy: 0.6736, Iter Time: 8.79s\n",
      "Epoch [179/700], Train Loss: 0.1996, Train Accuracy: 0.9547, Val Accuracy: 0.6319, Iter Time: 8.81s\n",
      "Epoch [180/700], Train Loss: 0.3247, Train Accuracy: 0.9280, Val Accuracy: 0.6806, Iter Time: 8.81s\n",
      "Epoch [181/700], Train Loss: 0.2819, Train Accuracy: 0.9320, Val Accuracy: 0.6875, Iter Time: 8.85s\n",
      "Epoch [182/700], Train Loss: 0.2102, Train Accuracy: 0.9481, Val Accuracy: 0.6875, Iter Time: 9.10s\n",
      "Epoch [183/700], Train Loss: 0.1315, Train Accuracy: 0.9678, Val Accuracy: 0.7431, Iter Time: 8.93s\n",
      "Epoch [184/700], Train Loss: 0.2800, Train Accuracy: 0.9375, Val Accuracy: 0.7222, Iter Time: 8.93s\n",
      "Epoch [185/700], Train Loss: 0.1865, Train Accuracy: 0.9558, Val Accuracy: 0.6875, Iter Time: 8.90s\n",
      "Epoch [186/700], Train Loss: 0.1468, Train Accuracy: 0.9689, Val Accuracy: 0.6944, Iter Time: 8.95s\n",
      "Epoch [187/700], Train Loss: 0.1908, Train Accuracy: 0.9492, Val Accuracy: 0.6111, Iter Time: 8.84s\n",
      "Epoch [188/700], Train Loss: 0.3920, Train Accuracy: 0.9075, Val Accuracy: 0.6042, Iter Time: 8.75s\n",
      "Epoch [189/700], Train Loss: 0.3347, Train Accuracy: 0.9167, Val Accuracy: 0.6319, Iter Time: 8.76s\n",
      "Epoch [190/700], Train Loss: 0.2146, Train Accuracy: 0.9463, Val Accuracy: 0.7083, Iter Time: 8.79s\n",
      "Epoch [191/700], Train Loss: 0.1966, Train Accuracy: 0.9558, Val Accuracy: 0.7153, Iter Time: 8.80s\n",
      "Epoch [192/700], Train Loss: 0.1715, Train Accuracy: 0.9638, Val Accuracy: 0.7500, Iter Time: 8.85s\n",
      "Epoch [193/700], Train Loss: 0.1142, Train Accuracy: 0.9744, Val Accuracy: 0.7431, Iter Time: 8.87s\n",
      "Epoch [194/700], Train Loss: 0.2035, Train Accuracy: 0.9529, Val Accuracy: 0.6458, Iter Time: 8.88s\n",
      "Epoch [195/700], Train Loss: 0.3183, Train Accuracy: 0.9342, Val Accuracy: 0.7083, Iter Time: 8.95s\n",
      "Epoch [196/700], Train Loss: 0.2167, Train Accuracy: 0.9499, Val Accuracy: 0.7500, Iter Time: 8.92s\n",
      "Epoch [197/700], Train Loss: 0.1409, Train Accuracy: 0.9682, Val Accuracy: 0.7153, Iter Time: 8.96s\n",
      "Epoch [198/700], Train Loss: 0.1656, Train Accuracy: 0.9664, Val Accuracy: 0.7361, Iter Time: 8.89s\n",
      "Epoch [199/700], Train Loss: 0.2247, Train Accuracy: 0.9492, Val Accuracy: 0.7292, Iter Time: 8.84s\n",
      "Epoch [200/700], Train Loss: 0.2978, Train Accuracy: 0.9327, Val Accuracy: 0.6389, Iter Time: 8.89s\n",
      "Epoch [201/700], Train Loss: 0.3719, Train Accuracy: 0.9192, Val Accuracy: 0.6458, Iter Time: 8.70s\n",
      "Epoch [202/700], Train Loss: 0.1779, Train Accuracy: 0.9569, Val Accuracy: 0.7292, Iter Time: 8.69s\n",
      "Epoch [203/700], Train Loss: 0.2221, Train Accuracy: 0.9474, Val Accuracy: 0.6806, Iter Time: 8.95s\n",
      "Epoch [204/700], Train Loss: 0.2614, Train Accuracy: 0.9386, Val Accuracy: 0.7292, Iter Time: 8.83s\n",
      "Epoch [205/700], Train Loss: 0.1733, Train Accuracy: 0.9598, Val Accuracy: 0.7083, Iter Time: 8.92s\n",
      "Epoch [206/700], Train Loss: 0.1321, Train Accuracy: 0.9671, Val Accuracy: 0.7847, Iter Time: 8.88s\n",
      "Epoch [207/700], Train Loss: 0.1273, Train Accuracy: 0.9697, Val Accuracy: 0.7222, Iter Time: 8.91s\n",
      "Epoch [208/700], Train Loss: 0.2823, Train Accuracy: 0.9324, Val Accuracy: 0.6944, Iter Time: 8.94s\n",
      "Epoch [209/700], Train Loss: 0.2813, Train Accuracy: 0.9419, Val Accuracy: 0.7292, Iter Time: 8.97s\n",
      "Epoch [210/700], Train Loss: 0.1965, Train Accuracy: 0.9572, Val Accuracy: 0.7014, Iter Time: 8.96s\n",
      "Epoch [211/700], Train Loss: 0.1697, Train Accuracy: 0.9591, Val Accuracy: 0.7153, Iter Time: 9.00s\n",
      "Epoch [212/700], Train Loss: 0.2202, Train Accuracy: 0.9550, Val Accuracy: 0.7292, Iter Time: 8.78s\n",
      "Epoch [213/700], Train Loss: 0.1149, Train Accuracy: 0.9711, Val Accuracy: 0.6389, Iter Time: 8.73s\n",
      "Epoch [214/700], Train Loss: 0.1006, Train Accuracy: 0.9748, Val Accuracy: 0.6458, Iter Time: 8.74s\n",
      "Epoch [215/700], Train Loss: 0.3601, Train Accuracy: 0.9192, Val Accuracy: 0.6319, Iter Time: 8.84s\n",
      "Epoch [216/700], Train Loss: 0.2831, Train Accuracy: 0.9306, Val Accuracy: 0.6736, Iter Time: 8.86s\n",
      "Epoch [217/700], Train Loss: 0.2088, Train Accuracy: 0.9543, Val Accuracy: 0.6597, Iter Time: 8.86s\n",
      "Epoch [218/700], Train Loss: 0.1236, Train Accuracy: 0.9722, Val Accuracy: 0.7361, Iter Time: 8.93s\n",
      "Epoch [219/700], Train Loss: 0.2019, Train Accuracy: 0.9543, Val Accuracy: 0.6944, Iter Time: 8.86s\n",
      "Epoch [220/700], Train Loss: 0.1592, Train Accuracy: 0.9642, Val Accuracy: 0.7500, Iter Time: 8.93s\n",
      "Epoch [221/700], Train Loss: 0.0878, Train Accuracy: 0.9821, Val Accuracy: 0.7361, Iter Time: 8.97s\n",
      "Epoch [222/700], Train Loss: 0.1605, Train Accuracy: 0.9667, Val Accuracy: 0.6944, Iter Time: 9.00s\n",
      "Epoch [223/700], Train Loss: 0.1608, Train Accuracy: 0.9642, Val Accuracy: 0.7639, Iter Time: 8.95s\n",
      "Epoch [224/700], Train Loss: 0.2224, Train Accuracy: 0.9481, Val Accuracy: 0.7153, Iter Time: 8.88s\n",
      "Epoch [225/700], Train Loss: 0.1828, Train Accuracy: 0.9543, Val Accuracy: 0.7014, Iter Time: 8.92s\n",
      "Epoch [226/700], Train Loss: 0.1525, Train Accuracy: 0.9609, Val Accuracy: 0.7083, Iter Time: 8.78s\n",
      "Epoch [227/700], Train Loss: 0.2311, Train Accuracy: 0.9433, Val Accuracy: 0.6736, Iter Time: 8.83s\n",
      "Epoch [228/700], Train Loss: 0.3228, Train Accuracy: 0.9258, Val Accuracy: 0.6597, Iter Time: 8.82s\n",
      "Epoch [229/700], Train Loss: 0.2454, Train Accuracy: 0.9441, Val Accuracy: 0.7847, Iter Time: 9.02s\n",
      "Epoch [230/700], Train Loss: 0.1384, Train Accuracy: 0.9693, Val Accuracy: 0.7917, Iter Time: 9.20s\n",
      "Epoch [231/700], Train Loss: 0.1123, Train Accuracy: 0.9762, Val Accuracy: 0.6875, Iter Time: 8.62s\n",
      "Epoch [232/700], Train Loss: 0.2065, Train Accuracy: 0.9510, Val Accuracy: 0.7431, Iter Time: 8.81s\n",
      "Epoch [233/700], Train Loss: 0.2450, Train Accuracy: 0.9452, Val Accuracy: 0.6875, Iter Time: 8.95s\n",
      "Epoch [234/700], Train Loss: 0.1792, Train Accuracy: 0.9587, Val Accuracy: 0.8194, Iter Time: 8.94s\n",
      "Epoch [235/700], Train Loss: 0.1159, Train Accuracy: 0.9759, Val Accuracy: 0.7431, Iter Time: 8.97s\n",
      "Epoch [236/700], Train Loss: 0.1154, Train Accuracy: 0.9755, Val Accuracy: 0.7847, Iter Time: 8.90s\n",
      "Epoch [237/700], Train Loss: 0.1699, Train Accuracy: 0.9697, Val Accuracy: 0.7569, Iter Time: 9.94s\n",
      "Epoch [238/700], Train Loss: 0.0698, Train Accuracy: 0.9879, Val Accuracy: 0.7847, Iter Time: 9.11s\n",
      "Epoch [239/700], Train Loss: 0.0384, Train Accuracy: 0.9909, Val Accuracy: 0.8264, Iter Time: 8.77s\n",
      "Epoch [240/700], Train Loss: 0.0827, Train Accuracy: 0.9795, Val Accuracy: 0.7014, Iter Time: 9.23s\n",
      "Epoch [241/700], Train Loss: 0.2792, Train Accuracy: 0.9360, Val Accuracy: 0.7083, Iter Time: 9.24s\n",
      "Epoch [242/700], Train Loss: 0.4713, Train Accuracy: 0.8988, Val Accuracy: 0.7222, Iter Time: 8.95s\n",
      "Epoch [243/700], Train Loss: 0.3808, Train Accuracy: 0.9141, Val Accuracy: 0.6944, Iter Time: 9.47s\n",
      "Epoch [244/700], Train Loss: 0.1592, Train Accuracy: 0.9613, Val Accuracy: 0.7292, Iter Time: 9.48s\n",
      "Epoch [245/700], Train Loss: 0.1051, Train Accuracy: 0.9755, Val Accuracy: 0.7500, Iter Time: 9.40s\n",
      "Epoch [246/700], Train Loss: 0.1737, Train Accuracy: 0.9678, Val Accuracy: 0.8056, Iter Time: 9.27s\n",
      "Epoch [247/700], Train Loss: 0.0909, Train Accuracy: 0.9770, Val Accuracy: 0.7917, Iter Time: 9.33s\n",
      "Epoch [248/700], Train Loss: 0.0821, Train Accuracy: 0.9825, Val Accuracy: 0.7778, Iter Time: 9.45s\n",
      "Epoch [249/700], Train Loss: 0.0787, Train Accuracy: 0.9803, Val Accuracy: 0.8056, Iter Time: 8.91s\n",
      "Epoch [250/700], Train Loss: 0.1289, Train Accuracy: 0.9744, Val Accuracy: 0.7361, Iter Time: 8.82s\n",
      "Epoch [251/700], Train Loss: 0.2808, Train Accuracy: 0.9335, Val Accuracy: 0.7153, Iter Time: 9.19s\n",
      "Epoch [252/700], Train Loss: 0.2748, Train Accuracy: 0.9335, Val Accuracy: 0.7292, Iter Time: 9.03s\n",
      "Epoch [253/700], Train Loss: 0.2492, Train Accuracy: 0.9455, Val Accuracy: 0.8403, Iter Time: 9.06s\n",
      "Epoch [254/700], Train Loss: 0.1234, Train Accuracy: 0.9704, Val Accuracy: 0.7569, Iter Time: 9.17s\n",
      "Epoch [255/700], Train Loss: 0.1522, Train Accuracy: 0.9653, Val Accuracy: 0.7778, Iter Time: 9.32s\n",
      "Epoch [256/700], Train Loss: 0.1574, Train Accuracy: 0.9719, Val Accuracy: 0.7639, Iter Time: 9.18s\n",
      "Epoch [257/700], Train Loss: 0.0321, Train Accuracy: 0.9920, Val Accuracy: 0.8125, Iter Time: 9.07s\n",
      "Epoch [258/700], Train Loss: 0.0525, Train Accuracy: 0.9920, Val Accuracy: 0.8125, Iter Time: 9.46s\n",
      "Epoch [259/700], Train Loss: 0.1100, Train Accuracy: 0.9781, Val Accuracy: 0.7778, Iter Time: 9.38s\n",
      "Epoch [260/700], Train Loss: 0.1752, Train Accuracy: 0.9616, Val Accuracy: 0.7153, Iter Time: 9.17s\n",
      "Epoch [261/700], Train Loss: 0.2877, Train Accuracy: 0.9415, Val Accuracy: 0.7569, Iter Time: 8.91s\n",
      "Epoch [262/700], Train Loss: 0.2801, Train Accuracy: 0.9393, Val Accuracy: 0.6597, Iter Time: 8.78s\n",
      "Epoch [263/700], Train Loss: 0.1067, Train Accuracy: 0.9733, Val Accuracy: 0.7639, Iter Time: 8.91s\n",
      "Epoch [264/700], Train Loss: 0.1281, Train Accuracy: 0.9726, Val Accuracy: 0.7500, Iter Time: 8.78s\n",
      "Epoch [265/700], Train Loss: 0.1992, Train Accuracy: 0.9642, Val Accuracy: 0.7778, Iter Time: 8.78s\n",
      "Epoch [266/700], Train Loss: 0.1084, Train Accuracy: 0.9740, Val Accuracy: 0.7639, Iter Time: 8.84s\n",
      "Epoch [267/700], Train Loss: 0.1099, Train Accuracy: 0.9744, Val Accuracy: 0.7222, Iter Time: 8.84s\n",
      "Epoch [268/700], Train Loss: 0.1461, Train Accuracy: 0.9697, Val Accuracy: 0.8264, Iter Time: 8.86s\n",
      "Epoch [269/700], Train Loss: 0.0301, Train Accuracy: 0.9927, Val Accuracy: 0.8056, Iter Time: 9.08s\n",
      "Epoch [270/700], Train Loss: 0.0472, Train Accuracy: 0.9923, Val Accuracy: 0.8194, Iter Time: 8.88s\n",
      "Epoch [271/700], Train Loss: 0.0966, Train Accuracy: 0.9792, Val Accuracy: 0.7708, Iter Time: 8.88s\n",
      "Epoch [272/700], Train Loss: 0.2577, Train Accuracy: 0.9448, Val Accuracy: 0.7014, Iter Time: 8.96s\n",
      "Epoch [273/700], Train Loss: 0.4214, Train Accuracy: 0.9061, Val Accuracy: 0.6736, Iter Time: 8.87s\n",
      "Epoch [274/700], Train Loss: 0.2724, Train Accuracy: 0.9437, Val Accuracy: 0.6736, Iter Time: 8.73s\n",
      "Epoch [275/700], Train Loss: 0.2011, Train Accuracy: 0.9550, Val Accuracy: 0.7500, Iter Time: 8.78s\n",
      "Epoch [276/700], Train Loss: 0.1401, Train Accuracy: 0.9719, Val Accuracy: 0.7986, Iter Time: 8.96s\n",
      "Epoch [277/700], Train Loss: 0.0699, Train Accuracy: 0.9865, Val Accuracy: 0.8056, Iter Time: 9.14s\n",
      "Epoch [278/700], Train Loss: 0.0228, Train Accuracy: 0.9942, Val Accuracy: 0.8264, Iter Time: 8.36s\n",
      "Epoch [279/700], Train Loss: 0.0103, Train Accuracy: 0.9982, Val Accuracy: 0.8194, Iter Time: 9.10s\n",
      "Epoch [280/700], Train Loss: 0.0013, Train Accuracy: 0.9996, Val Accuracy: 0.8333, Iter Time: 8.71s\n",
      "Epoch [281/700], Train Loss: 0.0090, Train Accuracy: 0.9993, Val Accuracy: 0.8264, Iter Time: 8.81s\n",
      "Epoch [282/700], Train Loss: 0.0069, Train Accuracy: 0.9993, Val Accuracy: 0.8542, Iter Time: 9.40s\n",
      "Epoch [283/700], Train Loss: 0.0025, Train Accuracy: 0.9993, Val Accuracy: 0.8194, Iter Time: 8.84s\n",
      "Epoch [284/700], Train Loss: 0.0012, Train Accuracy: 0.9996, Val Accuracy: 0.8333, Iter Time: 9.32s\n",
      "Epoch [285/700], Train Loss: 0.0046, Train Accuracy: 0.9993, Val Accuracy: 0.8472, Iter Time: 9.50s\n",
      "Epoch [286/700], Train Loss: 0.0039, Train Accuracy: 0.9993, Val Accuracy: 0.8264, Iter Time: 9.75s\n",
      "Epoch [287/700], Train Loss: 0.0029, Train Accuracy: 0.9993, Val Accuracy: 0.8194, Iter Time: 9.06s\n",
      "Epoch [288/700], Train Loss: 0.3103, Train Accuracy: 0.9507, Val Accuracy: 0.3889, Iter Time: 9.04s\n",
      "Epoch [289/700], Train Loss: 2.1560, Train Accuracy: 0.6517, Val Accuracy: 0.6736, Iter Time: 9.03s\n",
      "Epoch [290/700], Train Loss: 0.2187, Train Accuracy: 0.9408, Val Accuracy: 0.7431, Iter Time: 9.21s\n",
      "Epoch [291/700], Train Loss: 0.1090, Train Accuracy: 0.9711, Val Accuracy: 0.7361, Iter Time: 9.15s\n",
      "Epoch [292/700], Train Loss: 0.0465, Train Accuracy: 0.9879, Val Accuracy: 0.7639, Iter Time: 8.93s\n",
      "Epoch [293/700], Train Loss: 0.0226, Train Accuracy: 0.9945, Val Accuracy: 0.7847, Iter Time: 9.13s\n",
      "Epoch [294/700], Train Loss: 0.0086, Train Accuracy: 0.9982, Val Accuracy: 0.7708, Iter Time: 9.06s\n",
      "Epoch [295/700], Train Loss: 0.0022, Train Accuracy: 0.9996, Val Accuracy: 0.7778, Iter Time: 9.01s\n",
      "Epoch [296/700], Train Loss: 0.0024, Train Accuracy: 0.9996, Val Accuracy: 0.7778, Iter Time: 9.03s\n",
      "Epoch [297/700], Train Loss: 0.0013, Train Accuracy: 0.9996, Val Accuracy: 0.7778, Iter Time: 9.21s\n",
      "Epoch [298/700], Train Loss: 0.0013, Train Accuracy: 0.9996, Val Accuracy: 0.7778, Iter Time: 9.26s\n",
      "Epoch [299/700], Train Loss: 0.0012, Train Accuracy: 0.9996, Val Accuracy: 0.7778, Iter Time: 9.11s\n",
      "Epoch [300/700], Train Loss: 0.0011, Train Accuracy: 0.9996, Val Accuracy: 0.7778, Iter Time: 9.05s\n",
      "Epoch [301/700], Train Loss: 0.0016, Train Accuracy: 0.9996, Val Accuracy: 0.7847, Iter Time: 9.08s\n",
      "Epoch [302/700], Train Loss: 0.0012, Train Accuracy: 0.9996, Val Accuracy: 0.7847, Iter Time: 8.94s\n",
      "Epoch [303/700], Train Loss: 0.0009, Train Accuracy: 0.9996, Val Accuracy: 0.7847, Iter Time: 9.25s\n",
      "Epoch [304/700], Train Loss: 0.0012, Train Accuracy: 0.9996, Val Accuracy: 0.7847, Iter Time: 9.97s\n",
      "Epoch [305/700], Train Loss: 0.0012, Train Accuracy: 0.9996, Val Accuracy: 0.7847, Iter Time: 9.67s\n",
      "Epoch [306/700], Train Loss: 0.0007, Train Accuracy: 0.9996, Val Accuracy: 0.7847, Iter Time: 8.87s\n",
      "Epoch [307/700], Train Loss: 0.4266, Train Accuracy: 0.9335, Val Accuracy: 0.3681, Iter Time: 8.89s\n",
      "Epoch [308/700], Train Loss: 1.6615, Train Accuracy: 0.7010, Val Accuracy: 0.6806, Iter Time: 9.11s\n",
      "Epoch [309/700], Train Loss: 0.2536, Train Accuracy: 0.9295, Val Accuracy: 0.7500, Iter Time: 8.99s\n",
      "Epoch [310/700], Train Loss: 0.0851, Train Accuracy: 0.9799, Val Accuracy: 0.7917, Iter Time: 8.83s\n",
      "Epoch [311/700], Train Loss: 0.0345, Train Accuracy: 0.9927, Val Accuracy: 0.8472, Iter Time: 8.78s\n",
      "Epoch [312/700], Train Loss: 0.0086, Train Accuracy: 0.9978, Val Accuracy: 0.8542, Iter Time: 8.77s\n",
      "Epoch [313/700], Train Loss: 0.0127, Train Accuracy: 0.9974, Val Accuracy: 0.8333, Iter Time: 8.79s\n",
      "Epoch [314/700], Train Loss: 0.0156, Train Accuracy: 0.9974, Val Accuracy: 0.8333, Iter Time: 8.84s\n",
      "Epoch [315/700], Train Loss: 0.0093, Train Accuracy: 0.9989, Val Accuracy: 0.8472, Iter Time: 8.83s\n",
      "Epoch [316/700], Train Loss: 0.0031, Train Accuracy: 0.9989, Val Accuracy: 0.8403, Iter Time: 8.86s\n",
      "Epoch [317/700], Train Loss: 0.0138, Train Accuracy: 0.9963, Val Accuracy: 0.7847, Iter Time: 8.92s\n",
      "Epoch [318/700], Train Loss: 0.2842, Train Accuracy: 0.9412, Val Accuracy: 0.6875, Iter Time: 8.88s\n",
      "Epoch [319/700], Train Loss: 0.3264, Train Accuracy: 0.9229, Val Accuracy: 0.6875, Iter Time: 8.94s\n",
      "Epoch [320/700], Train Loss: 0.2597, Train Accuracy: 0.9426, Val Accuracy: 0.7222, Iter Time: 8.94s\n",
      "Epoch [321/700], Train Loss: 0.1133, Train Accuracy: 0.9719, Val Accuracy: 0.7778, Iter Time: 8.97s\n",
      "Epoch [322/700], Train Loss: 0.0619, Train Accuracy: 0.9876, Val Accuracy: 0.7569, Iter Time: 8.90s\n",
      "Epoch [323/700], Train Loss: 0.0278, Train Accuracy: 0.9923, Val Accuracy: 0.7917, Iter Time: 8.79s\n",
      "Epoch [324/700], Train Loss: 0.0396, Train Accuracy: 0.9927, Val Accuracy: 0.7708, Iter Time: 8.73s\n",
      "Epoch [325/700], Train Loss: 0.0643, Train Accuracy: 0.9934, Val Accuracy: 0.7569, Iter Time: 8.77s\n",
      "Epoch [326/700], Train Loss: 0.0860, Train Accuracy: 0.9825, Val Accuracy: 0.7431, Iter Time: 8.80s\n",
      "Epoch [327/700], Train Loss: 0.4755, Train Accuracy: 0.9115, Val Accuracy: 0.7083, Iter Time: 8.93s\n",
      "Epoch [328/700], Train Loss: 0.3045, Train Accuracy: 0.9371, Val Accuracy: 0.7222, Iter Time: 9.10s\n",
      "Epoch [329/700], Train Loss: 0.1006, Train Accuracy: 0.9737, Val Accuracy: 0.8056, Iter Time: 10.18s\n",
      "Epoch [330/700], Train Loss: 0.0671, Train Accuracy: 0.9865, Val Accuracy: 0.7639, Iter Time: 8.87s\n",
      "Epoch [331/700], Train Loss: 0.0374, Train Accuracy: 0.9894, Val Accuracy: 0.7292, Iter Time: 10.09s\n",
      "Epoch [332/700], Train Loss: 0.1064, Train Accuracy: 0.9773, Val Accuracy: 0.7917, Iter Time: 9.43s\n",
      "Early stopping after 332 epochs\n",
      "prediction: ['H', '6', '8', 'F', 'U', '9', 'H', 'M', 'S', '1', 'H', 'I', 'S', 'F', '8']\n",
      "real labels: ['A', 'B', 'N', 'O', 'R', 'M', 'A', 'L', 'I', 'Z', 'A', 'T', 'I', 'O', 'N']\n",
      "prediction: [0, 0, 0, 0, 0, 0, 1, 10, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 4]\n",
      "true labels: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3]\n",
      "Fold 2 FINAL RESULTS: \n",
      "accuracy: 0.036 // precision: 0.029 // recall: 0.031 // f1: 0.030\n",
      "\n",
      "Fold 3/5\n",
      "2880\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[120], line 45\u001B[0m\n\u001B[1;32m     43\u001B[0m         X_train \u001B[38;5;241m=\u001B[39m [(t[\u001B[38;5;241m0\u001B[39m],t[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m final_train_set]\n\u001B[1;32m     44\u001B[0m         y_train \u001B[38;5;241m=\u001B[39m [t[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m final_train_set]\n\u001B[0;32m---> 45\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n\u001B[1;32m     47\u001B[0m final_test_set \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     48\u001B[0m test_set \u001B[38;5;241m=\u001B[39m Subset(dataset, val_idx)\n",
      "Cell \u001B[0;32mIn[118], line 65\u001B[0m, in \u001B[0;36mCoAtNet.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n\u001B[1;32m     64\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     66\u001B[0m     scheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     68\u001B[0m toc \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:75\u001B[0m, in \u001B[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     73\u001B[0m instance\u001B[38;5;241m.\u001B[39m_step_count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     74\u001B[0m wrapped \u001B[38;5;241m=\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__get__\u001B[39m(instance, \u001B[38;5;28mcls\u001B[39m)\n\u001B[0;32m---> 75\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    381\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    382\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    383\u001B[0m             )\n\u001B[0;32m--> 385\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    388\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:166\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    155\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    157\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    158\u001B[0m         group,\n\u001B[1;32m    159\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    163\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    164\u001B[0m         state_steps)\n\u001B[0;32m--> 166\u001B[0m     adam(\n\u001B[1;32m    167\u001B[0m         params_with_grad,\n\u001B[1;32m    168\u001B[0m         grads,\n\u001B[1;32m    169\u001B[0m         exp_avgs,\n\u001B[1;32m    170\u001B[0m         exp_avg_sqs,\n\u001B[1;32m    171\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    172\u001B[0m         state_steps,\n\u001B[1;32m    173\u001B[0m         amsgrad\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mamsgrad\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    174\u001B[0m         has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    175\u001B[0m         beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    176\u001B[0m         beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    177\u001B[0m         lr\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    178\u001B[0m         weight_decay\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    179\u001B[0m         eps\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    180\u001B[0m         maximize\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    181\u001B[0m         foreach\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mforeach\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    182\u001B[0m         capturable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    183\u001B[0m         differentiable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    184\u001B[0m         fused\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfused\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    185\u001B[0m         grad_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_scale\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    186\u001B[0m         found_inf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    187\u001B[0m     )\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:316\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    314\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 316\u001B[0m func(params,\n\u001B[1;32m    317\u001B[0m      grads,\n\u001B[1;32m    318\u001B[0m      exp_avgs,\n\u001B[1;32m    319\u001B[0m      exp_avg_sqs,\n\u001B[1;32m    320\u001B[0m      max_exp_avg_sqs,\n\u001B[1;32m    321\u001B[0m      state_steps,\n\u001B[1;32m    322\u001B[0m      amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[1;32m    323\u001B[0m      has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    324\u001B[0m      beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    325\u001B[0m      beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    326\u001B[0m      lr\u001B[38;5;241m=\u001B[39mlr,\n\u001B[1;32m    327\u001B[0m      weight_decay\u001B[38;5;241m=\u001B[39mweight_decay,\n\u001B[1;32m    328\u001B[0m      eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m    329\u001B[0m      maximize\u001B[38;5;241m=\u001B[39mmaximize,\n\u001B[1;32m    330\u001B[0m      capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[1;32m    331\u001B[0m      differentiable\u001B[38;5;241m=\u001B[39mdifferentiable,\n\u001B[1;32m    332\u001B[0m      grad_scale\u001B[38;5;241m=\u001B[39mgrad_scale,\n\u001B[1;32m    333\u001B[0m      found_inf\u001B[38;5;241m=\u001B[39mfound_inf)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:439\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    437\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (max_exp_avg_sqs[i]\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 439\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (exp_avg_sq\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    441\u001B[0m     param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n\u001B[1;32m    443\u001B[0m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:36:51.042555Z",
     "start_time": "2024-10-29T16:36:50.623252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\", add_to_git_credential=True)"
   ],
   "id": "482a15f658086fce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/jorgeleon/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T16:23:15.849762Z",
     "start_time": "2024-10-29T16:23:15.777933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = \"0.0\"\n",
    "\n",
    "# Access the environment variable\n",
    "# print(os.environ.get('MY_VARIABLE'))  # Output: my_value"
   ],
   "id": "d6fc51f9ffcfd378",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "str expected, not float",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[117], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Set the environment variable\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPYTORCH_MPS_HIGH_WATERMARK_RATIO\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n",
      "File \u001B[0;32m<frozen os>:683\u001B[0m, in \u001B[0;36m__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n",
      "File \u001B[0;32m<frozen os>:757\u001B[0m, in \u001B[0;36mencode\u001B[0;34m(value)\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: str expected, not float"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T23:15:56.944010Z",
     "start_time": "2024-10-30T23:15:55.414155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='spellchecker', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Singfng in rhe raun'\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ],
   "id": "a3e60c1fea63918a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singing in the rain\n",
      "Singing in the rear\n",
      "Singing in the rainn\n",
      "Singing in ther rain\n",
      "Singing in teh rain\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T20:51:00.642374Z",
     "start_time": "2024-10-30T20:51:00.602774Z"
    }
   },
   "cell_type": "code",
   "source": "torch.mps.set_per_process_memory_fraction(0.0)",
   "id": "e5591ae605bef10b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T21:51:41.108267Z",
     "start_time": "2024-10-30T21:19:00.925507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"most likely english word from: aaying\"},\n",
    "]\n",
    "device=\"mps\"\n",
    "llama_32=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-3B-Instruct\", device=device)\n",
    "# pipe(messages, max_new_tokens=256)\n",
    "\n",
    "generator = pipeline(model=llama_32, device_map=torch.device('mps'), torch_dtype=torch.float16)\n",
    "generation = generator(\n",
    "    messages,\n",
    "    # do_sample=False,\n",
    "    # temperature=1.0,\n",
    "    # top_p=1,\n",
    "    max_new_tokens=256\n",
    ")\n",
    "\n",
    "print(f\"Generation: {generation[0]['generated_text']}\")"
   ],
   "id": "4361fce179b7b34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b64520b7089947698c83c4bf940ebbd6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00004.safetensors:   1%|          | 31.5M/4.98G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b26fc18330541d0bda3768dc1d0575c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ecfd5d23113749f7ac2baa9517170aa4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f5738dcaed64fe8b1106d328c9a3264"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d815d2942f44836a0c152a00c649e97"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45dab167eee04191842d27e98ab69bd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04aa8dac238b43ba9753a64da28d17ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7b9bd3a4aed478abf112bcbf28bbcfe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d0db373de824170b359ca3233a3b25e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d86893852a64b3f9a2809edc1b31abe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 15\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-3B-Instruct\", device=device)\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# pipe(messages, max_new_tokens=256)\u001B[39;00m\n\u001B[1;32m     14\u001B[0m generator \u001B[38;5;241m=\u001B[39m pipeline(model\u001B[38;5;241m=\u001B[39mllama_32, device_map\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m'\u001B[39m), torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16)\n\u001B[0;32m---> 15\u001B[0m generation \u001B[38;5;241m=\u001B[39m generator(\n\u001B[1;32m     16\u001B[0m     messages,\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;66;03m# do_sample=False,\u001B[39;00m\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m# temperature=1.0,\u001B[39;00m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;66;03m# top_p=1,\u001B[39;00m\n\u001B[1;32m     20\u001B[0m     max_new_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m\n\u001B[1;32m     21\u001B[0m )\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGeneration: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgeneration[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:267\u001B[0m, in \u001B[0;36mTextGenerationPipeline.__call__\u001B[0;34m(self, text_inputs, **kwargs)\u001B[0m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m    263\u001B[0m     text_inputs, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, KeyDataset) \u001B[38;5;28;01mif\u001B[39;00m is_torch_available() \u001B[38;5;28;01melse\u001B[39;00m (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)\n\u001B[1;32m    264\u001B[0m ) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text_inputs[\u001B[38;5;241m0\u001B[39m], (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mdict\u001B[39m)):\n\u001B[1;32m    265\u001B[0m     \u001B[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001B[39;00m\n\u001B[1;32m    266\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text_inputs[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m--> 267\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(Chat(text_inputs), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    269\u001B[0m         chats \u001B[38;5;241m=\u001B[39m [Chat(chat) \u001B[38;5;28;01mfor\u001B[39;00m chat \u001B[38;5;129;01min\u001B[39;00m text_inputs]  \u001B[38;5;66;03m# 🐈 🐈 🐈\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1302\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m   1295\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[1;32m   1296\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1299\u001B[0m         )\n\u001B[1;32m   1300\u001B[0m     )\n\u001B[1;32m   1301\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1309\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[1;32m   1308\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[0;32m-> 1309\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[1;32m   1310\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n\u001B[1;32m   1311\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1209\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[0;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[1;32m   1207\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[1;32m   1208\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m-> 1209\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[1;32m   1210\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:370\u001B[0m, in \u001B[0;36mTextGenerationPipeline._forward\u001B[0;34m(self, model_inputs, **generate_kwargs)\u001B[0m\n\u001B[1;32m    367\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[1;32m    368\u001B[0m     generate_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration_config\n\u001B[0;32m--> 370\u001B[0m generated_sequence \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mgenerate(input_ids\u001B[38;5;241m=\u001B[39minput_ids, attention_mask\u001B[38;5;241m=\u001B[39mattention_mask, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgenerate_kwargs)\n\u001B[1;32m    371\u001B[0m out_b \u001B[38;5;241m=\u001B[39m generated_sequence\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    372\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1993\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1990\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m inputs_tensor\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1992\u001B[0m device \u001B[38;5;241m=\u001B[39m inputs_tensor\u001B[38;5;241m.\u001B[39mdevice\n\u001B[0;32m-> 1993\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_special_tokens(generation_config, kwargs_has_attention_mask, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m   1995\u001B[0m \u001B[38;5;66;03m# decoder-only models must use left-padding for batched generation.\u001B[39;00m\n\u001B[1;32m   1996\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m   1997\u001B[0m     \u001B[38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001B[39;00m\n\u001B[1;32m   1998\u001B[0m     \u001B[38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1842\u001B[0m, in \u001B[0;36mGenerationMixin._prepare_special_tokens\u001B[0;34m(self, generation_config, kwargs_has_attention_mask, device)\u001B[0m\n\u001B[1;32m   1836\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1837\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1838\u001B[0m     )\n\u001B[1;32m   1839\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():  \u001B[38;5;66;03m# Checks that depend on tensor-dependent control flow\u001B[39;00m\n\u001B[1;32m   1840\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1841\u001B[0m         eos_token_tensor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1842\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m isin_mps_friendly(elements\u001B[38;5;241m=\u001B[39meos_token_tensor, test_elements\u001B[38;5;241m=\u001B[39mpad_token_tensor)\u001B[38;5;241m.\u001B[39many()\n\u001B[1;32m   1843\u001B[0m     ):\n\u001B[1;32m   1844\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m kwargs_has_attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs_has_attention_mask:\n\u001B[1;32m   1845\u001B[0m             logger\u001B[38;5;241m.\u001B[39mwarning_once(\n\u001B[1;32m   1846\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe attention mask is not set and cannot be inferred from input because pad token is same as \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1847\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meos token. As a consequence, you may observe unexpected behavior. Please pass your input\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1848\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`attention_mask` to obtain reliable results.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1849\u001B[0m             )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pytorch_utils.py:325\u001B[0m, in \u001B[0;36misin_mps_friendly\u001B[0;34m(elements, test_elements)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    312\u001B[0m \u001B[38;5;124;03mSame as `torch.isin` without flags, but MPS-friendly. We can remove this function when we stop supporting\u001B[39;00m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;124;03mtorch <= 2.3. See https://github.com/pytorch/pytorch/issues/77764#issuecomment-2067838075\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;124;03m    and False otherwise\u001B[39;00m\n\u001B[1;32m    322\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m elements\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_greater_or_equal_than_2_4:\n\u001B[0;32m--> 325\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m elements\u001B[38;5;241m.\u001B[39mtile(test_elements\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39meq(test_elements\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39msum(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mbool()\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    327\u001B[0m     \u001B[38;5;66;03m# Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\u001B[39;00m\n\u001B[1;32m    328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misin(elements, test_elements)\n",
      "\u001B[0;31mIndexError\u001B[0m: tuple index out of range"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T20:40:50.736619Z",
     "start_time": "2024-10-30T20:38:53.318486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "# Set up device for MPS if available\n",
    "device = \"cpu\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\" # replace with the actual model path if necessary\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move model to MPS device\n",
    "model.to(device)\n",
    "\n",
    "# Example input\n",
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids=input_ids, max_length=50)\n",
    "    \n",
    "# Decode and print the output\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ],
   "id": "d9c86036a13240c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73adfc7372b745f1b2a749791127c98e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in the land of Azura, there lived a young girl named Luna. She was a curious and adventurous soul, with a heart full of wonder and a mind full of questions. Luna lived in a small village on the outskirts\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T14:52:47.517437Z",
     "start_time": "2024-10-30T14:52:30.395897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ],
   "id": "dc02416ad5cf61c7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ed0de87fbdf40fe84e730f8133ad020"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T19:05:02.637329Z",
     "start_time": "2024-10-27T18:52:43.662347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n",
    "\n",
    "# Check for MPS availability and use if possible\n",
    "device = 0 if torch.backends.mps.is_available() else -1\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-3B\", device=device)\n",
    "model_id = \"meta-llama/Llama-3.2-3B\""
   ],
   "id": "806e3166728cdf63",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0d7aec3ae564e6e99dc457898abc017"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4e54c3b13464d4fa04c4933c01db486"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b331ebb672224e859382ff4f169da93b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68992e3270034cceab0195d15f650d26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5589b3e290d94d16b1887464525d6c5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9efa30536fe448ca664ba0f62751982"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49a76cc941cf4d468e19e32f43eea30e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0b7bc90cdca464895930c3149b494c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9c6874028ed4febbe4e9c6aa0009bd6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce8aafc03ffa4539b4775f367a6a8fd7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T21:22:25.545844Z",
     "start_time": "2024-10-27T21:21:53.044314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    device=0\n",
    ")\n",
    "\n",
    "# pipe(\"The key to life is\")\n"
   ],
   "id": "5e1f878e0a42072c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6b18d3cd408471f8b5217fa72958477"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T21:24:53.048992Z",
     "start_time": "2024-10-27T21:24:40.362295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\", torch_dtype=torch.float16)"
   ],
   "id": "3941f9bf6f7687b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4e8d9b210db476b9568db2503689a86"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T21:35:24.575175Z",
     "start_time": "2024-10-27T21:35:24.486674Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9320db9e593fde4d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m pipe(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhow are you\u001B[39m\u001B[38;5;124m'\u001B[39m, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m, pad_token_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50256\u001B[39m, num_return_sequences\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:272\u001B[0m, in \u001B[0;36mTextGenerationPipeline.__call__\u001B[0;34m(self, text_inputs, **kwargs)\u001B[0m\n\u001B[1;32m    270\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(chats, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 272\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(text_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1302\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m   1295\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[1;32m   1296\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1299\u001B[0m         )\n\u001B[1;32m   1300\u001B[0m     )\n\u001B[1;32m   1301\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1309\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[1;32m   1308\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[0;32m-> 1309\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[1;32m   1310\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n\u001B[1;32m   1311\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1209\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[0;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[1;32m   1207\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[1;32m   1208\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m-> 1209\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[1;32m   1210\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:370\u001B[0m, in \u001B[0;36mTextGenerationPipeline._forward\u001B[0;34m(self, model_inputs, **generate_kwargs)\u001B[0m\n\u001B[1;32m    367\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[1;32m    368\u001B[0m     generate_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration_config\n\u001B[0;32m--> 370\u001B[0m generated_sequence \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mgenerate(input_ids\u001B[38;5;241m=\u001B[39minput_ids, attention_mask\u001B[38;5;241m=\u001B[39mattention_mask, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgenerate_kwargs)\n\u001B[1;32m    371\u001B[0m out_b \u001B[38;5;241m=\u001B[39m generated_sequence\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    372\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1993\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1990\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m inputs_tensor\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1992\u001B[0m device \u001B[38;5;241m=\u001B[39m inputs_tensor\u001B[38;5;241m.\u001B[39mdevice\n\u001B[0;32m-> 1993\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_special_tokens(generation_config, kwargs_has_attention_mask, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m   1995\u001B[0m \u001B[38;5;66;03m# decoder-only models must use left-padding for batched generation.\u001B[39;00m\n\u001B[1;32m   1996\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m   1997\u001B[0m     \u001B[38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001B[39;00m\n\u001B[1;32m   1998\u001B[0m     \u001B[38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1842\u001B[0m, in \u001B[0;36mGenerationMixin._prepare_special_tokens\u001B[0;34m(self, generation_config, kwargs_has_attention_mask, device)\u001B[0m\n\u001B[1;32m   1836\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1837\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1838\u001B[0m     )\n\u001B[1;32m   1839\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():  \u001B[38;5;66;03m# Checks that depend on tensor-dependent control flow\u001B[39;00m\n\u001B[1;32m   1840\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1841\u001B[0m         eos_token_tensor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1842\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m isin_mps_friendly(elements\u001B[38;5;241m=\u001B[39meos_token_tensor, test_elements\u001B[38;5;241m=\u001B[39mpad_token_tensor)\u001B[38;5;241m.\u001B[39many()\n\u001B[1;32m   1843\u001B[0m     ):\n\u001B[1;32m   1844\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m kwargs_has_attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs_has_attention_mask:\n\u001B[1;32m   1845\u001B[0m             logger\u001B[38;5;241m.\u001B[39mwarning_once(\n\u001B[1;32m   1846\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe attention mask is not set and cannot be inferred from input because pad token is same as \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1847\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meos token. As a consequence, you may observe unexpected behavior. Please pass your input\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1848\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`attention_mask` to obtain reliable results.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1849\u001B[0m             )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pytorch_utils.py:325\u001B[0m, in \u001B[0;36misin_mps_friendly\u001B[0;34m(elements, test_elements)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    312\u001B[0m \u001B[38;5;124;03mSame as `torch.isin` without flags, but MPS-friendly. We can remove this function when we stop supporting\u001B[39;00m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;124;03mtorch <= 2.3. See https://github.com/pytorch/pytorch/issues/77764#issuecomment-2067838075\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;124;03m    and False otherwise\u001B[39;00m\n\u001B[1;32m    322\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m elements\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_greater_or_equal_than_2_4:\n\u001B[0;32m--> 325\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m elements\u001B[38;5;241m.\u001B[39mtile(test_elements\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39meq(test_elements\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39msum(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mbool()\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    327\u001B[0m     \u001B[38;5;66;03m# Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\u001B[39;00m\n\u001B[1;32m    328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misin(elements, test_elements)\n",
      "\u001B[0;31mIndexError\u001B[0m: tuple index out of range"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T20:37:22.728961Z",
     "start_time": "2024-10-28T20:37:11.525513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define your input sequence\n",
    "input_sequence = \"peter\"\n",
    "inputs = tokenizer(input_sequence, return_tensors=\"pt\").to(\"mps\")\n",
    "# print(inputs)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "# Forward pass to get logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, min_length=10, max_length=50, pad_token_id = tokenizer.eos_token_id)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get the logits of the last token in the sequence\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Set the number of top words you want to retrieve\n",
    "top_k = 15\n",
    "probs = F.softmax(last_token_logits, dim=-1)\n",
    "top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
    "# print(probs)\n",
    "\n",
    "\n",
    "# # Decode the top-k token indices to words\n",
    "predicted_words = [tokenizer.decode([idx]).strip() for idx in top_k_indices]\n",
    "print(predicted_words)\n",
    "\n",
    "# Output the list of most likely words\n",
    "print(\"Top likely words:\", predicted_words)"
   ],
   "id": "51f7f36ebc5e9765",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "['@', 'j', 'h', \"'s\", 'b', 'and', 'm', 'p', 'h', 'j', 'm', 'borough', 'k', 'd', ',']\n",
      "Top likely words: ['@', 'j', 'h', \"'s\", 'b', 'and', 'm', 'p', 'h', 'j', 'm', 'borough', 'k', 'd', ',']\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T22:09:15.111103Z",
     "start_time": "2024-10-27T22:09:14.968999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define your input sequence\n",
    "input_sequence = \"The word starting with 'cat' is: \"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(input_sequence, return_tensors=\"pt\").to(\"mps\")\n",
    "print(inputs)\n",
    "\n",
    "# Generate multiple likely continuations\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=20,                 # Adjust max_length to fit expected word length\n",
    "    num_return_sequences=5,        # Number of likely completions\n",
    "    do_sample=True,                # Enables non-deterministic sampling\n",
    "    top_k=10,                       # Limits to top 10 likely completions\n",
    "    pad_token_id=tokenizer.eos_token_id  # Set pad_token_id to eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and collect unique words\n",
    "predicted_words = set()\n",
    "for o in output:\n",
    "    decoded_text = tokenizer.decode(o, skip_special_tokens=True).strip()\n",
    "    if len(decoded_text) > len(input_sequence):  # Ensure there's content after the prompt\n",
    "        try:\n",
    "            word = decoded_text[len(input_sequence):].split()[0]  # Get the first word after the prompt\n",
    "            predicted_words.add(word)\n",
    "        except IndexError:\n",
    "            continue  # Skip if there's an error\n",
    "\n",
    "# Decode and collect unique words\n",
    "# Decode and collect unique words\n",
    "predicted_words = list(set(\n",
    "    tokenizer.decode(o, skip_special_tokens=True).strip()[len(input_sequence):].split()[0] \n",
    "    for o in output if len(tokenizer.decode(o, skip_special_tokens=True).strip()) > len(input_sequence)\n",
    "))\n",
    "\n",
    "print(\"Top likely words:\", predicted_words)"
   ],
   "id": "5ffcfc5f0114fd9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,    791,   3492,   6041,    449,    364,   4719,      6,    374,\n",
      "             25,    220]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[37], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(inputs)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Generate multiple likely continuations\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m output \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs,\n\u001B[1;32m     11\u001B[0m     max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m,                 \u001B[38;5;66;03m# Adjust max_length to fit expected word length\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     num_return_sequences\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m,        \u001B[38;5;66;03m# Number of likely completions\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     do_sample\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,                \u001B[38;5;66;03m# Enables non-deterministic sampling\u001B[39;00m\n\u001B[1;32m     14\u001B[0m     top_k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,                       \u001B[38;5;66;03m# Limits to top 10 likely completions\u001B[39;00m\n\u001B[1;32m     15\u001B[0m     pad_token_id\u001B[38;5;241m=\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39meos_token_id  \u001B[38;5;66;03m# Set pad_token_id to eos_token_id\u001B[39;00m\n\u001B[1;32m     16\u001B[0m )\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Decode and collect unique words\u001B[39;00m\n\u001B[1;32m     19\u001B[0m predicted_words \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1993\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1990\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m inputs_tensor\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1992\u001B[0m device \u001B[38;5;241m=\u001B[39m inputs_tensor\u001B[38;5;241m.\u001B[39mdevice\n\u001B[0;32m-> 1993\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_special_tokens(generation_config, kwargs_has_attention_mask, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m   1995\u001B[0m \u001B[38;5;66;03m# decoder-only models must use left-padding for batched generation.\u001B[39;00m\n\u001B[1;32m   1996\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m   1997\u001B[0m     \u001B[38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001B[39;00m\n\u001B[1;32m   1998\u001B[0m     \u001B[38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1842\u001B[0m, in \u001B[0;36mGenerationMixin._prepare_special_tokens\u001B[0;34m(self, generation_config, kwargs_has_attention_mask, device)\u001B[0m\n\u001B[1;32m   1836\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1837\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1838\u001B[0m     )\n\u001B[1;32m   1839\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():  \u001B[38;5;66;03m# Checks that depend on tensor-dependent control flow\u001B[39;00m\n\u001B[1;32m   1840\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1841\u001B[0m         eos_token_tensor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1842\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m isin_mps_friendly(elements\u001B[38;5;241m=\u001B[39meos_token_tensor, test_elements\u001B[38;5;241m=\u001B[39mpad_token_tensor)\u001B[38;5;241m.\u001B[39many()\n\u001B[1;32m   1843\u001B[0m     ):\n\u001B[1;32m   1844\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m kwargs_has_attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs_has_attention_mask:\n\u001B[1;32m   1845\u001B[0m             logger\u001B[38;5;241m.\u001B[39mwarning_once(\n\u001B[1;32m   1846\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe attention mask is not set and cannot be inferred from input because pad token is same as \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1847\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meos token. As a consequence, you may observe unexpected behavior. Please pass your input\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1848\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`attention_mask` to obtain reliable results.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1849\u001B[0m             )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pytorch_utils.py:325\u001B[0m, in \u001B[0;36misin_mps_friendly\u001B[0;34m(elements, test_elements)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    312\u001B[0m \u001B[38;5;124;03mSame as `torch.isin` without flags, but MPS-friendly. We can remove this function when we stop supporting\u001B[39;00m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;124;03mtorch <= 2.3. See https://github.com/pytorch/pytorch/issues/77764#issuecomment-2067838075\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;124;03m    and False otherwise\u001B[39;00m\n\u001B[1;32m    322\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m elements\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmps\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_greater_or_equal_than_2_4:\n\u001B[0;32m--> 325\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m elements\u001B[38;5;241m.\u001B[39mtile(test_elements\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39meq(test_elements\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39msum(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mbool()\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    327\u001B[0m     \u001B[38;5;66;03m# Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\u001B[39;00m\n\u001B[1;32m    328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misin(elements, test_elements)\n",
      "\u001B[0;31mIndexError\u001B[0m: tuple index out of range"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "dataset = audioDatasetMfcc + audioDatasetMfccMasking\n",
    "X = [(t[0],t[1]) for t in audioDatasetMfcc]\n",
    "X_masking = [(t[0],t[1]) for t in audioDatasetMfccMasking]\n",
    "y = [t[2] for t in audioDatasetMfcc]\n",
    "y_masking = [t[2] for t in audioDatasetMfccMasking]\n",
    "\n",
    "model = MfccLSTM()\n",
    "\n",
    "param_grid = {\n",
    "    'patience': [120],\n",
    "    'batch_size': [32],\n",
    "}\n",
    "\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted', zero_division=0.0),\n",
    "    'precision_weighted': make_scorer(precision_score, average='weighted', zero_division=0.0),\n",
    "    'recall_weighted': make_scorer(recall_score, average='weighted', zero_division=0.0),\n",
    "}\n",
    "# \n",
    "grid_search = GridSearchCV(MfccLSTM(), param_grid, cv=5, scoring=scoring, refit=False, verbose=3)\n",
    "# # model = CoAtNet(patience=1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.01)\n",
    "grid_search.fit(X + X_masking, y + y_masking)\n",
    "# print(len(X))\n",
    "# model.fit(X+X_masking, y+y_masking)"
   ],
   "id": "d42799b2389b04c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "sorted_df = cv_results_df.sort_values(by=['rank_test_accuracy'])\n",
    "\n",
    "for ind, row in sorted_df.iterrows():\n",
    "    print(f'Rank: {row[\"rank_test_accuracy\"]}')\n",
    "    print(f'Params: {row[\"params\"]}')\n",
    "    print(f'Test accuracy: {row[\"mean_test_accuracy\"]:.3f}', end=\" / \")\n",
    "    print(f'F1 Weighted: {row[\"mean_test_f1_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Recall Weighted: {row[\"mean_test_recall_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Precision Weighted: {row[\"mean_test_precision_weighted\"]:.3f}', end=\"\\n\\n\")\n",
    "\n",
    "grid_search.cv_results_"
   ],
   "id": "45986cc16a06bddd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a52a2f4cde1eda15",
   "metadata": {},
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "dataset = audioDatasetFin + audioDatasetFinMasking\n",
    "X = [t[0] for t in audioDatasetFin]\n",
    "X_masking = [t[0] for t in audioDatasetFinMasking]\n",
    "y = [t[1] for t in audioDatasetFin]\n",
    "y_masking = [t[1] for t in audioDatasetFinMasking]\n",
    "print(np.array(X).shape)\n",
    "# first_el = dataset[0][0]\n",
    "print(np.array(y+y_masking).shape)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "param_grid = {\n",
    "    'num_epochs': [500],\n",
    "    # 'patience': [55, 75, 100],\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted', zero_division=0.0),\n",
    "    'precision_weighted': make_scorer(precision_score, average='weighted', zero_division=0.0),\n",
    "    'recall_weighted': make_scorer(recall_score, average='weighted', zero_division=0.0),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(CoAtNet(), param_grid, cv=5, scoring=scoring, refit=False, verbose=3)\n",
    "# model = CoAtNet(patience=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.01)\n",
    "grid_search.fit(X_train + X_masking, y_train + y_masking)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# grid_search.fit(X, y)\n",
    "# print(np.array(X_train).shape)\n",
    "# print(np.array(y_train).shape)\n",
    "# print(np.array(dataset).shape)\n",
    "# print(np.concatenate((X_train, y_train), axis=3).shape)\n",
    "# model.fit(X_train+X_masking, y_train+y_masking)\n",
    "# print(f'Prediction: {model.predict(np.array(X_test)).shape}')\n",
    "# final_labels_set = [original_set[ind] for ind in y_test]\n",
    "# print(f'Labels: {np.array(final_labels_set).shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f33b0326405e8528",
   "metadata": {},
   "source": [
    "cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "sorted_df = cv_results_df.sort_values(by=['rank_test_accuracy'])\n",
    "\n",
    "for ind, row in sorted_df.iterrows():\n",
    "    print(f'Rank: {row[\"rank_test_accuracy\"]}')\n",
    "    print(f'Params: {row[\"params\"]}')\n",
    "    print(f'Test accuracy: {row[\"mean_test_accuracy\"]:.3f}', end=\" / \")\n",
    "    print(f'F1 Weighted: {row[\"mean_test_f1_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Recall Weighted: {row[\"mean_test_recall_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Precision Weighted: {row[\"mean_test_precision_weighted\"]:.3f}', end=\"\\n\\n\")\n",
    "\n",
    "grid_search.cv_results_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ced4875a148ec525",
   "metadata": {},
   "source": [
    "# cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "# cv_results_df = cv_results_df[[\"params\", \"mean_test_accuracy\", \"rank_test_accuracy\", \"mean_test_f1_weighted\", \"rank_test_f1_weighted\", \"mean_test_precision_weighted\", \"rank_test_precision_weighted\", \n",
    "#  \"mean_test_recall_weighted\", \"rank_test_recall_weighted\"]]\n",
    "# cv_results_df\n",
    "\n",
    "# param_grid = {\n",
    "#     'num_epochs': [500, 700, 1100],\n",
    "#     'patience': [55, 75, 100],\n",
    "# }\n",
    "# \n",
    "# scoring = {\n",
    "#     'accuracy': make_scorer(accuracy_score),\n",
    "#     'f1_weighted': make_scorer(f1_score, average='weighted', zero_division=0.0),\n",
    "#     'precision_weighted': make_scorer(precision_score, average='weighted', zero_division=0.0),\n",
    "#     'recall_weighted': make_scorer(recall_score, average='weighted', zero_division=0.0),\n",
    "# }\n",
    "# \n",
    "# grid_search = GridSearchCV(model, param_grid, cv=5, scoring=scoring, refit=False, verbose=3)\n",
    "# grid_search.fit(X, y)\n",
    "# param_grid = {\n",
    "#     'num_epochs': [500, 700, 1100],\n",
    "#     # 'patience': [10, 15, 20, 30, 50, 100],\n",
    "# }\n",
    "# grid_search = GridSearchCV(MfccLSTM(), param_grid, cv=5, scoring=['accuracy', 'f1', 'precision', 'recall'], refit=False, verbose)\n",
    "# \n",
    "# dataset = audioDatasetMfcc + audioDatasetMfccMasking\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)\n",
    "# # print(X_train.shape)\n",
    "# grid_search.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7c9baaa8831abc71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a6ad6bc1bfba3d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "2204dec2710b9ee8",
   "metadata": {},
   "source": [
    "sorted_df = cv_results_df.sort_values(by=['rank_test_accuracy'])\n",
    "\n",
    "for ind, row in sorted_df.iterrows():\n",
    "    print(f'Rank: {row[\"rank_test_accuracy\"]}')\n",
    "    print(f'Params: {row[\"params\"]}')\n",
    "    print(f'Test accuracy: {row[\"mean_test_accuracy\"]:.3f}', end=\" / \")\n",
    "    print(f'F1 Weighted: {row[\"mean_test_f1_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Recall Weighted: {row[\"mean_test_recall_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Precision Weighted: {row[\"mean_test_precision_weighted\"]:.3f}', end=\"\\n\\n\")\n",
    "\n",
    "# best_params_list = grid_search.cv_results_['rank_test_accuracy']\n",
    "# print(grid_search.cv_results_)\n",
    "# print(best_params_list)\n",
    "# print(best_params_list)\n",
    "# print(best_params_list)\n",
    "# print(grid_search.cv_results_[\"mean_test_accuracy\"])\n",
    "# \n",
    "# for i in best_params_list:\n",
    "#     print(f'Parameter {i-1}: {grid_search.cv_results_[\"params\"][i-1]}')\n",
    "#     print(f'Mean Test accuracy: {grid_search.cv_results_[\"mean_test_accuracy\"][i-1]:.3f}')\n",
    "#     print(f'Std Test accuracy: {grid_search.cv_results_[\"std_test_accuracy\"][i-1]:.3f}')\n",
    "#     # print(f'Mean F1 weighted: {grid_search.cv_results_[\"mean_test_f1_weighted\"][i-1]:.3f}')\n",
    "#     # print(f'Mean Recall weighted: {grid_search.cv_results_[\"mean_test_recall_weighted\"][i-1]:.3f}')\n",
    "#     # print(f'Mean Precision weighted: {grid_search.cv_results_[\"mean_test_precision_weighted\"][i-1]:.3f}')\n",
    "#     print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "da929a94a783f0c0",
   "metadata": {},
   "source": [
    "dataset = audioDatasetFin + audioDatasetFinMasking\n",
    "# X = np.array(dataset)[:, :1]\n",
    "# y = np.array(dataset)[:, 2]\n",
    "# print(X.shape)\n",
    "# print(dataset[0])\n",
    "X = [t[0] for t in dataset]\n",
    "y = [t[1] for t in dataset]\n",
    "\n",
    "param_grid = {\n",
    "    'num_epochs': [500, 700, 1100],\n",
    "    'patience': [10, 15, 20, 30, 50, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(CoAtNet(), param_grid, cv=10, scoring=['accuracy', 'f1', 'precision', 'recall'], refit=False, verbose=3)\n",
    "\n",
    "dataset = audioDatasetMfcc + audioDatasetMfccMasking\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y),test_size=0.2)\n",
    "\n",
    "                                                    \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "grid_search.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dbb113bd8be14eda",
   "metadata": {},
   "source": [
    "param_grid = {\n",
    "    'num_epochs': [500, 700, 1100],\n",
    "    'patience': [10, 15, 20, 30, 50, 100],\n",
    "}\n",
    "grid_search = GridSearchCV(MfccLSTM(), param_grid, cv=10, scoring=['accuracy', 'f1', 'precision', 'recall'], refit=False)\n",
    "\n",
    "dataset = audioDatasetMfcc + audioDatasetMfccMasking\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)\n",
    "# print(X_train.shape)\n",
    "grid_search.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "876758bd07bc26b7",
   "metadata": {},
   "source": [
    "# Model architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=36):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.LazyLinear(512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 14 * 14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e1ab811f0d1abfe",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "def train_cnnlstm_with_cross_val(dataset, num_epochs, model_name, device, num_classes=36, patience=10, random_state=42, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        print(f'Fold {fold+1}/{n_splits}')\n",
    "        \n",
    "        # Split the dataset into training and validation sets\n",
    "        train_set = Subset(dataset, train_idx)\n",
    "        val_set = Subset(dataset, val_idx)\n",
    "        train_loader = DataLoader(train_set, batch_size=16)\n",
    "        val_loader = DataLoader(val_set, batch_size=16)\n",
    "        \n",
    "        # Initialize model, optimizer, and loss function\n",
    "        model = MfccLSTM(input_size=20, hidden_size=32, num_classes=num_classes, output_size=64)\n",
    "        model = model.to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_acc, epochs_no_imp = 0, 0\n",
    "        train_accuracies, val_accuracies = [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            epoch_train_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            tic = time.perf_counter()\n",
    "            \n",
    "            for images, sequences, labels in train_loader:\n",
    "                images = images.to(device)\n",
    "                sequences = sequences.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                #converting labels to Long to avoid error \"not implemented for Int\"\n",
    "                labels = labels.long()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images, sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_train_loss += loss.item() * images.size(0)\n",
    "\n",
    "                _, predicted_train = torch.max(outputs.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted_train == labels).sum().item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            toc = time.perf_counter()\n",
    "            time_taken = toc - tic\n",
    "            \n",
    "            epoch_train_loss /= len(train_loader.dataset)\n",
    "            train_accuracy = correct_train / total_train\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            # Evaluation of the model\n",
    "            model.eval()\n",
    "            total, correct = 0, 0\n",
    "            for images, sequences, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                sequences = sequences.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images, sequences)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            #\n",
    "            val_accuracy = correct / total\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}, Iter Time: {time_taken:.2f}s\")\n",
    "                \n",
    "            if val_accuracy > best_val_acc:\n",
    "                best_val_acc = val_accuracy\n",
    "                epochs_no_imp = 0\n",
    "                best_model_state = model.state_dict()  # Save the best model\n",
    "            else:\n",
    "                epochs_no_imp += 1\n",
    "            if epochs_no_imp >= patience:\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "                model.load_state_dict(best_model_state)  # Load the best model\n",
    "                break\n",
    "        \n",
    "        fold_results.append(best_val_acc) \n",
    "        print(f'Fold {fold+1} Best Validation Accuracy: {best_val_acc:.4f}')\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    print(f'Training final results: {fold_results}')\n",
    "    \n",
    "    return num_epochs, np.average(fold_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "245a5cae2ee3f120",
   "metadata": {},
   "source": [
    "import time\n",
    "from coatnet import CoAtNet as CoAtNetImp\n",
    "\n",
    "num_blocks = [2, 2, 3, 5, 2]            # L\n",
    "channels = [64, 96, 192, 384, 768]      # D\n",
    "\n",
    "def train_coatnet_with_cross_val(dataset, num_epochs, model_name, device_external, num_classes=36, patience=10, random_state=42):\n",
    "    train_set, val_set = train_test_split(dataset, test_size=0.2, random_state=random_state)\n",
    "    train_loader, val_loader = DataLoader(train_set, batch_size=16), DataLoader(val_set, batch_size=16)\n",
    "    \n",
    "    # Initialize model, optimizer, and loss function\n",
    "    model = CoAtNetImp((64, 64), 1, num_blocks, channels, num_classes=num_classes)\n",
    "    device = torch.device(device_external) #default to mps\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc, epochs_no_imp = 0, 0\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        tic = time.perf_counter()\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            labels = labels.long() # converting labels to Long to avoid error \"not implemented for Int\"\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "            _, predicted_train = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted_train == labels).sum().item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        toc = time.perf_counter()\n",
    "        time_taken = toc - tic\n",
    "        \n",
    "        epoch_train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Evaluation of the model\n",
    "        model.eval()\n",
    "        total, correct = 0, 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}, Iter Time: {time_taken:.2f}s\")\n",
    "            \n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            epochs_no_imp = 0\n",
    "            best_model_state = model.state_dict()  # Save the best model\n",
    "        else:\n",
    "            epochs_no_imp += 1\n",
    "        if epochs_no_imp >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs')\n",
    "            model.load_state_dict(best_model_state)  # Load the best model\n",
    "            break\n",
    "            \n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    return epoch+1, best_val_acc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e58aef22cf15f974",
   "metadata": {},
   "source": [
    "def predict_mfcc(dataset, model_path, device_external):\n",
    "    images_test_set = [t[0] for t in dataset]\n",
    "    sequences_test_set = [t[1] for t in dataset]\n",
    "    \n",
    "    images = torch.stack(images_test_set)\n",
    "    sequences = torch.stack(sequences_test_set)\n",
    "    device = torch.device(device_external) #default to mps\n",
    "    images = images.to(device)\n",
    "    sequences = sequences.to(device)\n",
    "    model = MfccLSTM(input_size=20, hidden_size=32, num_classes=36, output_size=64)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path,map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images, sequences)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    pred = []\n",
    "    keyss = '1234567890QWERTYUIOPASDFGHJKLZXCVBNMÑ+-'\n",
    "    phrase = predicted.tolist()\n",
    "    for i in range(len(phrase)):\n",
    "        pred.append(keyss[phrase[i]])\n",
    "\n",
    "    pred_df = pd.DataFrame(pred)\n",
    "    return pred_df\n",
    "\n",
    "\n",
    "def predict(dataset, model_obj, argnames, model_path, device_external):\n",
    "    fin_dict = {}\n",
    "\n",
    "    # create the list with each of the ith range tuples\n",
    "    for i in range(len(dataset[0])-1):\n",
    "        fin_dict[argnames[i]] = [t[i] for t in dataset]\n",
    "        \n",
    "    # specify device: default to mps\n",
    "    device = torch.device(device_external) \n",
    "    \n",
    "    # torch.stack each one of the lists\n",
    "    for key in fin_dict.keys():\n",
    "        fin_dict[key] = torch.stack(fin_dict[key]).to(device)\n",
    "    \n",
    "    # model specifying\n",
    "    model = model_obj.to(device)\n",
    "    model.load_state_dict(torch.load(model_path,map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**fin_dict)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    pred = []\n",
    "    keyss = '1234567890QWERTYUIOPASDFGHJKLZXCVBNMÑ+-'\n",
    "\n",
    "    phrase = predicted.tolist()\n",
    "    for i in range(len(phrase)):\n",
    "        pred.append(keyss[phrase[i]])\n",
    "\n",
    "    pred_df = pd.DataFrame(pred)\n",
    "    return pred_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dea4576ee7857b85",
   "metadata": {},
   "source": [
    "def save_csv(model_name, num_epochs, description, accuracy, precision, recall, f1_score):\n",
    "    csv_file_path = 'model_comparison.csv'\n",
    "    \n",
    "    # Read the existing CSV file into a DataFrame\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, create an empty DataFrame with the correct columns\n",
    "        df = pd.DataFrame(columns=['Datetime', 'Name', 'Epochs', 'Description', 'Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "        \n",
    "    # Data to append\n",
    "    current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Remove newline characters from the description\n",
    "    description = description.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    \n",
    "    # Create a new column with the relevant information\n",
    "    new_data = {\n",
    "        'Datetime': [current_datetime],\n",
    "        'Name': [model_name],\n",
    "        'Epochs': [num_epochs],\n",
    "        'Description': [description],\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F1': [f1_score],\n",
    "    }\n",
    "    \n",
    "    new_df = pd.DataFrame(new_data)\n",
    "    \n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(csv_file_path, index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7407a641de41b299",
   "metadata": {},
   "source": [
    "# current random state to split the dataset\n",
    "random_state = 45\n",
    "curr_day = datetime.today().strftime('%Y-%m-%d')\n",
    "curr_time = datetime.today().strftime(\"%H:%M:%S\")\n",
    "datasets = [audioDatasetFin]\n",
    "patience = 15\n",
    "torch.manual_seed(random_state)\n",
    "\n",
    "for dataset in datasets:\n",
    "    # TRAIN PART\n",
    "    train_final_set, test_set = train_test_split(dataset, test_size=0.2, random_state=random_state)\n",
    "    num_epochs = 3\n",
    "    if dataset is audioDatasetMfcc:\n",
    "        for curr_dataset in [dataset, dataset + audioDatasetMfccMasking]:\n",
    "            main_architecture = f\"CNN_LSTM{'_masking' if curr_dataset != audioDatasetMfcc else ''}\"\n",
    "            # Current train function\n",
    "            train_function = train_cnnlstm_with_cross_val\n",
    "            model = MfccLSTM(input_size=20, hidden_size=32, num_classes=len(keys_2), output_size=64)\n",
    "            params_array = [\"images\", \"sequences\"]\n",
    "            description = f\"2 layer CNN (32 and 64 output channels) with final 2 Dense Layers (512 and num_classes) result concatenated with \\n 2 LSTMs (hidden_size=32),  from mfcc with 2 Dense Layers (64 and 16) with a final Lazy Linear layer output of num_classes. Dataset using time shift and masking. {'Original dataset' if original else 'Custom dataset'}. {'Masking Used' if dataset == audioDatasetMfccMasking else 'No Masking'}. Patience={patience}. Random_state={random_state}\"\n",
    "            model_name = f\"model_multiclass_{num_epochs}_{main_architecture}_{curr_day}_{curr_time}{'_custom' if not original and not concat else '_concat' if concat else '_original'}.pth\"\n",
    "            print(f'CURRENT MODEL: {model_name}:')\n",
    "            real_num_epochs, best_val_acc = train_function(curr_dataset, num_epochs, model_name, device, num_classes=len(complete_set if not original or concat else original_set), patience=patience, random_state=random_state)\n",
    "            # PREDICT PART\n",
    "            prediction = predict(test_set, model, params_array, model_name, device)\n",
    "            labels_set = [t[-1] for t in test_set]\n",
    "            final_labels_set = [complete_set[ind] for ind in labels_set]\n",
    "                \n",
    "            # Metrics\n",
    "            accuracy = accuracy_score(final_labels_set, prediction[0])\n",
    "            precision = precision_score(final_labels_set, prediction[0], average='macro')\n",
    "            recall = recall_score(final_labels_set, prediction[0], average='macro')\n",
    "            f1 = sklearn.metrics.f1_score(final_labels_set, prediction[0], average='macro')\n",
    "            \n",
    "            # Save csv data for later comparison\n",
    "            # save_csv(model_name, int(real_num_epochs), description, accuracy, precision, recall, f1)\n",
    "            \n",
    "            # Final results\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(description)\n",
    "            print(f\"Epochs: {num_epochs}\")\n",
    "            print(f\"Accuracy: {accuracy:.3f}\")\n",
    "            print(f\"Precision: {precision:.3f}\")\n",
    "            print(f\"Recall: {recall:.3f}\")\n",
    "            print(f\"F1 Score: {f1:.3f}\")\n",
    "            print(f\"Best val accuracy: {best_val_acc:.3f}\")\n",
    "    elif dataset is audioDatasetFin:\n",
    "        for curr_dataset in [dataset, dataset + audioDatasetFinMasking]:\n",
    "            main_architecture = f\"CoAtNetImp{'_masking' if curr_dataset != audioDatasetFin else ''}\"\n",
    "            # Current train function\n",
    "            train_function = train_coatnet_with_cross_val\n",
    "            model = CoAtNetImp((64, 64), 1, num_blocks, channels, num_classes=len(original_set))\n",
    "            params_array = [\"x\"]\n",
    "            description = f\"Imported CoAtNet model, with 2 Conv layers and then 2 Attention layers followed by a fully connected layer. {'Original dataset' if original else 'Custom dataset'}. {'Masking Used' if dataset == audioDatasetFinMasking else 'No masking'}. Patience={patience}. Random_state={random_state}\"\n",
    "            model_name = f\"model_multiclass_{num_epochs}_{main_architecture}_{curr_day}_{curr_time}{'_custom' if not original and not concat else '_concat' if concat else '_original'}.pth\"\n",
    "            print(f'CURRENT MODEL: {model_name}:')\n",
    "            real_num_epochs, best_val_acc = train_function(curr_dataset, num_epochs, model_name, device, num_classes=len(complete_set if not original or concat else original_set), patience=patience, random_state=random_state)\n",
    "            # PREDICT PART\n",
    "            prediction = np.squeeze(predict(test_set, model, params_array, model_name, device).to_numpy().T)\n",
    "            print(f'Prediction.shape: {prediction.shape}')\n",
    "            labels_set = [t[-1] for t in test_set]\n",
    "            final_labels_set = [complete_set[ind] for ind in labels_set]\n",
    "            print(f'final_labels_set.shape: {np.array(final_labels_set).shape}')\n",
    "                \n",
    "            # # Metrics\n",
    "            # accuracy = accuracy_score(final_labels_set, prediction[0])\n",
    "            # precision = precision_score(final_labels_set, prediction[0], average='macro')\n",
    "            # recall = recall_score(final_labels_set, prediction[0], average='macro')\n",
    "            # f1 = sklearn.metrics.f1_score(final_labels_set, prediction[0], average='macro')\n",
    "            \n",
    "            # Save csv data for later comparison\n",
    "            # save_csv(model_name, int(real_num_epochs), description, accuracy, precision, recall, f1)\n",
    "            \n",
    "            # Final results\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(description)\n",
    "            print(f\"Epochs: {num_epochs}\")\n",
    "            print(f\"Accuracy: {accuracy:.3f}\")\n",
    "            print(f\"Precision: {precision:.3f}\")\n",
    "            print(f\"Recall: {recall:.3f}\")\n",
    "            print(f\"F1 Score: {f1:.3f}\")\n",
    "            print(f\"Best val accuracy: {best_val_acc:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca3565e42e7f87ad",
   "metadata": {},
   "source": [
    "\n",
    "grid_search = GridSearchCV(CoAtNetImp, {})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd43561c586fd9b1",
   "metadata": {},
   "source": [
    "train_final_set, test_set = train_test_split(audioDatasetMfcc, test_size=0.2, random_state=1)\n",
    "\n",
    "model_name = \"model_multiclass_500_CNN_LSTM_masking_2024-09-08.pth\"\n",
    "# model_name = \"model_multiclass_500_CoAtNetImp_2024-09-08.pth\"\n",
    "# model = CoAtNetImp((64, 64), 1, num_blocks, channels, num_classes=len(complete_set))\n",
    "model = MfccLSTM(input_size=20, hidden_size=32, num_classes=len(complete_set), output_size=64)\n",
    "# arg_names = [\"x\"]\n",
    "arg_names = [\"images\", \"sequences\"]\n",
    "prediction = predict(test_set, model, arg_names, model_name, device)\n",
    "labels_set = [t[-1] for t in test_set]\n",
    "final_labels_set = pd.Series([complete_set[ind] for ind in labels_set])\n",
    "print(\"PREDICTION\")\n",
    "print(prediction[0])\n",
    "print(\"FINAL LABELS SET\")\n",
    "print(final_labels_set)\n",
    "# Metrics\n",
    "accuracy = accuracy_score(final_labels_set, prediction[0])\n",
    "precision = precision_score(final_labels_set, prediction[0], average='macro')\n",
    "recall = recall_score(final_labels_set, prediction[0], average='macro')\n",
    "f1 = sklearn.metrics.f1_score(final_labels_set, prediction[0], average='macro')\n",
    "\n",
    "# Save csv data for later comparison\n",
    "# save_csv(model_name, int(real_num_epochs), description, accuracy, precision, recall, f1)\n",
    "\n",
    "# Final results\n",
    "print(f\"Model: {model_name}\")\n",
    "# print(description)\n",
    "# print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n",
    "# print(f\"Best val accuracy: {best_val_acc:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e992282d8cbead9",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for name in [\"model_multiclass_500_CNN_LSTM_2024-09-08.pth\", \"model_multiclass_500_CNN_LSTM_masking_2024-09-08.pth\"]:\n",
    "        \n",
    "    train_final_set, test_set = train_test_split(audioDatasetMfcc, test_size=0.2, random_state=5)\n",
    "    \n",
    "    # model_name = \"model_multiclass_500_CoAtNetImp_masking_2024-09-08.pth\"\n",
    "    model_name = \"model_multiclass_500_CNN_LSTM_2024-09-08.pth\"\n",
    "    # model = CoAtNetImp((64, 64), 1, num_blocks, channels, num_classes=len(keys_2))\n",
    "    model = MfccLSTM(input_size=20, hidden_size=32, num_classes=len(keys_2), output_size=64)\n",
    "    # arg_names = [\"x\"]\n",
    "    arg_names = [\"images\", \"sequences\"]\n",
    "    prediction = predict(test_set, model, arg_names, name, device)\n",
    "    labels_set = [t[-1] for t in test_set]\n",
    "    final_labels_set = [keys_2[ind] for ind in labels_set]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(final_labels_set, prediction[0])\n",
    "    precision = precision_score(final_labels_set, prediction[0], average='macro')\n",
    "    recall = recall_score(final_labels_set, prediction[0], average='macro')\n",
    "    f1 = sklearn.metrics.f1_score(final_labels_set, prediction[0], average='macro')\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(final_labels_set, prediction[0])\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 8})\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "680fb321",
   "metadata": {},
   "source": [
    "# PREDICTION PART\n",
    "\n",
    "# All metrics are calculated from the model with the best validation accuracy\n",
    "# model = MfccLSTM(input_size=20, hidden_size=32, num_classes=21, output_size=64)\n",
    "model = CoAtNetImp((64, 64), 1, num_blocks, channels, num_classes=21)\n",
    "keys_s = '123456789-ABCDEFGHIJ+'\n",
    "\n",
    "# prediction = predict(test_set, model, [\"images\", \"sequences\"],model_name, device)\n",
    "prediction = predict(test_set, model, [\"x\"],model_name, device)\n",
    "# prediction = predict(test_set, model, main_architecture, model_name, random_state)\n",
    "# prediction = predict_mfcc(test_set, model_name, device)\n",
    "labels_set = [t[-1] for t in test_set]\n",
    "final_labels_set = [keys_s[ind] for ind in labels_set]\n",
    "print(list(prediction[0])[15:25])\n",
    "print(final_labels_set[15:25])\n",
    "\n",
    "# Metrics calculation\n",
    "accuracy = accuracy_score(final_labels_set, prediction[0])\n",
    "precision = precision_score(final_labels_set, prediction[0], average='macro')\n",
    "recall = recall_score(final_labels_set, prediction[0], average='macro')\n",
    "f1 = sklearn.metrics.f1_score(final_labels_set, prediction[0], average='macro')\n",
    "\n",
    "# Save in csv file\n",
    "# description = \"2 layer CNN (32 and 64 output channels) with final 2 Dense Layers (512 and num_classes) result concatenated with \\n 2 LSTMs (hidden_size=32),  from mfcc with 2 Dense Layers (64 and 16) with a final Lazy Linear layer output of num_classes. Dataset using time shift and masking. Using now dataset of 40 audio samples \"\n",
    "description = \"Imported CoAtNet model, with 2 Conv layers and then 2 Attention layers followed by a fully connected layer. Patience=10. Testing recently recorded denoised-normalized audio, part 2 to see if it is working. did 36 epochs. 21 keys recorded\"\n",
    "save_csv(model_name, int(real_num_epochs), description, accuracy, precision, recall, f1)\n",
    "\n",
    "# Print results\n",
    "print(\"Final Results!\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(description)\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n",
    "print(f\"Best val accuracy: {best_val_acc:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3179ef25db1fbef0",
   "metadata": {},
   "source": [
    "# prediction = predict(test_set, model, [\"images\", \"sequences\"],model_name, device)\n",
    "# \n",
    "# for ind, pred in enumerate(list(prediction[0])):\n",
    "#     print(f'prediction {ind}: {pred} / label: {final_labels_set[ind]}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57c064ba206085c2",
   "metadata": {},
   "source": [
    "from coatnet import CoAtNet\n",
    "\n",
    "img = torch.rand(16, 3, 64, 64)\n",
    "\n",
    "num_blocks = [2, 2, 3, 5, 2]            # L\n",
    "channels = [64, 96, 192, 384, 768]      # D\n",
    "block_types=['C', 'C', 'T', 'T']        # 'C' for MBConv, 'T' for Transformer\n",
    "\n",
    "net = CoAtNet((64, 64), 1, num_blocks, channels, block_types=block_types)\n",
    "out = net(torch.unsqueeze(audioDatasetFin[0][0], dim=0))\n",
    "print(f'final shape: {out.shape}')\n",
    "print(audioDatasetFin[0][0].shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cda12c04ba1a39d7",
   "metadata": {},
   "source": [
    "import csv\n",
    "    \n",
    "def empty_file(csv_file_path):\n",
    "    # Read the header (first row) of the CSV file\n",
    "    with open(csv_file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Read the first row (header)\n",
    "    \n",
    "    # Write only the header back to the CSV file\n",
    "    with open(csv_file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)  # Wr`ite the header back to the file\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5aa7e7268b306a",
   "metadata": {},
   "source": [
    "# empty_file('model_comparison.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "58c8e875",
   "metadata": {},
   "source": [
    "# Using custom audio\n",
    "\n",
    "The following code adapts the previous working segment to utilize custom audio recorded by the team. Work in progress."
   ]
  },
  {
   "cell_type": "code",
   "id": "23af0b4c",
   "metadata": {},
   "source": [
    "#Using audio from custom-audio to create the test_set\n",
    "keys_t_s='0123'\n",
    "labels = list(keys_t_s)\n",
    "keys_t = [k + '.wav' for k in labels]\n",
    "\n",
    "for key in keys_t:\n",
    "    sample_t, sr_t = librosa.load(f'../Dataset-custom-audio/base-audio/{key}')\n",
    "    print(sr_t)\n",
    "    print(len(isolator(sample_t, sr_t, 1024, 225, 2400, 12000, 0.06)), end=' ')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4879ba55",
   "metadata": {},
   "source": [
    "n_fft = 50 #1024\n",
    "hop_length = 225 #225\n",
    "before = 2400 #2400\n",
    "after = 10000 #12000\n",
    "\n",
    "data_dict_t= {'Key':[], 'File':[]} #for custom audio testing\n",
    "mbp_dataset_t = create_dataset(n_fft, hop_length, before, after, keys_t, custom_audio=True)\n",
    "mbp_dataset_t"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d31b40b02c559d5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3ffae0790160aac2"
  },
  {
   "cell_type": "code",
   "id": "a529fa2d",
   "metadata": {},
   "source": [
    "audio_samples_t = mbp_dataset_t['File'].values.tolist()\n",
    "labels_t = mbp_dataset_t['Key'].values.tolist()\n",
    "\n",
    "audioDataset_t = np.array(audio_samples_t, dtype = object)\n",
    "print(audio_samples_t[0].shape)\n",
    "mfcc_t = librosa.feature.mfcc(y=audio_samples_t[0], sr=44100) # shape: (n_mfcc, t)\n",
    "print(mfcc_t.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "feddd053",
   "metadata": {},
   "source": [
    "audio_samples_new_t = audio_samples_t.copy() # audio samples CNN\n",
    "\n",
    "for i, sample in enumerate(audio_samples_t):\n",
    "    audio_samples_new_t.append(time_shift(sample))\n",
    "    labels_t.append(labels_t[i])\n",
    "    \n",
    "# convert labels to a numpy array\n",
    "labels_t = np.array(labels_t)\n",
    "print(len(audio_samples_new_t))\n",
    "print(len(labels_t))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a5a0a118",
   "metadata": {},
   "source": [
    "audioDatasetFin_t, audioDatasetMfcc_t = [], []\n",
    "\n",
    "for i in range(len(audio_samples_new_t)):\n",
    "    transformed_sample_t = transform(audio_samples_new_t[i])\n",
    "    transformed_mfcc_t = transform_mfcc(audio_samples_new_t[i])\n",
    "    audioDatasetFin_t.append((transformed_sample_t, labels_t[i]))\n",
    "    audioDatasetMfcc_t.append((transformed_sample_t, transformed_mfcc_t, labels_t[i]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "189bbf34",
   "metadata": {},
   "source": [
    "#Using custom audio:\n",
    "# current random state to split the dataset\n",
    "random_state = 42\n",
    "\n",
    "# values for current run\n",
    "train_final_set, test_set = train_test_split(audioDatasetMfcc_t, test_size=0.2, random_state=random_state)\n",
    "num_epochs = 100\n",
    "main_architecture = \"CNN_LSTM\"\n",
    "currday = datetime.today().strftime('%Y-%m-%d')\n",
    "model_name = f\"model_multiclass_custom_audio_{num_epochs}_{main_architecture}_{currday}.pth\"\n",
    "description = \"2 layer CNN (32 and 64 output channels) with final 2 Dense Layers (512 and num_classes) result concatenated with \\n 2 LSTMs (hidden_size=32),  from mfcc with 2 Dense Layers (64 and 16) with a final Lazy Linear layer output of num_classes. \\n Using custom audio recorded for testing purposes. n_fft = 50\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "45734399",
   "metadata": {},
   "source": [
    "# Training part\n",
    "fold_stats = train_with_cross_validation(train_final_set, num_epochs, model_name, random_state=random_state)\n",
    "max_val = 0\n",
    "real_num_epochs = 0\n",
    "for fold_stat in fold_stats: #using folds instead of LOO\n",
    "    if fold_stat[1] > max_val:\n",
    "        max_val = fold_stat[1]\n",
    "        real_num_epochs = fold_stat[0]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "859fa190",
   "metadata": {},
   "source": [
    "# Prediction part\n",
    "prediction = predict_mfcc(test_set, model_name, device) #using the custom test_set\n",
    "labels_set = [t[2] for t in test_set]\n",
    "print(labels_set)\n",
    "print(prediction[0])\n",
    "final_labels_set = [keys_t_s[ind] for ind in labels_set]\n",
    "\n",
    "# Metrics calculation\n",
    "accuracy = accuracy_score(final_labels_set, prediction[0])\n",
    "precision = precision_score(final_labels_set, prediction[0], average='macro')\n",
    "recall = recall_score(final_labels_set, prediction[0], average='macro')\n",
    "f1 = sklearn.metrics.f1_score(final_labels_set, prediction[0], average='macro')\n",
    "\n",
    "# Save in csv file\n",
    "save_csv(model_name, real_num_epochs, description, accuracy, precision, recall, f1)\n",
    "\n",
    "# Print results\n",
    "print(\"Final Results!\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(description)\n",
    "print(f\"Epochs: {real_num_epochs}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
