{
 "cells": [
  {
   "cell_type": "code",
   "id": "2a122adee9a39c20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:39:48.776053Z",
     "start_time": "2025-02-05T15:39:46.643168Z"
    }
   },
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pydub import AudioSegment, silence\n",
    "import pickle\n",
    "import pyloudnorm as pyln\n",
    "from sklearn.metrics import make_scorer\n",
    "import soundfile as sf"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "9ad2373f64ed7deb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:39:51.740919Z",
     "start_time": "2025-02-05T15:39:51.735709Z"
    }
   },
   "source": [
    "# waveform function for me to not bang my keyboard\n",
    "def disp_waveform(signal,title='', sr=None, color='blue'):\n",
    "    plt.figure(figsize=(7,2))\n",
    "    plt.title(title)\n",
    "    librosa.display.waveshow(signal, sr=sr, color=color)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:39:52.264351Z",
     "start_time": "2025-02-05T15:39:51.888670Z"
    }
   },
   "source": [
    "import noisereduce as nr\n",
    "\n",
    "def signum(x):\n",
    "    return 1 if x>0 else -1\n",
    "\n",
    "def isolator(signal, sample_rate, n_fft, hop_length, before, after, threshold, show=False):\n",
    "    strokes = []\n",
    "    # -- signal'\n",
    "    denoised_signal = nr.reduce_noise(signal, sr=sample_rate)\n",
    "    # if show:\n",
    "    #     disp_waveform(denoised_signal, 'signal waveform DENOISED', sr=sample_rate)\n",
    "    #     disp_waveform(signal, 'signal waveform NOISED', sr=sample_rate)\n",
    "    #     disp_waveform(denoised_signal_boosted, 'signal waveform DENOISED n BOOSTED', sr=sample_rate)\n",
    "    # signal = denoised_signal\n",
    "    fft = librosa.stft(signal, n_fft=n_fft, hop_length=hop_length)\n",
    "    energy = np.abs(np.sum(fft, axis=0)).astype(float)\n",
    "    norm = np.linalg.norm(energy)\n",
    "    energy = energy/norm\n",
    "    # -- energy'\n",
    "    threshed = energy > threshold\n",
    "    # -- peaks'\n",
    "    if show:\n",
    "        # disp_waveform(threshed.astype(float), sr=sample_rate)\n",
    "        pass\n",
    "    peaks = np.where(threshed == True)[0]\n",
    "    peak_count = len(peaks)\n",
    "    prev_end = sample_rate*0.1*(-1)\n",
    "    # '-- isolating keystrokes'\n",
    "    timestamps = []\n",
    "    for i in range(peak_count):\n",
    "        this_peak = peaks[i]\n",
    "        timestamp = (this_peak*hop_length) + n_fft//2\n",
    "        if timestamp > prev_end + (0.1*sample_rate):\n",
    "            keystroke = signal[timestamp-before:timestamp+after]\n",
    "            # strokes.append(torch.tensor(keystroke)[None, :])\n",
    "            # keystroke = transform(keystroke)\n",
    "            if len(keystroke) >= before + after:\n",
    "                strokes.append(keystroke)\n",
    "                timestamps.append(timestamp)\n",
    "                if show and len(strokes) == 1:\n",
    "                    disp_waveform(keystroke, title=f'keystroke {len(strokes)}', sr=sample_rate)\n",
    "                prev_end = timestamp+after\n",
    "    return peaks, strokes, timestamps"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "23314fd12f6ff70f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:39:52.266558Z",
     "start_time": "2025-02-05T15:39:52.265163Z"
    }
   },
   "source": [
    "def partition_audio(samples_arr : np.array):\n",
    "    ret_samples = np.abs(samples_arr)\n",
    "    return ret_samples"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "925e1183ef02fb55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:39:52.271226Z",
     "start_time": "2025-02-05T15:39:52.267085Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "def find_key_presses(waveform, res, waveform_threshold, waveform_max, threshold_background, history_size, remove_low_power):\n",
    "    # Clear previous results\n",
    "    # res.clear()\n",
    "    # waveform_threshold = np.zeros_like(waveform)\n",
    "    # waveform_max = np.zeros_like(waveform)\n",
    "    # \n",
    "    rb_begin = 0\n",
    "    rb_average = 0.0\n",
    "    rb_samples = np.zeros(history_size)\n",
    "\n",
    "    k = history_size\n",
    "    que = deque(maxlen=k)\n",
    "\n",
    "    samples = np.abs(waveform)  # Taking absolute values like waveformAbs in C++\n",
    "    n = len(samples)\n",
    "    overall_loudness = 0\n",
    "    len_ovr_loudness = 0\n",
    "    for i in range(n):\n",
    "        ii = i - k // 2\n",
    "        if ii >= 0:\n",
    "            rb_average *= len(rb_samples)\n",
    "            rb_average -= rb_samples[rb_begin]\n",
    "            acur = samples[i]\n",
    "            rb_samples[rb_begin] = acur\n",
    "            rb_average += acur\n",
    "            rb_average /= len(rb_samples)\n",
    "            rb_begin = (rb_begin + 1) % len(rb_samples)\n",
    "        if i < k:\n",
    "            # Handling initial filling of the deque\n",
    "            while que and samples[i] >= samples[que[-1]]:\n",
    "                que.pop()\n",
    "            que.append(i)\n",
    "        else:\n",
    "            # Maintain the deque as a max-queue for the sliding window\n",
    "            while que and que[0] <= i - k:\n",
    "                que.popleft()\n",
    "\n",
    "            # same code as if i<k\n",
    "            while que and samples[i] >= samples[que[-1]]:\n",
    "                que.pop()\n",
    "            que.append(i)\n",
    "\n",
    "            itest = i - k // 2\n",
    "            if  k <= itest < n - k and que[0] == itest:\n",
    "                acur = samples[itest]\n",
    "                if acur > threshold_background * rb_average:\n",
    "                    res.append({\n",
    "                        'waveform': waveform[itest - k//6 : itest + (5*k)//6],\n",
    "                        'index': itest\n",
    "                    })\n",
    "                    quiet_part = samples[itest + (3*k)//6 : itest + (5*k)//6]\n",
    "                    len_ovr_loudness += len(quiet_part)\n",
    "                    overall_loudness += np.sum(quiet_part)\n",
    "            waveform_threshold[itest] = threshold_background * rb_average\n",
    "            waveform_max[itest] = samples[que[0]]\n",
    "\n",
    "    if remove_low_power:\n",
    "        while True:\n",
    "            old_n = len(res)\n",
    "\n",
    "            avg_power = sum(samples[kp[\"position\"]] for kp in res) / len(res)\n",
    "\n",
    "            tmp_res = res[:]\n",
    "            res.clear()\n",
    "\n",
    "            for kp in tmp_res:\n",
    "                if samples[kp[\"position\"]] > 0.3 * avg_power:\n",
    "                    res.append(kp)\n",
    "\n",
    "            if len(res) == old_n:\n",
    "                break\n",
    "                \n",
    "    if len_ovr_loudness > 0:\n",
    "        avg_loudness = overall_loudness / len_ovr_loudness\n",
    "    else:\n",
    "        avg_loudness = 0\n",
    "        \n",
    "    return {'waveform_threshold': waveform_threshold, \n",
    "            'waveform_max': waveform_max,\n",
    "            'res': res,\n",
    "            'avg_loudness': avg_loudness\n",
    "            }"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "b24a7c9ee1772887",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:39:52.443984Z",
     "start_time": "2025-02-05T15:39:52.441782Z"
    }
   },
   "source": [
    "import array\n",
    "\n",
    "def numpy_to_audiosegment(samples, sample_rate=44100, sample_width=2, channels=1):\n",
    "    # Ensure the numpy array is in the correct dtype (int16 or int32 based on sample_width)\n",
    "    if sample_width == 2:\n",
    "        samples = np.int16(samples)\n",
    "    elif sample_width == 4:\n",
    "        samples = np.int32(samples)\n",
    "    \n",
    "    # Convert numpy array to byte data\n",
    "    audio_data = array.array('h', samples)  # 'h' for 16-bit PCM audio\n",
    "    byte_data = audio_data.tobytes()\n",
    "    \n",
    "    # Create AudioSegment\n",
    "    audio_segment = AudioSegment(\n",
    "        data=byte_data,\n",
    "        sample_width=sample_width,  # 2 for 16-bit, 4 for 32-bit\n",
    "        frame_rate=sample_rate,\n",
    "        channels=channels\n",
    "    )\n",
    "    \n",
    "    return audio_segment"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "6cfc880f1fa8f48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:30:27.148553Z",
     "start_time": "2025-02-05T15:30:25.217334Z"
    }
   },
   "source": [
    "# constants\n",
    "N_FFT, HOP_LENGTH, BEFORE, AFTER = 1024, 225, 2400, 12000\n",
    "\n",
    "key_length = 8820\n",
    "for num in range(0, 5):\n",
    "    samples, sr = librosa.load(f'../MKA datasets/All Dataset/Raw Data/{num}/{num}mac.wav', sr=44100)\n",
    "    pydub_samples = AudioSegment.from_file(f'../MKA datasets/All Dataset/Raw Data/{num}/{num}mac.wav', format=\"wav\", frame_rate=44100)\n",
    "    \n",
    "    samples2, sr = librosa.load(f'../Dataset-for-Binary/base-audio/audio_{num}.wav', sr=44100)\n",
    "    pydub_samples2 = AudioSegment.from_file(f'../Dataset-for-Binary/base-audio/audio_{num}.wav', format='wav', frame_rate=44100, sample_width=4)\n",
    "    \n",
    "    silences = silence.detect_silence(pydub_samples, silence_thresh=1.01*pydub_samples.dBFS, min_silence_len=70)\n",
    "    ovr_dbms = []\n",
    "    for start_ind, final_ind in silences:\n",
    "        ovr_dbms.append(pydub_samples[start_ind:final_ind].dBFS)\n",
    "    avg_dbfs = np.average(ovr_dbms)\n",
    "    \n",
    "    # the number 2\n",
    "    silences2 = silence.detect_silence(pydub_samples2, silence_thresh=1.01*pydub_samples2.dBFS, min_silence_len=70)\n",
    "    ovr_dbms2 = []\n",
    "    for start_ind, final_ind in silences2:\n",
    "        ovr_dbms2.append(pydub_samples2[start_ind:final_ind].dBFS)\n",
    "    avg_dbfs2 = np.average(ovr_dbms2)\n",
    "    \n",
    "    threshold_vals = np.arange(0.13, 0.15, 0.002)\n",
    "    print(f'KEY {num}')\n",
    "    for i in threshold_vals:\n",
    "        print(f'i={i:.3f}')\n",
    "        print(\"AUDIO MKA\")\n",
    "        print(f'avg dBFS: {avg_dbfs:.3f}')\n",
    "        return_dic = find_key_presses(samples,[],{},{},np.abs(i*avg_dbfs), key_length, False)\n",
    "        print(f'\\tbackground_prof: {abs(i*avg_dbfs):.3f} / Number of keys: {len(return_dic[\"res\"])}')\n",
    "        print(\"AUDIO ORIGINAL\")\n",
    "        print(f'avg dBFS: {avg_dbfs2:.3f}')\n",
    "        return_dic2 = find_key_presses(samples2,[],{},{},np.abs(i*avg_dbfs2), key_length, False)\n",
    "        print(f'\\tbackground_prof: {abs(i*avg_dbfs2):.3f} / Number of keys: {len(return_dic2[\"res\"])}')\n",
    "        print()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY 0\n",
      "i=0.130\n",
      "AUDIO MKA\n",
      "avg dBFS: -47.495\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 31\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAUDIO MKA\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mavg dBFS: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_dbfs\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 31\u001B[0m return_dic \u001B[38;5;241m=\u001B[39m find_key_presses(samples,[],{},{},np\u001B[38;5;241m.\u001B[39mabs(i\u001B[38;5;241m*\u001B[39mavg_dbfs), key_length, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mbackground_prof: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mabs\u001B[39m(i\u001B[38;5;241m*\u001B[39mavg_dbfs)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m / Number of keys: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(return_dic[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mres\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAUDIO ORIGINAL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[5], line 44\u001B[0m, in \u001B[0;36mfind_key_presses\u001B[0;34m(waveform, res, waveform_threshold, waveform_max, threshold_background, history_size, remove_low_power)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m que \u001B[38;5;129;01mand\u001B[39;00m samples[i] \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m samples[que[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]]:\n\u001B[1;32m     43\u001B[0m     que\u001B[38;5;241m.\u001B[39mpop()\n\u001B[0;32m---> 44\u001B[0m que\u001B[38;5;241m.\u001B[39mappend(i)\n\u001B[1;32m     46\u001B[0m itest \u001B[38;5;241m=\u001B[39m i \u001B[38;5;241m-\u001B[39m k \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m  k \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m itest \u001B[38;5;241m<\u001B[39m n \u001B[38;5;241m-\u001B[39m k \u001B[38;5;129;01mand\u001B[39;00m que[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m itest:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "c3cd39ac1fe20468",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:39:55.644254Z",
     "start_time": "2025-02-05T15:39:55.631489Z"
    }
   },
   "source": [
    "from detecta import detect_peaks\n",
    "import scipy.signal as signal\n",
    "\n",
    "def count_peaks(samples, key_length=14400, show=True, percentile=96.6):\n",
    "    # meter = pyln.Meter(44100)  # Create BS.1770 meter\n",
    "    # loudness = meter.integrated_loudness(samples)\n",
    "    # target_loudness = -25.0\n",
    "    # samples = pyln.normalize.loudness(samples, loudness, target_loudness)\n",
    "    threshold = np.percentile(samples, percentile)\n",
    "    # final_samples = pyln.normalize.peak(samples, 0.75)\n",
    "    indexes = detect_peaks(samples[int(key_length/3): -int(key_length/3)], show=show, mpd=key_length - key_length/3, mph=threshold)\n",
    "    return len(indexes)\n",
    "\n",
    "def isolator_new(file_path, sr, key_length=14400, k=0.15):\n",
    "    pydub_samples = AudioSegment.from_file(file_path, format=\"wav\", frame_rate=sr)\n",
    "    silences = silence.detect_silence(pydub_samples, silence_thresh=1.01*pydub_samples.dBFS, min_silence_len=50)\n",
    "    ovr_dbms = []\n",
    "    for start_ind, final_ind in silences:\n",
    "        ovr_dbms.append(pydub_samples[start_ind:final_ind].dBFS)\n",
    "    avg_dbfs = np.average(ovr_dbms)\n",
    "    samples, sr = librosa.load(file_path, sr=44100)\n",
    "    samples = nr.reduce_noise(samples, sr=44100)\n",
    "    return_dic = find_key_presses(samples,[],{},{},np.abs(k*avg_dbfs), key_length, False)\n",
    "    return return_dic"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:39:55.819652Z",
     "start_time": "2025-02-05T15:39:55.810765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import resampy\n",
    "\n",
    "def load_like_librosa(path, sr=22050):\n",
    "    # Read with soundfile\n",
    "    y, orig_sr = sf.read(path)\n",
    "    \n",
    "    if len(y.shape) > 1:\n",
    "        y = 0.5 * (y[:, 0] + y[:, 1])\n",
    "        \n",
    "    if orig_sr != sr:\n",
    "        y = resampy.resample(y, orig_sr, sr)\n",
    "        \n",
    "    y = y.astype(np.float32)\n",
    "    if y.max() > 1.0 or y.min() < -1.0:\n",
    "        y = y / np.max(np.abs(y))\n",
    "        \n",
    "    return y, sr"
   ],
   "id": "874ed6258f626edc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:39:56.049562Z",
     "start_time": "2025-02-05T15:39:56.045040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataset_viejo(n_fft, hop_length, before, after, keys, audio_dir, curr_labels, prom=0.2391, original=True, key_length=14400, reduce_noise=False, percentile=96.6, normalize_loudness=False):\n",
    "    data_dict = {'Key':[], 'File':[]}\n",
    "    base_step = 0.01\n",
    "    for i, File in enumerate(keys):\n",
    "        curr_step = base_step\n",
    "        loc = audio_dir + File\n",
    "\n",
    "        # samples, sr = librosa.load(loc)\n",
    "        # samples, sr = sf.read(loc)\n",
    "        samples, sr = load_like_librosa(loc, 44100)\n",
    "        if reduce_noise or keys[i] == \"audio_-.wav\":\n",
    "            samples = nr.reduce_noise(samples, sr=44100)\n",
    "        show = (File[6 if original else 0] == '0')\n",
    "        peaks_count = count_peaks(samples, key_length, keys[i] == \"audio_-.wav\", percentile)\n",
    "        strokes = isolator(samples, sr, n_fft, hop_length, before, after, prom, show)[1]\n",
    "        \n",
    "        # Add noise to the samples\n",
    "        \n",
    "        num_keys = len(strokes)\n",
    "        count = 0\n",
    "        k = prom\n",
    "        prev_k = prom\n",
    "        print(f'num_keys: {num_keys} // peaks_count: {peaks_count} // prom: {prom}')\n",
    "        \n",
    "        while num_keys != peaks_count:\n",
    "            if num_keys > peaks_count:\n",
    "                if count > 0 and prev_k == k + curr_step:\n",
    "                    curr_step /= 2\n",
    "                elif count > 0:\n",
    "                    curr_step += (curr_step / 2)\n",
    "                prev_k = k\n",
    "                k += curr_step\n",
    "            else:\n",
    "                if count > 0 and prev_k == k - curr_step:\n",
    "                    curr_step /= 2\n",
    "                elif count > 0:\n",
    "                    curr_step += (curr_step / 2)\n",
    "                prev_k = k\n",
    "                k += -curr_step\n",
    "            strokes = isolator(samples, sr, n_fft, hop_length, before, after, k, show)[1]\n",
    "            num_keys = len(strokes)\n",
    "            count += 1\n",
    "            # print(f'{num_keys} // {peaks_count}')\n",
    "        \n",
    "        isolator(samples, sr, n_fft, hop_length, before, after, k, False)\n",
    "        print(f'{File}. Len strokes: {len(strokes)}')\n",
    "        if show:\n",
    "            print(f'Length strokes: {len(strokes)}')\n",
    "        label = [curr_labels[i]]*len(strokes)\n",
    "        data_dict['Key'] += label\n",
    "        data_dict['File'] += strokes\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for l in df['Key']:\n",
    "        if not l in mapper:\n",
    "            mapper[l] = counter\n",
    "            counter += 1\n",
    "    df.replace({'Key': mapper}, inplace = True)\n",
    "\n",
    "    return df"
   ],
   "id": "fddfae8f4d5b9034",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "23a14d8cd7e215a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:39:56.092359Z",
     "start_time": "2025-02-05T15:39:56.087030Z"
    }
   },
   "source": [
    "def create_dataset(keys, audio_dir, initial_k, key_length=8820):\n",
    "    data_dict = {'Key':[], 'File':[]}\n",
    "    base_step = 0.01\n",
    "    # file_path_function = lambda currkey, keyb: f'../MKA datasets/All Dataset/Raw Data/{currkey}/{currkey}{keyb}.wav'\n",
    "    for keyb in ['mac']:\n",
    "        for i, key in enumerate(keys):\n",
    "            curr_step = base_step\n",
    "            # if key.isalpha and not key.isalnum(): # if is a string\n",
    "            #     curr_key = key.lower()\n",
    "            # file_path = f'../MKA datasets/All Dataset/Raw Data/{curr_key}/{curr_key}{keyb}.wav'\n",
    "            file_path = audio_dir + key\n",
    "            # file_path = f'../Dataset-for-Binary/base-audio/audio_{curr_key}.wav'\n",
    "            samples, sr = librosa.load(file_path, sr=44100)\n",
    "            samples = nr.reduce_noise(samples, sr=44100)\n",
    "        \n",
    "            show_val = True\n",
    "            peaks_count = count_peaks(samples, key_length, show_val)\n",
    "            \n",
    "            # isolator\n",
    "            k = initial_k \n",
    "            curr_array = isolator_new(file_path, sr, key_length, k)['res']\n",
    "            strokes = [curr['waveform'] for curr in curr_array]\n",
    "            num_keys = len(strokes)\n",
    "            prev_k = k \n",
    "            count = 0\n",
    "            while num_keys != peaks_count:\n",
    "                if num_keys > peaks_count:\n",
    "                    if count > 0 and prev_k == k + curr_step:\n",
    "                        curr_step /= 2\n",
    "                    elif count > 0:\n",
    "                        curr_step += (curr_step / 2)\n",
    "                    prev_k = k\n",
    "                    k += curr_step\n",
    "                else:\n",
    "                    if count > 0 and prev_k == k - curr_step:\n",
    "                        curr_step /= 2\n",
    "                    elif count > 0:\n",
    "                        curr_step += (curr_step / 2)\n",
    "                    prev_k = k\n",
    "                    k += -curr_step\n",
    "                curr_arr = isolator_new(file_path, sr, key_length, k)['res']\n",
    "                strokes = [arr['waveform'] for arr in curr_arr]\n",
    "                num_keys = len(strokes)\n",
    "                count += 1\n",
    "                # print(f'k={k:.4f}\\tnum_keys={num_keys}')\n",
    "            print(f'key {key} {keyb} final k={k:.4f}\\tnum_keys={num_keys}\\tpeaks={peaks_count}')\n",
    "            print()\n",
    "            \n",
    "            # now get the actual keys file\n",
    "            label = [keys[i]]*num_keys\n",
    "            data_dict['Key'] += label\n",
    "            data_dict['File'] += strokes\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for l in df['Key']:\n",
    "        if not l in mapper:\n",
    "            mapper[l] = counter\n",
    "            counter += 1\n",
    "    df.replace({'Key': mapper}, inplace = True)\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:39:56.217462Z",
     "start_time": "2025-02-05T15:39:56.215724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ],
   "id": "fb1e97647d26f096",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "c165936fc0ca1752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:41:11.139668Z",
     "start_time": "2025-02-05T15:39:56.371023Z"
    }
   },
   "source": [
    "# currently used keys\n",
    "curr_keys = list('1234567890QWERTYUIOPASDFGHJKLZXCVBNM-')\n",
    "# curr_keys.append(\"space\")\n",
    "N_FFT, HOP_LENGTH, BEFORE, AFTER = 1024, 225, 2400, 12000\n",
    "MBP_AUDIO_DIR, labels, audiostr = ('/Users/jorgeleon/Binary-class/Dataset-for-Binary/base-audio/', list('1234567890QWERTYUIOPASDFGHJKLZXCVBNM-'), 'audio_')\n",
    "# MBP_AUDIO_DIR, audiostr = '/Users/jorgeleon/Binary-class/MKA datasets/Mac/Raw data/', ''\n",
    "keys = [audiostr + k + '.wav' for k in labels]\n",
    "# key_length = 9200\n",
    "key_length=14400\n",
    "BEFORE = int(key_length / 6)\n",
    "AFTER = int(5 * (key_length / 6))\n",
    "# print(BEFORE, AFTER)\n",
    "# Create the final dataset\n",
    "mbp_dataset = create_dataset_viejo(N_FFT, HOP_LENGTH, BEFORE, AFTER, keys, MBP_AUDIO_DIR, labels, prom=0.2391,original=False, key_length=key_length)\n",
    "# original_dataset = create_dataset(curr_keys, initial_k=0.5, key_length=14400)\n",
    "# MBP_AUDIO_DIR = \"/Users/jorgeleon/Binary-class/Dataset-custom-audio/base-audio-denoised-normalized/\"\n",
    "# labels = list('-')\n",
    "# audiostr = ''\n",
    "# keys = [audiostr + k + '.wav' for k in labels]\n",
    "# mbp_dataset_extended = create_dataset_viejo(N_FFT, HOP_LENGTH, BEFORE, AFTER, keys, MBP_AUDIO_DIR, labels, prom=0.2391,original=False, key_length=key_length, reduce_noise=True, percentile=99.7)\n",
    "# mbp_dataset_extended['Key'] = mbp_dataset_extended['Key'] + 36\n",
    "# final_df = pd.concat([mbp_dataset, mbp_dataset_extended], axis=0)\n",
    "# final_df['Key'] = final_df['Key'] + 36\n",
    "# mbp_dataset = create_dataset(keys,MBP_AUDIO_DIR, 0.15, 14400)\n",
    "# print(original_dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_1.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_2.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_3.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_4.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_5.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_6.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_7.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_8.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_9.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_0.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_Q.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_W.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_E.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_R.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_T.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_Y.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_U.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_I.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_O.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_P.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_A.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_S.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_D.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_F.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_G.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_H.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 26 // prom: 0.2391\n",
      "audio_J.wav. Len strokes: 26\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_K.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_L.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_Z.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_X.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_C.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_V.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_B.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_N.wav. Len strokes: 25\n",
      "num_keys: 0 // peaks_count: 25 // prom: 0.2391\n",
      "audio_M.wav. Len strokes: 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_keys: 0 // peaks_count: 29 // prom: 0.2391\n",
      "audio_-.wav. Len strokes: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/8bpx1vc91zq76n48xvq00vkr0000gn/T/ipykernel_90352/1522711625.py:60: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace({'Key': mapper}, inplace = True)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:41:11.475590Z",
     "start_time": "2025-02-05T15:41:11.140739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loc = \"/Users/jorgeleon/Binary-class/Dataset-for-Binary/base-audio/audio_-.wav\"\n",
    "samples, sr = load_like_librosa(loc, 44100)\n",
    "samples = nr.reduce_noise(samples, sr=44100)\n",
    "disp_waveform(samples, 'Space -', sr=44100)"
   ],
   "id": "99db9f04fe7220c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 700x200 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:41:11.507952Z",
     "start_time": "2025-02-05T15:41:11.476188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read curr_dataset back from the file\n",
    "# Write curr_dataset to a file to avoid running this shit over & over again\n",
    "with open('mbp_dataset_latest_w_space.pkl', 'wb') as f:\n",
    "    pickle.dump(mbp_dataset, f)\n",
    "\n",
    "# with open('mbp_dataset_31-10-24.pkl', 'rb') as f:\n",
    "#     mbp_dataset = pickle.load(f)"
   ],
   "id": "d52aca795d8f78b5",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:41:11.519052Z",
     "start_time": "2025-02-05T15:41:11.509986Z"
    }
   },
   "cell_type": "code",
   "source": "mbp_dataset",
   "id": "dfef6976f967c80c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Key                                               File\n",
       "0      0  [-0.0002537508, -0.00021889925, -0.00017753136...\n",
       "1      0  [-0.0003979571, -0.00040632932, -0.00041234834...\n",
       "2      0  [-0.00047642284, -0.00048464013, -0.0005088254...\n",
       "3      0  [-0.00047592734, -0.00049532985, -0.0005491621...\n",
       "4      0  [0.0006792293, 0.000631534, 0.00060479593, 0.0...\n",
       "..   ...                                                ...\n",
       "925   36  [-4.8833637e-12, -4.68339e-12, -4.5051124e-12,...\n",
       "926   36  [1.6506524e-11, 1.7184476e-11, 1.7678366e-11, ...\n",
       "927   36  [-4.5952703e-10, -4.6088117e-10, -4.6215332e-1...\n",
       "928   36  [-1.5309504e-10, -1.5897848e-10, -1.6422666e-1...\n",
       "929   36  [-3.2444605e-10, -3.0345476e-10, -2.7969319e-1...\n",
       "\n",
       "[930 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0002537508, -0.00021889925, -0.00017753136...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0003979571, -0.00040632932, -0.00041234834...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.00047642284, -0.00048464013, -0.0005088254...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.00047592734, -0.00049532985, -0.0005491621...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.0006792293, 0.000631534, 0.00060479593, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>36</td>\n",
       "      <td>[-4.8833637e-12, -4.68339e-12, -4.5051124e-12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>36</td>\n",
       "      <td>[1.6506524e-11, 1.7184476e-11, 1.7678366e-11, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>36</td>\n",
       "      <td>[-4.5952703e-10, -4.6088117e-10, -4.6215332e-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>36</td>\n",
       "      <td>[-1.5309504e-10, -1.5897848e-10, -1.6422666e-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>36</td>\n",
       "      <td>[-3.2444605e-10, -3.0345476e-10, -2.7969319e-1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>930 rows  2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "8a7918b824b24998",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:41:11.546754Z",
     "start_time": "2025-02-05T15:41:11.519925Z"
    }
   },
   "source": "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:41:11.550996Z",
     "start_time": "2025-02-05T15:41:11.547820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataset(n_fft, hop_length, before, after, keys, audio_dir, curr_labels, prom=0.2391, original=True):\n",
    "    data_dict = {'Key':[], 'File':[]}\n",
    "    for i, File in enumerate(keys):\n",
    "        loc = audio_dir + File\n",
    "        samples, sr = librosa.load(loc)\n",
    "        show = (File[6 if original else 0] == '0')\n",
    "        strokes = isolator(samples, sr, n_fft, hop_length, before, after, prom, show)\n",
    "        if show:\n",
    "            print(f'Length strokes: {len(strokes)}')\n",
    "        label = [curr_labels[i]]*len(strokes)\n",
    "        data_dict['Key'] += label\n",
    "        data_dict['File'] += strokes\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for l in df['Key']:\n",
    "        if not l in mapper:\n",
    "            mapper[l] = counter\n",
    "            counter += 1\n",
    "    df.replace({'Key': mapper}, inplace = True)\n",
    "\n",
    "    return df"
   ],
   "id": "2fee322473e2bfe1",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "2b8b07040da6027e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:41:11.563431Z",
     "start_time": "2025-02-05T15:41:11.552033Z"
    }
   },
   "source": [
    "import audiosegment\n",
    "\n",
    "def get_audio_length(audio_path):\n",
    "    audio = audiosegment.from_file(audio_path)\n",
    "    return audio.duration_seconds\n",
    "\n",
    "def convert_to_ms(t):\n",
    "    return round(t*1000)\n",
    "\n",
    "def get_audio_length_average(audio_path, keys):\n",
    "    lengths = []\n",
    "    for i, File in enumerate(keys):\n",
    "        loc = audio_path + File\n",
    "        length = get_audio_length(loc)\n",
    "        print(f'File {loc} length: {length:2f}\\n')\n",
    "        lengths.append(length)\n",
    "    average = np.mean(lengths)\n",
    "    return convert_to_ms(average)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "ac3598b8c7bcd285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:41:11.569510Z",
     "start_time": "2025-02-05T15:41:11.564484Z"
    }
   },
   "source": [
    "\n",
    "def time_shift(samples):\n",
    "    samples = samples.flatten()\n",
    "    shift = int(len(samples) * 0.4) #Max shift (0.4)\n",
    "    random_shift = random.randint(10, shift) #Random number between 0 and 0.4*len(samples)\n",
    "    data_roll = np.roll(samples, random_shift)\n",
    "    return data_roll\n",
    "\n",
    "def masking(samples):\n",
    "    num_mask = 2\n",
    "    freq_masking_max_percentage=0.10\n",
    "    time_masking_max_percentage=0.10\n",
    "    spec = samples\n",
    "    mean_value = spec.mean()\n",
    "    for i in range(num_mask):\n",
    "        all_frames_num, all_freqs_num = spec.shape[1], spec.shape[1] \n",
    "        freq_percentage = random.uniform(0.0, freq_masking_max_percentage)\n",
    "\n",
    "        num_freqs_to_mask = int(freq_percentage * all_freqs_num)\n",
    "        f0 = np.random.uniform(low=0.0, high=all_freqs_num - num_freqs_to_mask)\n",
    "        f0 = int(f0)\n",
    "        spec[:, f0:f0 + num_freqs_to_mask] = mean_value\n",
    "\n",
    "        time_percentage = random.uniform(0.0, time_masking_max_percentage)\n",
    "\n",
    "        num_frames_to_mask = int(time_percentage * all_frames_num)\n",
    "        t0 = np.random.uniform(low=0.0, high=all_frames_num - num_frames_to_mask)\n",
    "        t0 = int(t0)\n",
    "        spec[t0:t0 + num_frames_to_mask, :] = mean_value\n",
    "    return spec"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "4a429af86b3206df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:41:11.608025Z",
     "start_time": "2025-02-05T15:41:11.570665Z"
    }
   },
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "class ToMelSpectrogram:\n",
    "    def __init__(self, audio_length=14400):\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        if len(samples) > self.audio_length:\n",
    "            samples = samples[:self.audio_length]\n",
    "        elif len(samples) < self.audio_length:\n",
    "            samples = np.pad(samples, (0, self.audio_length - len(samples)), mode='constant')\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=samples, sr=44100, n_mels=64, n_fft=1024, hop_length=225)\n",
    "        mel_spec_resized = resize(mel_spec, (64, 64), anti_aliasing=True)\n",
    "        mel_spec_resized = np.expand_dims(mel_spec_resized, axis=0)\n",
    "        return torch.tensor(mel_spec_resized)\n",
    "\n",
    "\n",
    "class ToMelSpectrogramMfcc:\n",
    "    def __init__(self, audio_length=14400):\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        if len(samples) > self.audio_length:\n",
    "            samples = samples[:self.audio_length]\n",
    "        elif len(samples) < self.audio_length:\n",
    "            samples = np.pad(samples, (0, self.audio_length - len(samples)), mode='constant')\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=samples, sr=44100, n_mels=64, n_fft=n_fft, hop_length=hop_length)\n",
    "        mel_spec = librosa.feature.mfcc(S=librosa.power_to_db(mel_spec))\n",
    "        mel_spec_resized = resize(mel_spec, (64, 64), anti_aliasing=True)\n",
    "        # mel_spec_resized = np.expand_dims(mel_spec_resized, axis=0)\n",
    "\n",
    "        return torch.tensor(mel_spec_resized)\n",
    "\n",
    "\n",
    "class ToMfcc:\n",
    "    def __init__(self, audio_length=14400):\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        if len(samples) > self.audio_length:\n",
    "            samples = samples[:self.audio_length]\n",
    "        elif len(samples) < self.audio_length:\n",
    "            samples = np.pad(samples, (0, self.audio_length - len(samples)), mode='constant')\n",
    "        \n",
    "        mfcc_spec = librosa.feature.mfcc(y=samples, sr=44100)\n",
    "        mfcc_spec = np.transpose(mfcc_spec)\n",
    "        \n",
    "        return torch.tensor(mfcc_spec)\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "f019fc2bc0e25204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:41:11.613717Z",
     "start_time": "2025-02-05T15:41:11.611196Z"
    }
   },
   "source": [
    "transform = Compose([ToMelSpectrogram(key_length)])\n",
    "transform_mfcc = Compose([ToMfcc(key_length)])"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "44b3b3ca9f37f4c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:41:11.632661Z",
     "start_time": "2025-02-05T15:41:11.614991Z"
    }
   },
   "source": [
    "audio_samples = mbp_dataset['File'].values.tolist()\n",
    "labels = mbp_dataset['Key'].values.tolist()\n",
    "\n",
    "audio_samples_no_masking = audio_samples.copy()\n",
    "labels_no_masking = labels.copy()\n",
    "audio_samples_new = audio_samples.copy() # audio samples CNN\n",
    "print(len(audio_samples))\n",
    "\n",
    "print(type(audio_samples[0]))\n",
    "\n",
    "for i, sample in enumerate(audio_samples):\n",
    "    audio_samples_new.append(time_shift(sample))\n",
    "    labels.append(labels[i])\n",
    "\n",
    "# convert labels to a numpy array\n",
    "labels = np.array(labels)\n",
    "print(len(audio_samples_new))\n",
    "print(len(labels))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930\n",
      "<class 'numpy.ndarray'>\n",
      "1860\n",
      "1860\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "4a0f8d9f11e77431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:42:00.938417Z",
     "start_time": "2025-02-05T15:41:11.633856Z"
    }
   },
   "source": [
    "audioDatasetFin, audioDatasetFinMasking, audioDatasetMfcc, audioDatasetMfccMasking = [], [], [], []\n",
    "\n",
    "for i in range(len(audio_samples_new)):\n",
    "    transformed_sample = transform(audio_samples_new[i])\n",
    "    transformed_mfcc = transform_mfcc(audio_samples_new[i])\n",
    "    \n",
    "    # CoAtNet part\n",
    "    audioDatasetFin.append((transformed_sample, labels[i]))\n",
    "    audioDatasetFinMasking.append((masking(transformed_sample), labels[i]))\n",
    "    \n",
    "    # masking part\n",
    "    audioDatasetMfcc.append((transformed_sample, transformed_mfcc, labels[i]))\n",
    "    audioDatasetMfccMasking.append((masking(transformed_sample), transformed_mfcc, labels[i]))\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "a2fe8d5f61e30540",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:42:01.018081Z",
     "start_time": "2025-02-05T15:42:00.941181Z"
    }
   },
   "source": [
    "# check for lengths of datasets\n",
    "len(audioDatasetMfcc), len(audioDatasetMfcc + audioDatasetMfccMasking), len(audioDatasetFin), len(audioDatasetFin + audioDatasetFinMasking)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1860, 3720, 1860, 3720)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "21c53f1c392a2eb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:42:01.038781Z",
     "start_time": "2025-02-05T15:42:01.019857Z"
    }
   },
   "source": [
    "import time\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class MfccLSTM(nn.Module, BaseEstimator):\n",
    "    def __init__(self, batch_size=16, num_epochs=500, patience=120):\n",
    "        super(MfccLSTM, self).__init__()        \n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        hidden_size = 32\n",
    "        input_size = 20\n",
    "        dropout = 0.2 \n",
    "        num_classes = 36\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.LazyLinear(64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.LazyLinear(128)\n",
    "        self.final_lstm = nn.LSTM(1, 128, batch_first=True, proj_size=64)\n",
    "        \n",
    "        self.fc = nn.LazyLinear(num_classes)\n",
    "    \n",
    "    def forward(self, images, sequences):\n",
    "        # must return shape (batch_size, num_classes) \n",
    "        # batch_size: right now is 16\n",
    "        # num_classes: right now is 36\n",
    "        x1 = self.conv(images)\n",
    "        # print(f'input of first lstm: {sequences.shape[1:]}')\n",
    "        out1, _ = self.lstm(sequences)\n",
    "        out1_dp = self.dropout(out1)\n",
    "        # print(f'output of first lstm: {out1_dp.shape[1:]}')\n",
    "        # print(f'input of second lstm: {out1_dp[:, -1, :].shape[1:]}')\n",
    "        out2, _ = self.lstm2(out1_dp[:, -1, :])\n",
    "        out2_dp = self.dropout(out2)\n",
    "        # print(f'output of second lstm: {out2_dp.shape[1:]}')\n",
    "        x2 = self.fc2(self.fc1(out2_dp))\n",
    "        x3 = torch.cat((x1, x2), 1)\n",
    "        # print(f'output of concatenation: {x3.shape[1:]}')\n",
    "        # x4 = self.fc3(x3)\n",
    "        # # print(f'input final lstm: {x4[:,-1,:].shape[1:]}')\n",
    "        # print(f'x4.shape: {x4.shape[1:]}')\n",
    "        # x_final = self.final_lstm(x4)\n",
    "        # # x = self.fc(final_out[:, -1, :])\n",
    "        x = self.fc(x3)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._optimizer = optim.Adam(self.parameters(), lr=5e-4)\n",
    "        # same training method but now inside the class\n",
    "        model = self.to(device)\n",
    "        \n",
    "        # loss criterion\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # # concatenate so it has the same shape as before\n",
    "        # dataset = np.concatenate((X, y), axis=1)\n",
    "         # concatenate so it has the same shape as before\n",
    "        dataset = [(X[i], y[i]) for i in range(len(X))]\n",
    "        train_set, val_set = train_test_split(dataset, test_size=0.005)\n",
    "        train_loader = DataLoader(train_set, batch_size=self.batch_size)\n",
    "        val_loader = DataLoader(val_set, batch_size=self.batch_size)\n",
    "        \n",
    "        best_val_acc, epochs_no_imp = 0, 0\n",
    "        train_accuracies, val_accuracies = [], []\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            model.train()\n",
    "            epoch_train_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            tic = time.perf_counter()\n",
    "            \n",
    "            for (images, sequences), labels in train_loader:\n",
    "                images = images.to(device)\n",
    "                sequences = sequences.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                self._optimizer.zero_grad()\n",
    "        \n",
    "                # converting labels to Long to avoid error \"not implemented for Int\"\n",
    "                labels = labels.long()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images, sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "                _, predicted_train = torch.max(outputs.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted_train == labels).sum().item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                self._optimizer.step()\n",
    "            \n",
    "            toc = time.perf_counter()\n",
    "            time_taken = toc - tic\n",
    "            \n",
    "            epoch_train_loss /= len(train_loader.dataset)\n",
    "            train_accuracy = correct_train / total_train\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            # Evaluation of the model\n",
    "            model.eval()\n",
    "            total, correct = 0, 0\n",
    "            \n",
    "            for (images, sequences), labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                sequences = sequences.to(device)\n",
    "                labels = labels.to(device)\n",
    "        \n",
    "                outputs = model(images, sequences)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            #\n",
    "            val_accuracy = correct / total\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            if (epoch + 1) % 1 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{self.num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}, Iter Time: {time_taken:.2f}s\")\n",
    "                \n",
    "            if val_accuracy > best_val_acc:\n",
    "                best_val_acc = val_accuracy\n",
    "                epochs_no_imp = 0\n",
    "                best_model_state = model.state_dict()  # Save the best model\n",
    "            else:\n",
    "                epochs_no_imp += 1\n",
    "            if epochs_no_imp >= self.patience:\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "                model.load_state_dict(best_model_state)  # Load the best model\n",
    "                break\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        argnames=[\"images\", \"sequences\"]\n",
    "        fin_dict = {}\n",
    "        # create the list with each of the ith range tuples\n",
    "        for i in range(len(X[0])-1):\n",
    "            fin_dict[argnames[i]] = [torch.tensor(t[i]) for t in dataset]\n",
    "\n",
    "        # torch.stack each one of the lists\n",
    "        for key in fin_dict.keys():\n",
    "            fin_dict[key] = torch.stack(fin_dict[key]).to(device)\n",
    "        \n",
    "        images = [tup[0] for tup in X]\n",
    "        sequences = [tup[1] for tup in X]\n",
    "        images_torch, sequences_torch = torch.tensor(np.array(images)).to(device), torch.tensor(np.array(sequences)).to(device)\n",
    "        # model specifying\n",
    "        model = self.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(images_torch, sequences_torch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        pred = []\n",
    "        # phrase = predicted.tolist()\n",
    "        # for i in range(len(phrase)):\n",
    "        #     pred.append(self.keys[phrase[i]])\n",
    "        # \n",
    "        # pred_df = pd.DataFrame(pred)\n",
    "        # return np.squeeze(pred_df.to_numpy().T)\n",
    "        return predicted.tolist()"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "817ee4d2ff56f788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:42:01.068795Z",
     "start_time": "2025-02-05T15:42:01.040377Z"
    }
   },
   "source": [
    "from coatnet import CoAtNet as CoAtNetImp\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CyclicLR\n",
    "\n",
    "# num_blocks = [1, 2, 2]            \n",
    "# channels = [16, 32, 64]      # Reduced channels throughout\n",
    "\n",
    "num_blocks = [1, 1, 1, 1, 1]            \n",
    "channels = [16, 32, 64, 128, 256]     \n",
    "\n",
    "class CoAtNet(nn.Module, BaseEstimator):\n",
    "    def __init__(self, lr=1e-5, num_epochs=500, patience=30, keys='1234567890QWERTYUIOPASDFGHJKLZXCVBNM'):\n",
    "        super(CoAtNet, self).__init__()    \n",
    "        self.keys = keys\n",
    "        self.model = CoAtNetImp((64, 64), 1, num_blocks, channels, num_classes=len(self.keys), block_types=['C', 'T'])\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        \"\"\"\n",
    "        Reinitialize all model weights to their default initialization state\n",
    "        \"\"\"\n",
    "        def weight_reset(m):\n",
    "            if hasattr(m, 'reset_parameters'):\n",
    "                m.reset_parameters()\n",
    "        \n",
    "        self.model.apply(weight_reset)\n",
    "    \n",
    "        # Reset optimizer\n",
    "        self._optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "    \n",
    "    def fit(self, X, y, model_name, batch_size=16, random_state=42):\n",
    "        # concatenate so it has the same shape as before\n",
    "        dataset = [(X[i], y[i]) for i in range(np.array(X).shape[0])]\n",
    "        # dataset = np.concatenate((X, y), axis=1)\n",
    "        train_set, val_set = train_test_split(dataset, test_size=0.1, random_state=random_state, shuffle=True)\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(SEED)\n",
    "        train_loader, val_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, generator=g), DataLoader(val_set, batch_size=batch_size, shuffle=True, generator=g)\n",
    "\n",
    "        # Initialize model, optimizer, and loss function\n",
    "        self._optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "        # Add learning rate scheduler\n",
    "        # Add OneCycleLR scheduler which often works well with Adam\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "            self._optimizer,\n",
    "            gamma=0.98  # Multiply LR by 0.95 each epoch\n",
    "        )\n",
    "        model = self.model.to(device)\n",
    "        \n",
    "        # loss criterion\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_acc, epochs_no_imp = 0, 0\n",
    "        train_accuracies, val_accuracies = [], []\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            model.train()\n",
    "            epoch_train_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            tic = time.perf_counter()\n",
    "            \n",
    "            for images, labels in train_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                self._optimizer.zero_grad()\n",
    "        \n",
    "                # converting labels to Long to avoid error \"not implemented for Int\"\n",
    "                labels = labels.long()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "                _, predicted_train = torch.max(outputs.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted_train == labels).sum().item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Add gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                                            \n",
    "                self._optimizer.step()\n",
    "                \n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            # scheduler.step()\n",
    "            toc = time.perf_counter()\n",
    "            time_taken = toc - tic\n",
    "            \n",
    "            epoch_train_loss /= len(train_loader.dataset)\n",
    "            train_accuracy = correct_train / total_train\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            # Evaluation of the model\n",
    "            model.eval()\n",
    "            total, correct = 0, 0\n",
    "            \n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "        \n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            val_accuracy = correct / total\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            if (epoch + 1) % 1 == 0 or epoch == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{self.num_epochs}], Train Loss: {epoch_train_loss:.3f}, Train Accuracy: {train_accuracy:.3f}, Val Accuracy: {val_accuracy:.3f} LR: {scheduler.get_last_lr()[0]:.7f} Iter Time: {time_taken:.2f}s\")\n",
    "            if val_accuracy > best_val_acc:\n",
    "                best_val_acc = val_accuracy\n",
    "                epochs_no_imp = 0\n",
    "                best_model_state = model.state_dict()  # Save the best model\n",
    "            else:\n",
    "                epochs_no_imp += 1\n",
    "            if epochs_no_imp >= self.patience:\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "                print(f'Highest val accuracy: {best_val_acc:.3f}')\n",
    "                model.load_state_dict(best_model_state)  # Load the best model\n",
    "                break\n",
    "            \n",
    "        torch.save(self.model.state_dict(), f'models/{model_name}.pth')\n",
    "        epochs_completed = len(train_accuracies)\n",
    "        # Plot accuracy curves\n",
    "        plt.plot(range(1, epochs_completed+1), train_accuracies, label='Training Accuracy')\n",
    "        plt.plot(range(1, epochs_completed+1), val_accuracies, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy vs Epoch')\n",
    "        plt.legend()\n",
    "        # plt.savefig(f'plots/{model_name}_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        return train_accuracies, val_accuracies\n",
    "    \n",
    "    def predict(self, X):\n",
    "        argnames=[\"x\"]\n",
    "        fin_dict = {}\n",
    "        # create the list with each of the ith range tuples\n",
    "        # print(range(len(X[0])-1))\n",
    "        # for i in range(len(X[0])-1):\n",
    "        #     fin_dict[argnames[i]] = [t[i] for t in dataset]\n",
    "        #     \n",
    "        # # torch.stack each one of the lists\n",
    "        # for key in fin_dict.keys():\n",
    "        #     fin_dict[key] = torch.stack(fin_dict[key]).to(device)\n",
    "        \n",
    "        X = torch.tensor(np.array(X)).to(device)\n",
    "        \n",
    "        # model specifying\n",
    "        model = self.model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        pred = []\n",
    "        # phrase = predicted.tolist()\n",
    "        # for i in range(len(phrase)):\n",
    "        #     pred.append(self.keys[phrase[i]])\n",
    "        # \n",
    "        # pred_df = pd.DataFrame(pred)\n",
    "        # return np.squeeze(pred_df.to_numpy().T)\n",
    "        return predicted.tolist()"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:42:01.084891Z",
     "start_time": "2025-02-05T15:42:01.070303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=36):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.LazyLinear(512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 14 * 14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, dataset, num_epochs, model_path, patience=30, leave_one_out=False):\n",
    "        \"\"\"Trains the CNN model with early stopping functionality.\n",
    "        \n",
    "        Args:\n",
    "            dataset: Dataset containing training samples and labels\n",
    "            num_epochs: Maximum number of training epochs\n",
    "            model_path: Path to save the trained model\n",
    "            patience: Number of epochs to wait for improvement before early stopping\n",
    "            leave_one_out: Boolean indicating whether to use leave-one-out validation\n",
    "            \n",
    "        Returns:\n",
    "            Lists containing training/validation metrics history\n",
    "        \"\"\"\n",
    "        train_losses, train_accuracies = [], []\n",
    "        val_losses, val_accuracies = [], []\n",
    "        \n",
    "        # Split dataset into training and validation sets\n",
    "        if leave_one_out:\n",
    "            train_set, val_set = dataset[:-1], [dataset[-1]]\n",
    "        else:\n",
    "            train_set, val_set = train_test_split(dataset, test_size=0.10)\n",
    "        \n",
    "        train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "        val_loader = DataLoader(val_set, batch_size=16, shuffle=True)\n",
    "    \n",
    "        # Initialize optimizer, loss criterion and early stopping variables\n",
    "        self.to(device)\n",
    "        optimizer = optim.Adam(self.parameters(), lr=5e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_acc = 0\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = None\n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            self.train()\n",
    "            epoch_train_loss = 0.0\n",
    "            correct_train, total_train = 0, 0\n",
    "            tic = time.perf_counter()\n",
    "    \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_train_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "                _, predicted_train = torch.max(outputs.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted_train == labels).sum().item()\n",
    "    \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "    \n",
    "            toc = time.perf_counter()\n",
    "            time_taken = toc - tic\n",
    "    \n",
    "            # Calculate training metrics\n",
    "            epoch_train_loss /= len(train_loader.dataset)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "            train_accuracy = correct_train / total_train\n",
    "            train_accuracies.append(train_accuracy)\n",
    "    \n",
    "            # Validation phase\n",
    "            self.eval()\n",
    "            total, correct = 0, 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "    \n",
    "                    outputs = self(inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "    \n",
    "            val_accuracy = correct / total\n",
    "            val_accuracies.append(val_accuracy)\n",
    "    \n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                  f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "                  f\"Train Accuracy: {train_accuracy:.4f}, \"\n",
    "                  f\"Val Accuracy: {val_accuracy:.4f}, \"\n",
    "                  f\"Iter Time: {time_taken:.2f}s\")\n",
    "    \n",
    "            # Early stopping check\n",
    "            if val_accuracy > best_val_acc:\n",
    "                best_val_acc = val_accuracy\n",
    "                epochs_no_improve = 0\n",
    "                best_model_state = self.state_dict()\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "    \n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                print(f'Highest val accuracy: {best_val_acc:.3f}')\n",
    "                self.load_state_dict(best_model_state)  # Restore best model\n",
    "                break\n",
    "    \n",
    "            # Plot final training curves\n",
    "            if (epoch == num_epochs - 1 or epochs_no_improve >= patience) and epoch != 0:\n",
    "                plt.plot(range(len(train_accuracies)), train_accuracies, \n",
    "                        label='Training Accuracy', color=\"blue\")\n",
    "                plt.plot(range(len(val_accuracies)), val_accuracies,\n",
    "                        label='Validation Accuracy', color=\"orange\")\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Accuracy') \n",
    "                plt.title('Training vs Validation Accuracy')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "    \n",
    "        # Save the best model\n",
    "        torch.save(best_model_state, model_path)\n",
    "        \n",
    "        return train_losses, train_accuracies, val_losses, val_accuracies\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicts class indices for input samples.\n",
    "        \n",
    "        Args:\n",
    "            X: Input tensor or numpy array of shape [1, 64, 64] or batch of inputs\n",
    "            \n",
    "        Returns:\n",
    "            List of predicted class indices\n",
    "        \"\"\"\n",
    "        # Ensure model is in evaluation mode\n",
    "        self.eval() \n",
    "        self.to(device)\n",
    "        \n",
    "        # Convert input to torch tensor if numpy array\n",
    "        X = torch.tensor(np.array(X))\n",
    "            \n",
    "        # Move input to device\n",
    "        X = X.to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        return predicted.tolist()"
   ],
   "id": "79f898c1100eb099",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:42:01.094964Z",
     "start_time": "2025-02-05T15:42:01.086108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "def train(model, dataset, num_epochs, model_path, leave_one_out=False):\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "    \n",
    "    if leave_one_out:\n",
    "        train_set, val_set = dataset[:-1], [dataset[-1]]\n",
    "    else:\n",
    "        train_set, val_set = train_test_split(dataset, test_size=0.10)\n",
    "    train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=16, shuffle=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.BCELoss()\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        correct_train = 0  # correct training examples\n",
    "        total_train = 0 # total training examples\n",
    "        tic = time.perf_counter()\n",
    "        \n",
    "        # print(f'---- EPOCH {epoch} ----')\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted_train = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted_train == labels).sum().item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        toc = time.perf_counter()\n",
    "        time_taken = toc - tic\n",
    "        \n",
    "        epoch_train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Evaluation of the model\n",
    "        model.eval()\n",
    "        total, correct = 0, 0\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # print(\"VALIDATION\")\n",
    "            # print(f'outputs: {outputs}')\n",
    "            # print(f'predicted: {predicted}')\n",
    "            # print(f'labels: {labels}')\n",
    "            \n",
    "        val_accuracy = correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} / Val Accuracy: {val_accuracy:.4f} Iter Time: {time_taken:.2f}s\")\n",
    "        \n",
    "        if epoch == num_epochs - 1 and epoch != 0:\n",
    "            plt.plot(range(epoch+1), train_accuracies, label='Training Accuracy', color=\"blue\")\n",
    "            plt.plot(range(epoch+1), val_accuracies, label='Validation Accuracy', color=\"orange\")\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title(f'Training vs Validation Accuracy')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "    torch.save(model.state_dict(), model_path)"
   ],
   "id": "b384783593ef4781",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:42:01.098802Z",
     "start_time": "2025-02-05T15:42:01.096505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def getIndCurrKeys(ind: int):\n",
    "    return curr_keys[ind]"
   ],
   "id": "44ed2a512289bde3",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:42:01.109548Z",
     "start_time": "2025-02-05T15:42:01.100095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ],
   "id": "c6e08104a3dc0684",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x158d7ea30>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:42:01.275574Z",
     "start_time": "2025-02-05T15:42:01.111108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "# Create the SpecAugment augmenter once|\n",
    "spec_aug = T.SpecAugment(\n",
    "    time_mask_param=6,\n",
    "    freq_mask_param=6,\n",
    "    n_time_masks=2,\n",
    "    n_freq_masks=2\n",
    ")"
   ],
   "id": "ea1c3398d8c60e9e",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:42:01.284139Z",
     "start_time": "2025-02-05T15:42:01.276644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from specAugment.sparse_image_warp_zcaceres import sparse_image_warp\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "\n",
    "def time_warp(spec, W=5):\n",
    "    num_rows = spec.shape[0]\n",
    "    spec_len = spec.shape[1]\n",
    "    y = num_rows // 2\n",
    "    horizontal_line_at_ctr = spec[:, y, :]\n",
    "    assert horizontal_line_at_ctr.shape[1] == spec_len\n",
    "\n",
    "    point_to_warp = torch.Tensor(horizontal_line_at_ctr[0, random.randrange(W, spec_len - W)])\n",
    "    assert isinstance(point_to_warp, torch.Tensor)\n",
    "\n",
    "    # Uniform distribution from (0,W) with chance to be up to W negative\n",
    "    dist_to_warp = random.randrange(-W, W)\n",
    "    src_pts, dest_pts = torch.tensor([[[y, point_to_warp]]]), torch.tensor([[[y, point_to_warp + dist_to_warp]]])\n",
    "    warped_spectro, dense_flows = sparse_image_warp(spec, src_pts, dest_pts)\n",
    "    return warped_spectro.squeeze(3)\n",
    "\n",
    "\n",
    "def spec_augment(mel_spectrogram, time_warping_para=80, frequency_masking_para=27,\n",
    "                 time_masking_para=100, frequency_mask_num=1, time_mask_num=1):\n",
    "    \"\"\"Spec augmentation Calculation Function.\n",
    "\n",
    "    'SpecAugment' have 3 steps for audio data augmentation.\n",
    "    first step is time warping using Tensorflow's image_sparse_warp function.\n",
    "    Second step is frequency masking, last step is time masking.\n",
    "\n",
    "    # Arguments:\n",
    "      mel_spectrogram(numpy array): audio file path of you want to warping and masking.\n",
    "      time_warping_para(float): Augmentation parameter, \"time warp parameter W\".\n",
    "        If none, default = 80 for LibriSpeech.\n",
    "      frequency_masking_para(float): Augmentation parameter, \"frequency mask parameter F\"\n",
    "        If none, default = 100 for LibriSpeech.\n",
    "      time_masking_para(float): Augmentation parameter, \"time mask parameter T\"\n",
    "        If none, default = 27 for LibriSpeech.\n",
    "      frequency_mask_num(float): number of frequency masking lines, \"m_F\".\n",
    "        If none, default = 1 for LibriSpeech.\n",
    "      time_mask_num(float): number of time masking lines, \"m_T\".\n",
    "        If none, default = 1 for LibriSpeech.\n",
    "\n",
    "    # Returns\n",
    "      mel_spectrogram(numpy array): warped and masked mel spectrogram.\n",
    "    \"\"\"\n",
    "    v = mel_spectrogram.shape[0]\n",
    "    tau = mel_spectrogram.shape[1]\n",
    " \n",
    "    # Step 1 : Time warping (TO DO...)\n",
    "    # warped_mel_spectrogram = time_warp(mel_spectrogram)\n",
    "    warped_mel_spectrogram = mel_spectrogram\n",
    "\n",
    "    # Step 2 : Frequency masking\n",
    "    for i in range(frequency_mask_num):\n",
    "        f = np.random.uniform(low=0.0, high=frequency_masking_para)\n",
    "        f = int(f)\n",
    "        f0 = random.randint(0, v - f)\n",
    "        warped_mel_spectrogram[f0:f0 + f, :] = 0\n",
    "\n",
    "    # Step 3 : Time masking\n",
    "    for i in range(time_mask_num):\n",
    "        t = np.random.uniform(low=0.0, high=time_masking_para)\n",
    "        t = int(t)\n",
    "        t0 = random.randint(0, tau - t)\n",
    "        warped_mel_spectrogram[:, t0:t0 + t] = 0\n",
    "\n",
    "    return warped_mel_spectrogram\n",
    "\n",
    "\n",
    "def visualization_spectrogram(mel_spectrogram, title):\n",
    "    \"\"\"visualizing result of SpecAugment\n",
    "\n",
    "    # Arguments:\n",
    "      mel_spectrogram(ndarray): mel_spectrogram to visualize.\n",
    "      title(String): plot figure's title\n",
    "    \"\"\"\n",
    "    # Show mel-spectrogram using librosa's specshow.\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(librosa.power_to_db(mel_spectrogram, ref=np.max), y_axis='mel', fmax=8000, x_axis='time')\n",
    "    # plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "d263379d3882d7a4",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:42:19.138252Z",
     "start_time": "2025-02-05T15:42:01.284971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "accuracies, precisions, f1_scores, recalls, highest_val_accs = [], [], [], [], []\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'Training time #{i}')\n",
    "    dataset = audio_samples\n",
    "    dataset_labels = labels_no_masking\n",
    "    train_set, test_set, labels_train_set, labels_test_set = train_test_split(dataset, dataset_labels, test_size=0.2, random_state=SEED, shuffle=True)\n",
    "    final_train_set = []\n",
    "    coatnet, cnn = True, False\n",
    "    lr = 5e-5\n",
    "    curr_keys = list('1234567890QWERTYUIOPASDFGHJKLZXCVBNM-')\n",
    "    \n",
    "    if coatnet:\n",
    "        model = CoAtNet(lr=lr, keys=curr_keys, num_epochs=700)\n",
    "        model.reset_weights()\n",
    "        for i in range(len(train_set)):\n",
    "            # Original sample\n",
    "            transformed_sample = transform(train_set[i])\n",
    "            final_train_set.append((transformed_sample, labels_train_set[i]))\n",
    "    \n",
    "            # # Time-shifted sample\n",
    "            time_shifted_sample = transform(time_shift(train_set[i]))\n",
    "            final_train_set.append((time_shifted_sample, labels_train_set[i]))\n",
    "    \n",
    "            # Time-shifted sample\n",
    "            # time_shifted_sample2 = transform(time_shift(train_set[i]))\n",
    "            # final_train_set.append((spec_aug(time_shifted_sample2), labels_train_set[i]))\n",
    "    \n",
    "            # # Masked original sample\n",
    "            augmented_spec = spec_aug(transformed_sample)\n",
    "            final_train_set.append((augmented_spec, labels_train_set[i]))\n",
    "            # \n",
    "            # # Masked time-shifted sample\n",
    "            augmented_time_shifted = spec_aug(time_shifted_sample)\n",
    "            final_train_set.append((augmented_time_shifted, labels_train_set[i]))\n",
    "        #   Copy final train set to iterate over it\n",
    "        X_train = [t[0] for t in final_train_set]\n",
    "        y_train = [t[1] for t in final_train_set]\n",
    "        print(f'LEN FINAL TRAIN SET: {len(final_train_set)}')\n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.now()\n",
    "        formatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M\")\n",
    "        # print(\"Formatted Date and Time:\", formatted_datetime)\n",
    "        batch_size = 16\n",
    "        model_name = f'model_with_recorded_space_no_time_shift_ONLY_masking_{formatted_datetime}'\n",
    "        _, val_accuracies = model.fit(X_train, y_train, model_name, batch_size=batch_size)\n",
    "\n",
    "        highest_val_accs.append(np.max(val_accuracies))\n",
    "        # train(cnn_model, final_train_set, 200, model_name, leave_one_out=False)\n",
    "        final_test_set = list(map(transform, test_set))\n",
    "        prediction = model.predict(final_test_set)\n",
    "        np_prediction = np.array(prediction)\n",
    "        accuracy = accuracy_score(labels_test_set, np_prediction)\n",
    "        print(f'Final Accuracy: {accuracy:.3f}')\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision = precision_score(labels_test_set, np_prediction, average='weighted')\n",
    "        recall = recall_score(labels_test_set, np_prediction, average='weighted')\n",
    "        f1 = f1_score(labels_test_set, np_prediction, average='weighted')\n",
    "        print(f'Precision: {precision:.3f}')\n",
    "        print(f'Recall: {recall:.3f}')\n",
    "        print(f'F1 Score: {f1:.3f}')\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "    elif not cnn :\n",
    "        model = MfccLSTM()\n",
    "        for i in range(len(train_set)):\n",
    "            transformed_mfcc = transform_mfcc(train_set[i])\n",
    "            transformed_sample = transform(train_set[i])\n",
    "            final_train_set.append((transformed_sample, transformed_mfcc, labels_train_set[i]))\n",
    "            final_train_set.append((masking(transformed_sample), transformed_mfcc, labels_train_set[i]))\n",
    "        X_train = [(t[0],t[1]) for t in final_train_set]\n",
    "        y_train = [t[2] for t in final_train_set]\n",
    "    elif cnn:\n",
    "            for i in range(len(train_set)):\n",
    "                transformed_sample = transform(train_set[i])\n",
    "                final_train_set.append((transformed_sample, labels_train_set[i]))\n",
    "        \n",
    "                # # Time-shifted sample\n",
    "                time_shifted_sample = transform(time_shift(train_set[i]))\n",
    "                final_train_set.append((time_shifted_sample, labels_train_set[i]))\n",
    "        \n",
    "                # # Time-shifted sample\n",
    "                # time_shifted_sample2 = transform(time_shift(train_set[i]))\n",
    "                # final_train_set.append((spec_aug(time_shifted_sample2), labels_train_set[i]))\n",
    "        \n",
    "                # # Masked original sample\n",
    "                # augmented_spec = spec_aug(transformed_sample)\n",
    "                # final_train_set.append((augmented_spec, labels_train_set[i]))\n",
    "                # \n",
    "                # # Masked time-shifted sample\n",
    "                augmented_time_shifted = spec_aug(time_shifted_sample)\n",
    "                final_train_set.append((augmented_time_shifted, labels_train_set[i]))\n",
    "            \n",
    "            cnn_model = CNN(len(curr_keys))\n",
    "            model_name = \"simple_CNN_test_1\"\n",
    "            _, _, _, val_accuracies = cnn_model.fit(final_train_set, 200, model_name, patience=30, leave_one_out=False)\n",
    "            highest_val_accs.append(np.max(val_accuracies))\n",
    "            # train(cnn_model, final_train_set, 200, model_name, leave_one_out=False)\n",
    "            final_test_set = list(map(transform, test_set))\n",
    "            prediction = cnn_model.predict(final_test_set)\n",
    "            np_prediction = np.array(prediction)\n",
    "            accuracy = accuracy_score(labels_test_set, np_prediction)\n",
    "            print(f'Final Accuracy: {accuracy:.3f}')\n",
    "            # Calculate precision, recall, and F1 score\n",
    "            precision = precision_score(labels_test_set, np_prediction, average='weighted')\n",
    "            recall = recall_score(labels_test_set, np_prediction, average='weighted')\n",
    "            f1 = f1_score(labels_test_set, np_prediction, average='weighted')\n",
    "            print(f'Precision: {precision:.3f}')\n",
    "            print(f'Recall: {recall:.3f}')\n",
    "            print(f'F1 Score: {f1:.3f}')\n",
    "            accuracies.append(accuracy)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "print(\"FINAL AVERAGES OF CNN: \")\n",
    "print(f'Final Accuracy: {np.average(accuracies):.3f}')\n",
    "print(f'Final Precision: {np.average(precisions):.3f}')\n",
    "print(f'Final Recall: {np.average(recalls):.3f}')\n",
    "print(f'Final F1 Score: {np.average(f1_scores):.3f}')\n",
    "print(f'Final Highest Validation Accuracy: {np.max(highest_val_accs):.3f}')\n",
    "        \n",
    "# param_grid = {\n",
    "#     'patience': [20],\n",
    "#     'lr': [1e-5, 1e-6],\n",
    "#     'num_epochs': [700]\n",
    "# }\n",
    "# \n",
    "# scoring = {\n",
    "#     'accuracy': make_scorer(accuracy_score),\n",
    "#     'f1_weighted': make_scorer(f1_score, average='weighted', zero_division=0.0),\n",
    "#     'precision_weighted': make_scorer(precision_score, average='weighted', zero_division=0.0),\n",
    "#     'recall_weighted': make_scorer(recall_score, average='weighted', zero_division=0.0),\n",
    "# }\n",
    "# \n",
    "# grid_search = GridSearchCV(CoAtNet(), param_grid, cv=5, scoring=scoring, refit=False, verbose=3)\n",
    "# fit_params = {\"model_name\": 'grid_search_model_28-01-25', \"batch_size\": 16}\n",
    "# grid_search.fit(X_train, y_train, **fit_params)\n",
    "# # Get the current date and time\n",
    "# current_datetime = datetime.now()\n",
    "# formatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M\")\n",
    "# # print(\"Formatted Date and Time:\", formatted_datetime)\n",
    "# batch_size = 16\n",
    "# model_name = f'model_with_recorded_space_no_time_shift_ONLY_masking_{formatted_datetime}'\n",
    "# model.fit(X_train, y_train, model_name, batch_size=batch_size)\n",
    "\n",
    "# # # # Load the existing checkpoint\n",
    "# # checkpoint = torch.load(\"models/13-01-24.pth\", weights_only=True)\n",
    "# #\n",
    "# # # Rename the keys\n",
    "# # new_checkpoint = {}\n",
    "# # for k, v in checkpoint.items():\n",
    "# #     new_checkpoint[f\"model.{k}\"] = v\n",
    "# #\n",
    "# # model.load_state_dict(new_checkpoint, strict=True)\n",
    "# # model.load_state_dict(torch.load(\"models/13-01-24.pth\", weights_only=True), strict=True)\n",
    "#\n",
    "# final_test_set = list(map(transform, test_set))\n",
    "# print(final_test_set[0].shape)\n",
    "# prediction = model.predict(final_test_set)\n",
    "# np_prediction = np.array(prediction)\n",
    "# accuracy = accuracy_score(labels_test_set, np_prediction)\n",
    "# print(f'Final Accuracy: {accuracy:.3f}')\n",
    "# # Calculate precision, recall, and F1 score\n",
    "# precision = precision_score(labels_test_set, np_prediction, average='weighted')\n",
    "# recall = recall_score(labels_test_set, np_prediction, average='weighted')\n",
    "# f1 = f1_score(labels_test_set, np_prediction, average='weighted')\n",
    "# \n",
    "# print(f'Precision: {precision:.3f}')\n",
    "# print(f'Recall: {recall:.3f}')\n",
    "# print(f'F1 Score: {f1:.3f}')"
   ],
   "id": "582f0155d3d48bef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN FINAL TRAIN SET: 2976\n",
      "Before pooling:torch.Size([16, 64, 8, 8])\n",
      "After pooling:torch.Size([400, 64])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (400) to match target batch_size (16).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 48\u001B[0m\n\u001B[1;32m     46\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m16\u001B[39m\n\u001B[1;32m     47\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_with_recorded_space_no_time_shift_ONLY_masking_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mformatted_datetime\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 48\u001B[0m _, val_accuracies \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, model_name, batch_size\u001B[38;5;241m=\u001B[39mbatch_size)\n\u001B[1;32m     50\u001B[0m highest_val_accs\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39mmax(val_accuracies))\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# train(cnn_model, final_train_set, 200, model_name, leave_one_out=False)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[26], line 75\u001B[0m, in \u001B[0;36mCoAtNet.fit\u001B[0;34m(self, X, y, model_name, batch_size, random_state)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[1;32m     74\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(images)\n\u001B[0;32m---> 75\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     76\u001B[0m epoch_train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m images\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     78\u001B[0m _, predicted_train \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs\u001B[38;5;241m.\u001B[39mdata, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1293\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1292\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mcross_entropy(\n\u001B[1;32m   1294\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   1295\u001B[0m         target,\n\u001B[1;32m   1296\u001B[0m         weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight,\n\u001B[1;32m   1297\u001B[0m         ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mignore_index,\n\u001B[1;32m   1298\u001B[0m         reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduction,\n\u001B[1;32m   1299\u001B[0m         label_smoothing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_smoothing,\n\u001B[1;32m   1300\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3479\u001B[0m, in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   3477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3478\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3479\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mcross_entropy_loss(\n\u001B[1;32m   3480\u001B[0m     \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   3481\u001B[0m     target,\n\u001B[1;32m   3482\u001B[0m     weight,\n\u001B[1;32m   3483\u001B[0m     _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction),\n\u001B[1;32m   3484\u001B[0m     ignore_index,\n\u001B[1;32m   3485\u001B[0m     label_smoothing,\n\u001B[1;32m   3486\u001B[0m )\n",
      "\u001B[0;31mValueError\u001B[0m: Expected input batch_size (400) to match target batch_size (16)."
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f701a66316c93347"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T20:41:44.344783Z",
     "start_time": "2025-02-04T20:41:44.344720Z"
    }
   },
   "cell_type": "code",
   "source": "final_test_set = list(map(transform, test_set))",
   "id": "55ce0f48a6143cb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T20:41:44.345341Z",
     "start_time": "2025-02-04T20:41:44.345306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(labels_test_set))\n",
    "prediction = model.predict(final_test_set)\n",
    "np_prediction = np.array(prediction)\n",
    "labels_test_set = np.array(labels_test_set)\n",
    "print(labels_test_set[:10])\n",
    "print(np_prediction[:10])\n",
    "print(labels_test_set[np.isin(labels_test_set, [36])])\n",
    "print(np_prediction[np.isin(np_prediction, [36])])\n",
    "accuracy = accuracy_score(labels_test_set, np_prediction)\n",
    "print(f'Final Accuracy: {accuracy:.3f}')\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(labels_test_set, np_prediction, average='weighted')\n",
    "recall = recall_score(labels_test_set, np_prediction, average='weighted')\n",
    "f1 = f1_score(labels_test_set, np_prediction, average='weighted')\n"
   ],
   "id": "43a9f8add20fd0bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_standard_mapper(keys='1234567890QWERTYUIOPASDFGHJKLZXCVBNM-'):\n",
    "    \"\"\"\n",
    "    Create a standard mapper using the predefined keys.\n",
    "    \n",
    "    Args:\n",
    "        keys: String of valid keys in order\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Mapping from characters to indices\n",
    "    \"\"\"\n",
    "    return {char: idx for idx, char in enumerate(keys)}\n",
    "\n",
    "def create_reverse_mapper(mapper):\n",
    "    \"\"\"\n",
    "    Create a reverse mapper (from indices to characters).\n",
    "    \n",
    "    Args:\n",
    "        mapper: Dictionary mapping characters to indices\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Mapping from indices to characters\n",
    "    \"\"\"\n",
    "    return {v: k for k, v in mapper.items()}\n",
    "\n",
    "def convert_test_set_to_df(test_set, labels_test_set, standard_keys='1234567890QWERTYUIOPASDFGHJKLZXCVBNM-'):\n",
    "    \"\"\"\n",
    "    Convert test set and labels into a pandas DataFrame format.\n",
    "    \n",
    "    Args:\n",
    "        test_set: List/array of keystroke waveforms\n",
    "        labels_test_set: List/array of corresponding labels\n",
    "        standard_keys: String of valid keys in order\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'Key' and 'File' columns\n",
    "        Dict: Mapper from characters to indices\n",
    "    \"\"\"\n",
    "    # Create the standard mapper\n",
    "    mapper = create_standard_mapper(standard_keys)\n",
    "    \n",
    "    # Create dictionary for DataFrame\n",
    "    data_dict = {\n",
    "        'Key': labels_test_set,  # These should already be numerical\n",
    "        'File': test_set\n",
    "    }\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    return df, mapper"
   ],
   "id": "c12cbfa07692473c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from scipy.io import wavfile\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "def mono_to_stereo(mono_signal: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert a mono signal to stereo by duplicating the channel.\"\"\"\n",
    "    return np.stack([mono_signal, mono_signal], axis=1)\n",
    "\n",
    "def add_natural_silence(length: int) -> np.ndarray:\n",
    "    \"\"\"Generate a stereo silence.\"\"\"\n",
    "    return np.zeros((length, 2))\n",
    "\n",
    "def get_keystroke_by_label(dataset: pd.DataFrame, \n",
    "                          label: Union[str, int], \n",
    "                          mapper: Dict[str, int] = None) -> np.ndarray:\n",
    "    \"\"\"Get a keystroke sample with minimal processing.\"\"\"\n",
    "    try:\n",
    "        if mapper and isinstance(label, str):\n",
    "            if label not in mapper:\n",
    "                raise KeyError(f\"Character '{label}' not found in mapper\")\n",
    "            label = mapper[label]\n",
    "        \n",
    "        # Get all samples for this label\n",
    "        samples = dataset[dataset['Key'] == label]['File'].values\n",
    "        \n",
    "        if len(samples) == 0:\n",
    "            raise ValueError(f\"No keystroke found for label {label}\")\n",
    "        \n",
    "        # Get a random sample and ensure correct format\n",
    "        mono_sample = np.random.choice(samples)\n",
    "        return mono_to_stereo(mono_sample)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing label '{label}': {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def reconstruct_word(word: str, \n",
    "                    dataset: pd.DataFrame,\n",
    "                    mapper: Dict[str, int] = None,\n",
    "                    silence_length: float = 0.3,\n",
    "                    keystroke_pause_variation: float = 0.1,\n",
    "                    initial_silence: float = 0.5,\n",
    "                    sr: int = 44100) -> np.ndarray:\n",
    "    \"\"\"Reconstruct a word from keystrokes.\"\"\"\n",
    "    segments = []\n",
    "    \n",
    "    # Add initial silence\n",
    "    initial_silence_samples = int(initial_silence * sr)\n",
    "    segments.append(add_natural_silence(initial_silence_samples))\n",
    "    \n",
    "    # Process each character\n",
    "    for i, char in enumerate(word.upper()):\n",
    "        try:\n",
    "            # Get keystroke audio\n",
    "            keystroke = get_keystroke_by_label(dataset, char, mapper)\n",
    "            segments.append(keystroke)\n",
    "            \n",
    "            # Add varied silence after keystroke (except for last character)\n",
    "            if i < len(word) - 1:\n",
    "                varied_silence = silence_length + np.random.uniform(-keystroke_pause_variation, keystroke_pause_variation)\n",
    "                silence_samples = int(varied_silence * sr)\n",
    "                segments.append(add_natural_silence(silence_samples))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Skipping character '{char}': {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Add final silence\n",
    "    final_silence = int(0.5 * sr)\n",
    "    segments.append(add_natural_silence(final_silence))\n",
    "    \n",
    "    # Concatenate all segments\n",
    "    if segments:\n",
    "        return np.vstack(segments)\n",
    "    else:\n",
    "        return np.array([])\n",
    "\n",
    "def save_reconstructed_word(word: str,\n",
    "                                  dataset: pd.DataFrame,\n",
    "                                  output_base: str,\n",
    "                                  mapper: Dict[str, int] = None,\n",
    "                                  silence_length: float = 0.3,\n",
    "                                  keystroke_pause_variation: float = 0.1,\n",
    "                                  initial_silence: float = 0.5,\n",
    "                                  sr: int = 44100) -> None:\n",
    "    \"\"\"Save reconstructed word using multiple methods.\"\"\"\n",
    "    try:\n",
    "        # Generate the audio\n",
    "        audio = reconstruct_word(word, dataset, mapper, silence_length, \n",
    "                               keystroke_pause_variation, initial_silence, sr)\n",
    "        \n",
    "        if len(audio) > 0:\n",
    "            # Method 1: soundfile with float32\n",
    "            sf.write(\n",
    "                f\"{output_base}_sf_float.wav\",\n",
    "                audio,\n",
    "                sr,\n",
    "                subtype='FLOAT',\n",
    "                format='WAV'\n",
    "            )\n",
    "            \n",
    "            # Method 2: soundfile with PCM_24\n",
    "            # sf.write(\n",
    "            #     f\"{output_base}_sf_pcm24.wav\",\n",
    "            #     audio,\n",
    "            #     sr,\n",
    "            #     subtype='PCM_24'\n",
    "            # )\n",
    "            \n",
    "            # Method 3: scipy.io.wavfile (32-bit float)\n",
    "            # Ensure range is -1 to 1\n",
    "            audio_norm = audio / np.max(np.abs(audio))\n",
    "            # wavfile.write(\n",
    "            #     f\"{output_base}_scipy_float32.wav\",\n",
    "            #     sr,\n",
    "            #     audio_norm.astype(np.float32)\n",
    "            # )\n",
    "            \n",
    "            # Method 4: scipy.io.wavfile (16-bit int)\n",
    "            audio_int16 = (audio_norm * 32767).astype(np.int16)\n",
    "            # wavfile.write(\n",
    "            #     f\"{output_base}_scipy_int16.wav\",\n",
    "            #     sr,\n",
    "            #     audio_int16\n",
    "            # )\n",
    "            \n",
    "            print(f\"Saved reconstructed audio using multiple methods\")\n",
    "            print(f\"Audio shape: {audio.shape}\")\n",
    "            print(f\"Sample rate: {sr}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"No audio was reconstructed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error reconstructing word: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "save_reconstructed_word_methods(\n",
    "    word=\"HELLO\",\n",
    "    dataset=test_df,\n",
    "    output_base=\"hello_test\",\n",
    "    mapper=test_mapper,\n",
    "    silence_length=0.3,\n",
    "    keystroke_pause_variation=0.1,\n",
    "    initial_silence=0.5\n",
    ")\n",
    "\"\"\""
   ],
   "id": "f26b2f8f63c2f2a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def transform_final(file_path, key_length=14400):\n",
    "    # initial config\n",
    "    n_fft, hop_length, before, after, prom, show = 1024, 225, 2400, 12000, 0.2391, True\n",
    "    curr_step = 0.01\n",
    "\n",
    "    # rest of transform\n",
    "    samples, sr = load_like_librosa(file_path, 44100)\n",
    "    # samples, sr = librosa.load(file_path)\n",
    "    # samples = nr.reduce_noise(samples, sr=44100)\n",
    "    peaks_count = count_peaks(samples, key_length, True)\n",
    "    strokes = isolator(samples, sr, n_fft, hop_length, before, after, prom, False)[1]\n",
    "    num_keys = len(strokes)\n",
    "    count = 0\n",
    "    k = prom\n",
    "    prev_k = prom\n",
    "    print(f'num_keys: {num_keys} // peaks_count: {peaks_count} // prom: {prom}')\n",
    "    while num_keys != peaks_count:\n",
    "        if num_keys > peaks_count:\n",
    "            if count > 0 and prev_k == k + curr_step:\n",
    "                curr_step /= 2\n",
    "            elif count > 0:\n",
    "                curr_step += (curr_step / 2)\n",
    "            prev_k = k\n",
    "            k += curr_step\n",
    "        else:\n",
    "            if count > 0 and prev_k == k - curr_step:\n",
    "                curr_step /= 2\n",
    "            elif count > 0:\n",
    "                curr_step += (curr_step / 2)\n",
    "            prev_k = k\n",
    "            k += -curr_step\n",
    "        strokes = isolator(samples, sr, n_fft, hop_length, before, after, k, False)[1]\n",
    "        num_keys = len(strokes)\n",
    "        if num_keys == peaks_count:\n",
    "            print(f'k: {k} // peaks_count: {peaks_count} // num_keys: {num_keys}')\n",
    "            isolator(samples, sr, n_fft, hop_length, before, after, k, True)\n",
    "        count += 1\n",
    "\n",
    "    final_transform = Compose([ToMelSpectrogram(key_length)])\n",
    "    return list(map(final_transform, strokes))"
   ],
   "id": "279486acaba5157f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert your test set to DataFrame\n",
    "test_df, test_mapper = convert_test_set_to_df(test_set, labels_test_set, curr_keys)\n",
    "\n",
    "with open('test_dataset_only_space_mbp_2_jorge.pkl', 'wb') as f:\n",
    "    pickle.dump(test_df, f)"
   ],
   "id": "ca3a138f9600cc2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print some debug information\n",
    "print(\"Mapper:\", test_mapper)\n",
    "print(\"Available keys:\", sorted(test_mapper.keys()))\n",
    "print(\"Unique labels in dataset:\", sorted(test_df['Key'].unique()))\n",
    "\n",
    "words = [\"Basketball\", \"Esternocleidomastoideo\", \"Electroencefalografista\", \"Caleidoscopia\", \"Arterioesclerosis\", \"Pediatria\", \"Filosofia\", \"Megalodon\"]\n",
    "accuracies = []\n",
    "for word in words: \n",
    "    final_word = word.replace(' ', '-')\n",
    "    final_word_list = [char for char in word.upper()]\n",
    "    # Now try reconstructing a word\n",
    "    # Create 32-bit stereo WAV file\n",
    "    save_reconstructed_word(\n",
    "        word=final_word,\n",
    "        dataset=test_df,\n",
    "        output_base=f\"test_{final_word}\",\n",
    "        mapper=test_mapper,\n",
    "        silence_length=0.4,\n",
    "        keystroke_pause_variation=0.0,\n",
    "    )\n",
    "    \n",
    "    file_transformed = transform_final(f\"test_{final_word}_sf_float.wav\")\n",
    "    prediction = model.predict(file_transformed)\n",
    "    # keys = '1234567890QWERTYUIOPASDFGHJKLZXCVBNM'\n",
    "    final_prediction = [curr_keys[key_ind] for key_ind in prediction]\n",
    "    print(final_prediction)\n",
    "    print(final_word_list)\n",
    "    accuracy = accuracy_score(final_prediction, final_word_list)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f'Final Accuracy: {accuracy:.3f}')\n",
    "\n",
    "print(f'Accuracy avg: {np.average(np.array(accuracies)):.3f}')\n",
    "\n"
   ],
   "id": "bb4d0eb4a659fb16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# file_transformed = transform(\"../Dataset-for-Binary/base-audio/audio_H.wav\")\n",
    "\n",
    "# file_transformed = transform(\"../Dataset-for-Binary/base-audio/audio_H.wav\")\n",
    "# prediction = model.predict(file_transformed)\n",
    "# final_prediction = [keys[key_ind] for key_ind in prediction]\n",
    "# print(final_prediction)\n",
    "\n",
    "\n"
   ],
   "id": "386b2e08a55ec10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sf.available_subtypes()",
   "id": "77cd041427ee2bf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    " \n",
    "sorted_df = cv_results_df.sort_values(by=['rank_test_accuracy'])\n",
    "\n",
    "for ind, row in sorted_df.iterrows():\n",
    "    print(f'Rank: {row[\"rank_test_accuracy\"]}')\n",
    "    print(f'Params: {row[\"params\"]}')\n",
    "    print(f'Test accuracy: {row[\"mean_test_accuracy\"]:.3f}', end=\" / \")\n",
    "    print(f'F1 Weighted: {row[\"mean_test_f1_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Recall Weighted: {row[\"mean_test_recall_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Precision Weighted: {row[\"mean_test_precision_weighted\"]:.3f}', end=\"\\n\\n\")"
   ],
   "id": "7abd3d77de634e98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_test_set = list(map(transform, test_set))\n",
    "print(final_test_set[0].shape)\n",
    "prediction = model.predict(final_test_set)\n",
    "np_prediction = np.array(prediction)\n",
    "accuracy = accuracy_score(labels_test_set, np_prediction)\n",
    "print(f'Final Accuracy: {accuracy:.3f}')\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(labels_test_set, np_prediction, average='weighted')\n",
    "recall = recall_score(labels_test_set, np_prediction, average='weighted')\n",
    "f1 = f1_score(labels_test_set, np_prediction, average='weighted')\n",
    "\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')"
   ],
   "id": "6a6a1addf467f062",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7ef7b7b8c02a010e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "df452b6574d7ca53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_set, test_set, labels_train_set, labels_test_set",
   "id": "cdb0b9b7fca4bd2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import wave\n",
    "\n",
    "output_wav_file = 'new_output_19-01-24.wav'\n",
    "\n",
    "while True:\n",
    "    word = input(\"\\nIntroduce la palabra:\")\n",
    "    if word == 'exit':\n",
    "        break\n",
    "    word = word.upper()\n",
    "    curr_word, curr_raw_word, curr_labels = [], [], []\n",
    "\n",
    "    for letter in word:\n",
    "        letter_index = curr_keys.index(letter)\n",
    "        # Convert numpy array to list\n",
    "        labels_test_set_list = labels_test_set.tolist()\n",
    "        \n",
    "        # Get the index of the first occurrence\n",
    "        try:\n",
    "            final_index = labels_test_set_list.index(letter_index)\n",
    "            curr_word.append(transform(test_set[final_index]))\n",
    "            curr_raw_word.append(test_set[final_index])\n",
    "            random_array = np.random.uniform(1e-10, 1e-09, 300)\n",
    "            curr_raw_word.append(random_array)\n",
    "        except ValueError:\n",
    "            print(f'letter {letter} not found')\n",
    "            final_index = None  # Handle the case when the value is not found\n",
    "    \n",
    "    final_audio = np.concatenate(curr_raw_word, axis=0)\n",
    "    print(np.mean(final_audio))\n",
    "    \n",
    "    with wave.open(output_wav_file, 'w') as wav_file:\n",
    "        n_channels = 2\n",
    "        sampwidth = 4\n",
    "        sample_rate = 44100\n",
    "        \n",
    "        wav_file.setnchannels(2)\n",
    "        wav_file.setsampwidth(4)\n",
    "        wav_file.setframerate(sample_rate)\n",
    "        \n",
    "        wav_file.writeframes(final_audio)\n",
    "        # for letter in curr_word:\n",
    "        #     # wav_file.writeframesraw(letter.numpy())\n",
    "        #     ww\n",
    "            # print(f'curr_letter length: {len(letter)} / num of frames of file: {wav_file.getnframes()}')"
   ],
   "id": "d0d3579db4fa2ab3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "e",
   "id": "57deb2613ace7a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "print(sorted(labels_test_set))\n",
    "while True:\n",
    "    word = input(\"\\nIntroduce la palabra:\")\n",
    "    if word == 'exit':\n",
    "        break\n",
    "    word = word.upper()\n",
    "    curr_word, curr_labels = [], []\n",
    "\n",
    "    for letter in word:\n",
    "        letter_index = curr_keys.index(letter)\n",
    "        # Convert numpy array to list\n",
    "        labels_test_set_list = labels_test_set.tolist()\n",
    "        \n",
    "        # Get the index of the first occurrence\n",
    "        try:\n",
    "            final_index = labels_test_set_list.index(letter_index)\n",
    "        except ValueError:\n",
    "            print(f'letter {letter} not found')\n",
    "            final_index = None  # Handle the case when the value is not found\n",
    "        # append to curr_word the value in that index of X_test\n",
    "        curr_word.append(transform(test_set[final_index]))\n",
    "        curr_labels.append(labels_test_set[final_index])\n",
    "    print(\"curr_word[0].shape\")\n",
    "    print(curr_word[0].shape)\n",
    "    model.eval()\n",
    "    prediction = model.predict(curr_word)\n",
    "    prediction_list = list(map(getIndCurrKeys, prediction)) \n",
    "    print(f'prediction: {prediction_list}')\n",
    "    print(f'real labels: {list(map(getIndCurrKeys, curr_labels))}')\n",
    "    \n",
    "    response = ollama.chat(model='spanishSpellchecker', messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': ''.join(prediction_list)\n",
    "      },\n",
    "    ])\n",
    "    print(response['message']['content'])\n",
    "    # time.sleep(3)"
   ],
   "id": "9557487afa5294ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "type(dataset)\n",
    "import ollama\n",
    "\n",
    "samples, sr = librosa.load('harmony.wav')\n",
    "samples = nr.reduce_noise(samples, sr=44100)\n",
    "peaks_count = count_peaks(samples, key_length, True)\n",
    "N_FFT, HOP_LENGTH, BEFORE, AFTER, prom = 1024, 225, 2400, 12000, 0.2391\n",
    "strokes = isolator(samples, sr, N_FFT, HOP_LENGTH, BEFORE, AFTER, prom, False)[1]\n",
    "num_keys = len(strokes)\n",
    "count = 0\n",
    "k = prom\n",
    "prev_k = prom\n",
    "curr_step = 0.01\n",
    "print(f'num_keys: {num_keys} // peaks_count: {peaks_count} // prom: {prom}')\n",
    "while num_keys != peaks_count:\n",
    "    if num_keys > peaks_count:\n",
    "        if count > 0 and prev_k == k + curr_step:\n",
    "            curr_step /= 2\n",
    "        elif count > 0:\n",
    "            curr_step += (curr_step / 2)\n",
    "        prev_k = k\n",
    "        k += curr_step\n",
    "    else:\n",
    "        if count > 0 and prev_k == k - curr_step:\n",
    "            curr_step /= 2\n",
    "        elif count > 0:\n",
    "            curr_step += (curr_step / 2)\n",
    "        prev_k = k\n",
    "        k += -curr_step\n",
    "    strokes = isolator(samples, sr, N_FFT, HOP_LENGTH, BEFORE, AFTER, k, True)[1]\n",
    "    num_keys = len(strokes)\n",
    "    # print(f'actual k: {k:.7f} // num strokes: {num_keys}')\n",
    "    # time.sleep(1)\n",
    "    count += 1\n",
    "\n",
    "model.eval()\n",
    "final_word = []\n",
    "\n",
    "for i in range(len(strokes)):\n",
    "    transformed_sample = transform(strokes[i])\n",
    "    final_word.append(transformed_sample)\n",
    "\n",
    "print(len(final_word))\n",
    "prediction = model.predict(final_word)\n",
    "prediction_list = list(map(getIndCurrKeys, prediction)) \n",
    "print(f'prediction: {prediction_list}')\n",
    "\n",
    "response = ollama.chat(model='spellchecker', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': ''.join(prediction_list)\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ],
   "id": "11882f601dfa1178",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "N_SPLITS = 5\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True)\n",
    "fold_results = []\n",
    "accuracies, recalls, precisions, f1_scores = [], [], [], []\n",
    "\n",
    "dataset = audio_samples_new\n",
    "coatnet = True\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)): \n",
    "    print(f'Fold {fold+1}/{N_SPLITS}')\n",
    "    train_set = Subset(dataset, train_idx)\n",
    "    labels_train_set = Subset(labels, train_idx)\n",
    "    test_set = Subset(dataset, val_idx)\n",
    "    labels_test_set = Subset(labels, val_idx)\n",
    "    final_train_set = []\n",
    "    if coatnet:\n",
    "        model = CoAtNet(keys=curr_keys)\n",
    "        # # for i in range(len(train_set)):\n",
    "        #     transformed_sample = transform(train_set[i])\n",
    "        #     final_train_set.append((transformed_sample, labels_train_set[i]))\n",
    "        #     final_train_set.append((masking(transformed_sample), labels_train_set[i]))\n",
    "        #     X_train = [t[0] for t in final_train_set]\n",
    "        #     y_train = [t[1] for t in final_train_set]\n",
    "        for i in range(len(train_set)):\n",
    "            transformed_sample = transform(dataset[i])\n",
    "            final_train_set.append((transformed_sample, labels_train_set[i]))\n",
    "            final_train_set.append((masking(transformed_sample), labels_train_set[i]))\n",
    "        X_train = [t[0] for t in final_train_set]\n",
    "        y_train = [t[1] for t in final_train_set]\n",
    "        print(len(final_train_set))\n",
    "    else:\n",
    "        model = MfccLSTM()\n",
    "        for i in range(len(train_set)):\n",
    "            transformed_mfcc = transform_mfcc(train_set[i])\n",
    "            transformed_sample = transform(train_set[i])\n",
    "            final_train_set.append((transformed_sample, transformed_mfcc, labels_train_set[i]))\n",
    "            final_train_set.append((masking(transformed_sample), transformed_mfcc, labels_train_set[i]))\n",
    "            X_train = [(t[0],t[1]) for t in final_train_set]\n",
    "            y_train = [t[2] for t in final_train_set]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    final_test_set = []\n",
    "    test_set = Subset(dataset, val_idx)\n",
    "    labels_test_set = Subset(labels, val_idx)\n",
    "    for i in range(len(test_set)):\n",
    "        transformed_sample = transform(test_set[i])\n",
    "        final_test_set.append((transformed_sample, labels_test_set[i]))\n",
    "    X_test = [t[0] for t in final_test_set]\n",
    "    y_test = [t[1] for t in final_test_set]\n",
    "    \n",
    "    word = 'abnormalization'\n",
    "    word = word.upper()\n",
    "    curr_word = []\n",
    "    curr_labels = []\n",
    "    for letter in word:\n",
    "        letter_index = curr_keys.index(letter)\n",
    "        # find first index that is equal to letter index in y_test\n",
    "        final_index = y_test.index(letter_index)\n",
    "        # append to curr_word the value in that index of X_test\n",
    "        curr_word.append(X_test[final_index])\n",
    "        curr_labels.append(y_test[final_index])\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    prediction = model.predict(curr_word)\n",
    "    print(f'prediction: {list(map(getIndCurrKeys, prediction))}')\n",
    "    print(f'real labels: {list(map(getIndCurrKeys, curr_labels))}')\n",
    "    \n",
    "    if coatnet:\n",
    "        final_test_set = []\n",
    "        for i in range(len(test_set)):\n",
    "            transformed_sample = transform(test_set[i])\n",
    "            final_test_set.append((transformed_sample, labels_test_set[i]))\n",
    "        X_test = [t[0] for t in final_test_set]\n",
    "        y_test = [t[1] for t in final_test_set]\n",
    "        prediction = model.predict(X_test)\n",
    "    else: \n",
    "        final_test_set = []\n",
    "        for i in range(len(test_set)):\n",
    "            transformed_mfcc = transform_mfcc(test_set[i])\n",
    "            transformed_sample = transform(test_set[i])\n",
    "            final_test_set.append((transformed_sample, transformed_mfcc, labels_test_set[i]))\n",
    "        X_test = [(t[0],t[1]) for t in final_test_set]\n",
    "        y_test = [t[2] for t in final_test_set]\n",
    "        prediction = model.predict(X_test)\n",
    "    \n",
    "    print(f'prediction: {prediction[:20]}')\n",
    "    print(f'true labels: {y_test[:20]}')\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, prediction)\n",
    "    precision = precision_score(y_test, prediction, average='macro')  # For binary classification\n",
    "    recall = recall_score(y_test, prediction, average='macro')\n",
    "    f1 = f1_score(y_test, prediction, average='macro')\n",
    "    \n",
    "    print(f'Fold {fold+1} FINAL RESULTS: ')\n",
    "    print(f'accuracy: {accuracy:.3f} // precision: {precision:.3f} // recall: {recall:.3f} // f1: {f1:.3f}\\n')\n",
    "    accuracies.append(accuracy)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "print(f'\\nAverage accuracy: {np.mean(accuracies):.3f}')\n",
    "print(f'Average precision: {np.mean(precisions):.3f}')\n",
    "print(f'Average recall: {np.mean(recalls):.3f}')\n",
    "print(f'Average f1: {np.mean(f1_scores):.3f}')"
   ],
   "id": "461576e21a0e7175",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\", add_to_git_credential=True)"
   ],
   "id": "482a15f658086fce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = \"0.0\"\n",
    "\n",
    "# Access the environment variable\n",
    "# print(os.environ.get('MY_VARIABLE'))  # Output: my_value"
   ],
   "id": "d6fc51f9ffcfd378",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='spellchecker', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Singfng in rhe raun'\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ],
   "id": "a3e60c1fea63918a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.mps.set_per_process_memory_fraction(0.0)",
   "id": "e5591ae605bef10b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"most likely english word from: aaying\"},\n",
    "]\n",
    "device=\"mps\"\n",
    "llama_32=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-3B-Instruct\", device=device)\n",
    "# pipe(messages, max_new_tokens=256)\n",
    "\n",
    "generator = pipeline(model=llama_32, device_map=torch.device('mps'), torch_dtype=torch.float16)\n",
    "generation = generator(\n",
    "    messages,\n",
    "    # do_sample=False,\n",
    "    # temperature=1.0,\n",
    "    # top_p=1,\n",
    "    max_new_tokens=256\n",
    ")\n",
    "\n",
    "print(f\"Generation: {generation[0]['generated_text']}\")"
   ],
   "id": "4361fce179b7b34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "# Set up device for MPS if available\n",
    "device = \"cpu\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\" # replace with the actual model path if necessary\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move model to MPS device\n",
    "model.to(device)\n",
    "\n",
    "# Example input\n",
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids=input_ids, max_length=50)\n",
    "    \n",
    "# Decode and print the output\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ],
   "id": "d9c86036a13240c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ],
   "id": "dc02416ad5cf61c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n",
    "\n",
    "# Check for MPS availability and use if possible\n",
    "device = 0 if torch.backends.mps.is_available() else -1\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-3B\", device=device)\n",
    "model_id = \"meta-llama/Llama-3.2-3B\""
   ],
   "id": "806e3166728cdf63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    device=0\n",
    ")\n",
    "\n",
    "# pipe(\"The key to life is\")\n"
   ],
   "id": "5e1f878e0a42072c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B\", torch_dtype=torch.float16)"
   ],
   "id": "3941f9bf6f7687b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9320db9e593fde4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define your input sequence\n",
    "input_sequence = \"peter\"\n",
    "inputs = tokenizer(input_sequence, return_tensors=\"pt\").to(\"mps\")\n",
    "# print(inputs)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "# Forward pass to get logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, min_length=10, max_length=50, pad_token_id = tokenizer.eos_token_id)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get the logits of the last token in the sequence\n",
    "last_token_logits = logits[0, -1, :]\n",
    "\n",
    "# Set the number of top words you want to retrieve\n",
    "top_k = 15\n",
    "probs = F.softmax(last_token_logits, dim=-1)\n",
    "top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
    "# print(probs)\n",
    "\n",
    "\n",
    "# # Decode the top-k token indices to words\n",
    "predicted_words = [tokenizer.decode([idx]).strip() for idx in top_k_indices]\n",
    "print(predicted_words)\n",
    "\n",
    "# Output the list of most likely words\n",
    "print(\"Top likely words:\", predicted_words)"
   ],
   "id": "51f7f36ebc5e9765",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define your input sequence\n",
    "input_sequence = \"The word starting with 'cat' is: \"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(input_sequence, return_tensors=\"pt\").to(\"mps\")\n",
    "print(inputs)\n",
    "\n",
    "# Generate multiple likely continuations\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=20,                 # Adjust max_length to fit expected word length\n",
    "    num_return_sequences=5,        # Number of likely completions\n",
    "    do_sample=True,                # Enables non-deterministic sampling\n",
    "    top_k=10,                       # Limits to top 10 likely completions\n",
    "    pad_token_id=tokenizer.eos_token_id  # Set pad_token_id to eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and collect unique words\n",
    "predicted_words = set()\n",
    "for o in output:\n",
    "    decoded_text = tokenizer.decode(o, skip_special_tokens=True).strip()\n",
    "    if len(decoded_text) > len(input_sequence):  # Ensure there's content after the prompt\n",
    "        try:\n",
    "            word = decoded_text[len(input_sequence):].split()[0]  # Get the first word after the prompt\n",
    "            predicted_words.add(word)\n",
    "        except IndexError:\n",
    "            continue  # Skip if there's an error\n",
    "\n",
    "# Decode and collect unique words\n",
    "# Decode and collect unique words\n",
    "predicted_words = list(set(\n",
    "    tokenizer.decode(o, skip_special_tokens=True).strip()[len(input_sequence):].split()[0] \n",
    "    for o in output if len(tokenizer.decode(o, skip_special_tokens=True).strip()) > len(input_sequence)\n",
    "))\n",
    "\n",
    "print(\"Top likely words:\", predicted_words)"
   ],
   "id": "5ffcfc5f0114fd9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "dataset = audioDatasetMfcc + audioDatasetMfccMasking\n",
    "X = [(t[0],t[1]) for t in audioDatasetMfcc]\n",
    "X_masking = [(t[0],t[1]) for t in audioDatasetMfccMasking]\n",
    "y = [t[2] for t in audioDatasetMfcc]\n",
    "y_masking = [t[2] for t in audioDatasetMfccMasking]\n",
    "\n",
    "model = MfccLSTM()\n",
    "\n",
    "param_grid = {\n",
    "    'patience': [120],\n",
    "    'batch_size': [32],\n",
    "}\n",
    "\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted', zero_division=0.0),\n",
    "    'precision_weighted': make_scorer(precision_score, average='weighted', zero_division=0.0),\n",
    "    'recall_weighted': make_scorer(recall_score, average='weighted', zero_division=0.0),\n",
    "}\n",
    "# \n",
    "grid_search = GridSearchCV(MfccLSTM(), param_grid, cv=5, scoring=scoring, refit=False, verbose=3)\n",
    "# # model = CoAtNet(patience=1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.01)\n",
    "grid_search.fit(X + X_masking, y + y_masking)\n",
    "# print(len(X))\n",
    "# model.fit(X+X_masking, y+y_masking)"
   ],
   "id": "d42799b2389b04c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "sorted_df = cv_results_df.sort_values(by=['rank_test_accuracy'])\n",
    "\n",
    "for ind, row in sorted_df.iterrows():\n",
    "    print(f'Rank: {row[\"rank_test_accuracy\"]}')\n",
    "    print(f'Params: {row[\"params\"]}')\n",
    "    print(f'Test accuracy: {row[\"mean_test_accuracy\"]:.3f}', end=\" / \")\n",
    "    print(f'F1 Weighted: {row[\"mean_test_f1_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Recall Weighted: {row[\"mean_test_recall_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Precision Weighted: {row[\"mean_test_precision_weighted\"]:.3f}', end=\"\\n\\n\")\n",
    "\n",
    "grid_search.cv_results_"
   ],
   "id": "45986cc16a06bddd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a52a2f4cde1eda15",
   "metadata": {},
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "dataset = audioDatasetFin + audioDatasetFinMasking\n",
    "X = [t[0] for t in audioDatasetFin]\n",
    "X_masking = [t[0] for t in audioDatasetFinMasking]\n",
    "y = [t[1] for t in audioDatasetFin]\n",
    "y_masking = [t[1] for t in audioDatasetFinMasking]\n",
    "print(np.array(X).shape)\n",
    "# first_el = dataset[0][0]\n",
    "print(np.array(y+y_masking).shape)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "param_grid = {\n",
    "    'num_epochs': [500],\n",
    "    # 'patience': [55, 75, 100],\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted', zero_division=0.0),\n",
    "    'precision_weighted': make_scorer(precision_score, average='weighted', zero_division=0.0),\n",
    "    'recall_weighted': make_scorer(recall_score, average='weighted', zero_division=0.0),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(CoAtNet(), param_grid, cv=5, scoring=scoring, refit=False, verbose=3)\n",
    "# model = CoAtNet(patience=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.01)\n",
    "grid_search.fit(X_train + X_masking, y_train + y_masking)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# grid_search.fit(X, y)\n",
    "# print(np.array(X_train).shape)\n",
    "# print(np.array(y_train).shape)\n",
    "# print(np.array(dataset).shape)\n",
    "# print(np.concatenate((X_train, y_train), axis=3).shape)\n",
    "# model.fit(X_train+X_masking, y_train+y_masking)\n",
    "# print(f'Prediction: {model.predict(np.array(X_test)).shape}')\n",
    "# final_labels_set = [original_set[ind] for ind in y_test]\n",
    "# print(f'Labels: {np.array(final_labels_set).shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f33b0326405e8528",
   "metadata": {},
   "source": [
    "cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "sorted_df = cv_results_df.sort_values(by=['rank_test_accuracy'])\n",
    "\n",
    "for ind, row in sorted_df.iterrows():\n",
    "    print(f'Rank: {row[\"rank_test_accuracy\"]}')\n",
    "    print(f'Params: {row[\"params\"]}')\n",
    "    print(f'Test accuracy: {row[\"mean_test_accuracy\"]:.3f}', end=\" / \")\n",
    "    print(f'F1 Weighted: {row[\"mean_test_f1_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Recall Weighted: {row[\"mean_test_recall_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Precision Weighted: {row[\"mean_test_precision_weighted\"]:.3f}', end=\"\\n\\n\")\n",
    "\n",
    "grid_search.cv_results_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ced4875a148ec525",
   "metadata": {},
   "source": [
    "# cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "# cv_results_df = cv_results_df[[\"params\", \"mean_test_accuracy\", \"rank_test_accuracy\", \"mean_test_f1_weighted\", \"rank_test_f1_weighted\", \"mean_test_precision_weighted\", \"rank_test_precision_weighted\", \n",
    "#  \"mean_test_recall_weighted\", \"rank_test_recall_weighted\"]]\n",
    "# cv_results_df\n",
    "\n",
    "# param_grid = {\n",
    "#     'num_epochs': [500, 700, 1100],\n",
    "#     'patience': [55, 75, 100],\n",
    "# }\n",
    "# \n",
    "# scoring = {\n",
    "#     'accuracy': make_scorer(accuracy_score),\n",
    "#     'f1_weighted': make_scorer(f1_score, average='weighted', zero_division=0.0),\n",
    "#     'precision_weighted': make_scorer(precision_score, average='weighted', zero_division=0.0),\n",
    "#     'recall_weighted': make_scorer(recall_score, average='weighted', zero_division=0.0),\n",
    "# }\n",
    "# \n",
    "# grid_search = GridSearchCV(model, param_grid, cv=5, scoring=scoring, refit=False, verbose=3)\n",
    "# grid_search.fit(X, y)\n",
    "# param_grid = {\n",
    "#     'num_epochs': [500, 700, 1100],\n",
    "#     # 'patience': [10, 15, 20, 30, 50, 100],\n",
    "# }\n",
    "# grid_search = GridSearchCV(MfccLSTM(), param_grid, cv=5, scoring=['accuracy', 'f1', 'precision', 'recall'], refit=False, verbose)\n",
    "# \n",
    "# dataset = audioDatasetMfcc + audioDatasetMfccMasking\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)\n",
    "# # print(X_train.shape)\n",
    "# grid_search.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7c9baaa8831abc71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a6ad6bc1bfba3d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "2204dec2710b9ee8",
   "metadata": {},
   "source": [
    "sorted_df = cv_results_df.sort_values(by=['rank_test_accuracy'])\n",
    "\n",
    "for ind, row in sorted_df.iterrows():\n",
    "    print(f'Rank: {row[\"rank_test_accuracy\"]}')\n",
    "    print(f'Params: {row[\"params\"]}')\n",
    "    print(f'Test accuracy: {row[\"mean_test_accuracy\"]:.3f}', end=\" / \")\n",
    "    print(f'F1 Weighted: {row[\"mean_test_f1_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Recall Weighted: {row[\"mean_test_recall_weighted\"]:.3f}', end=\" / \")\n",
    "    print(f'Precision Weighted: {row[\"mean_test_precision_weighted\"]:.3f}', end=\"\\n\\n\")\n",
    "\n",
    "# best_params_list = grid_search.cv_results_['rank_test_accuracy']\n",
    "# print(grid_search.cv_results_)\n",
    "# print(best_params_list)\n",
    "# print(best_params_list)\n",
    "# print(best_params_list)\n",
    "# print(grid_search.cv_results_[\"mean_test_accuracy\"])\n",
    "# \n",
    "# for i in best_params_list:\n",
    "#     print(f'Parameter {i-1}: {grid_search.cv_results_[\"params\"][i-1]}')\n",
    "#     print(f'Mean Test accuracy: {grid_search.cv_results_[\"mean_test_accuracy\"][i-1]:.3f}')\n",
    "#     print(f'Std Test accuracy: {grid_search.cv_results_[\"std_test_accuracy\"][i-1]:.3f}')\n",
    "#     # print(f'Mean F1 weighted: {grid_search.cv_results_[\"mean_test_f1_weighted\"][i-1]:.3f}')\n",
    "#     # print(f'Mean Recall weighted: {grid_search.cv_results_[\"mean_test_recall_weighted\"][i-1]:.3f}')\n",
    "#     # print(f'Mean Precision weighted: {grid_search.cv_results_[\"mean_test_precision_weighted\"][i-1]:.3f}')\n",
    "#     print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "da929a94a783f0c0",
   "metadata": {},
   "source": [
    "dataset = audioDatasetFin + audioDatasetFinMasking\n",
    "# X = np.array(dataset)[:, :1]\n",
    "# y = np.array(dataset)[:, 2]\n",
    "# print(X.shape)\n",
    "# print(dataset[0])\n",
    "X = [t[0] for t in dataset]\n",
    "y = [t[1] for t in dataset]\n",
    "\n",
    "param_grid = {\n",
    "    'num_epochs': [500, 700, 1100],\n",
    "    'patience': [10, 15, 20, 30, 50, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(CoAtNet(), param_grid, cv=10, scoring=['accuracy', 'f1', 'precision', 'recall'], refit=False, verbose=3)\n",
    "\n",
    "dataset = audioDatasetMfcc + audioDatasetMfccMasking\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y),test_size=0.2)\n",
    "\n",
    "                                                    \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "grid_search.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dbb113bd8be14eda",
   "metadata": {},
   "source": [
    "param_grid = {\n",
    "    'num_epochs': [500, 700, 1100],\n",
    "    'patience': [10, 15, 20, 30, 50, 100],\n",
    "}\n",
    "grid_search = GridSearchCV(MfccLSTM(), param_grid, cv=10, scoring=['accuracy', 'f1', 'precision', 'recall'], refit=False)\n",
    "\n",
    "dataset = audioDatasetMfcc + audioDatasetMfccMasking\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)\n",
    "# print(X_train.shape)\n",
    "grid_search.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "876758bd07bc26b7",
   "metadata": {},
   "source": [
    "# Model architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=36):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.LazyLinear(512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 14 * 14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e1ab811f0d1abfe",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "def train_cnnlstm_with_cross_val(dataset, num_epochs, model_name, device, num_classes=36, patience=10, random_state=42, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        print(f'Fold {fold+1}/{n_splits}')\n",
    "        \n",
    "        # Split the dataset into training and validation sets\n",
    "        train_set = Subset(dataset, train_idx)\n",
    "        val_set = Subset(dataset, val_idx)\n",
    "        train_loader = DataLoader(train_set, batch_size=16)\n",
    "        val_loader = DataLoader(val_set, batch_size=16)\n",
    "        \n",
    "        # Initialize model, optimizer, and loss function\n",
    "        model = MfccLSTM(input_size=20, hidden_size=32, num_classes=num_classes, output_size=64)\n",
    "        model = model.to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_acc, epochs_no_imp = 0, 0\n",
    "        train_accuracies, val_accuracies = [], []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            epoch_train_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            tic = time.perf_counter()\n",
    "            \n",
    "            for images, sequences, labels in train_loader:\n",
    "                images = images.to(device)\n",
    "                sequences = sequences.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                #converting labels to Long to avoid error \"not implemented for Int\"\n",
    "                labels = labels.long()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images, sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_train_loss += loss.item() * images.size(0)\n",
    "\n",
    "                _, predicted_train = torch.max(outputs.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted_train == labels).sum().item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            toc = time.perf_counter()\n",
    "            time_taken = toc - tic\n",
    "            \n",
    "            epoch_train_loss /= len(train_loader.dataset)\n",
    "            train_accuracy = correct_train / total_train\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            # Evaluation of the model\n",
    "            model.eval()\n",
    "            total, correct = 0, 0\n",
    "            for images, sequences, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                sequences = sequences.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images, sequences)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            #\n",
    "            val_accuracy = correct / total\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}, Iter Time: {time_taken:.2f}s\")\n",
    "                \n",
    "            if val_accuracy > best_val_acc:\n",
    "                best_val_acc = val_accuracy\n",
    "                epochs_no_imp = 0\n",
    "                best_model_state = model.state_dict()  # Save the best model\n",
    "            else:\n",
    "                epochs_no_imp += 1\n",
    "            if epochs_no_imp >= patience:\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "                model.load_state_dict(best_model_state)  # Load the best model\n",
    "                break\n",
    "        \n",
    "        fold_results.append(best_val_acc) \n",
    "        print(f'Fold {fold+1} Best Validation Accuracy: {best_val_acc:.4f}')\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    print(f'Training final results: {fold_results}')\n",
    "    \n",
    "    return num_epochs, np.average(fold_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "245a5cae2ee3f120",
   "metadata": {},
   "source": [
    "import time\n",
    "from coatnet import CoAtNet as CoAtNetImp\n",
    "\n",
    "num_blocks = [2, 2, 3, 5, 2]            # L\n",
    "channels = [64, 96, 192, 384, 768]      # D\n",
    "\n",
    "def train_coatnet_with_cross_val(dataset, num_epochs, model_name, device_external, num_classes=36, patience=10, random_state=42):\n",
    "    train_set, val_set = train_test_split(dataset, test_size=0.2, random_state=random_state)\n",
    "    train_loader, val_loader = DataLoader(train_set, batch_size=16), DataLoader(val_set, batch_size=16)\n",
    "    \n",
    "    # Initialize model, optimizer, and loss function\n",
    "    model = CoAtNetImp((64, 64), 1, num_blocks, channels, num_classes=num_classes)\n",
    "    device = torch.device(device_external) #default to mps\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_acc, epochs_no_imp = 0, 0\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        tic = time.perf_counter()\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            labels = labels.long() # converting labels to Long to avoid error \"not implemented for Int\"\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "            _, predicted_train = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted_train == labels).sum().item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        toc = time.perf_counter()\n",
    "        time_taken = toc - tic\n",
    "        \n",
    "        epoch_train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Evaluation of the model\n",
    "        model.eval()\n",
    "        total, correct = 0, 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}, Iter Time: {time_taken:.2f}s\")\n",
    "            \n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            epochs_no_imp = 0\n",
    "            best_model_state = model.state_dict()  # Save the best model\n",
    "        else:\n",
    "            epochs_no_imp += 1\n",
    "        if epochs_no_imp >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs')\n",
    "            model.load_state_dict(best_model_state)  # Load the best model\n",
    "            break\n",
    "            \n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    return epoch+1, best_val_acc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e58aef22cf15f974",
   "metadata": {},
   "source": [
    "def predict_mfcc(dataset, model_path, device_external):\n",
    "    images_test_set = [t[0] for t in dataset]\n",
    "    sequences_test_set = [t[1] for t in dataset]\n",
    "    \n",
    "    images = torch.stack(images_test_set)\n",
    "    sequences = torch.stack(sequences_test_set)\n",
    "    device = torch.device(device_external) #default to mps\n",
    "    images = images.to(device)\n",
    "    sequences = sequences.to(device)\n",
    "    model = MfccLSTM(input_size=20, hidden_size=32, num_classes=36, output_size=64)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path,map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images, sequences)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    pred = []\n",
    "    keyss = '1234567890QWERTYUIOPASDFGHJKLZXCVBNM+-'\n",
    "    phrase = predicted.tolist()\n",
    "    for i in range(len(phrase)):\n",
    "        pred.append(keyss[phrase[i]])\n",
    "\n",
    "    pred_df = pd.DataFrame(pred)\n",
    "    return pred_df\n",
    "\n",
    "\n",
    "def predict(dataset, model_obj, argnames, model_path, device_external):\n",
    "    fin_dict = {}\n",
    "\n",
    "    # create the list with each of the ith range tuples\n",
    "    for i in range(len(dataset[0])-1):\n",
    "        fin_dict[argnames[i]] = [t[i] for t in dataset]\n",
    "        \n",
    "    # specify device: default to mps\n",
    "    device = torch.device(device_external) \n",
    "    \n",
    "    # torch.stack each one of the lists\n",
    "    for key in fin_dict.keys():\n",
    "        fin_dict[key] = torch.stack(fin_dict[key]).to(device)\n",
    "    \n",
    "    # model specifying\n",
    "    model = model_obj.to(device)\n",
    "    model.load_state_dict(torch.load(model_path,map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**fin_dict)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    pred = []\n",
    "    keyss = '1234567890QWERTYUIOPASDFGHJKLZXCVBNM+-'\n",
    "\n",
    "    phrase = predicted.tolist()\n",
    "    for i in range(len(phrase)):\n",
    "        pred.append(keyss[phrase[i]])\n",
    "\n",
    "    pred_df = pd.DataFrame(pred)\n",
    "    return pred_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dea4576ee7857b85",
   "metadata": {},
   "source": [
    "def save_csv(model_name, num_epochs, description, accuracy, precision, recall, f1_score):\n",
    "    csv_file_path = 'model_comparison.csv'\n",
    "    \n",
    "    # Read the existing CSV file into a DataFrame\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, create an empty DataFrame with the correct columns\n",
    "        df = pd.DataFrame(columns=['Datetime', 'Name', 'Epochs', 'Description', 'Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "        \n",
    "    # Data to append\n",
    "    current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Remove newline characters from the description\n",
    "    description = description.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    \n",
    "    # Create a new column with the relevant information\n",
    "    new_data = {\n",
    "        'Datetime': [current_datetime],\n",
    "        'Name': [model_name],\n",
    "        'Epochs': [num_epochs],\n",
    "        'Description': [description],\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F1': [f1_score],\n",
    "    }\n",
    "    \n",
    "    new_df = pd.DataFrame(new_data)\n",
    "    \n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(csv_file_path, index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7407a641de41b299",
   "metadata": {},
   "source": [
    "# current random state to split the dataset\n",
    "random_state = 45\n",
    "curr_day = datetime.today().strftime('%Y-%m-%d')\n",
    "curr_time = datetime.today().strftime(\"%H:%M:%S\")\n",
    "datasets = [audioDatasetFin]\n",
    "patience = 15\n",
    "torch.manual_seed(random_state)\n",
    "\n",
    "for dataset in datasets:\n",
    "    # TRAIN PART\n",
    "    train_final_set, test_set = train_test_split(dataset, test_size=0.2, random_state=random_state)\n",
    "    num_epochs = 3\n",
    "    if dataset is audioDatasetMfcc:\n",
    "        for curr_dataset in [dataset, dataset + audioDatasetMfccMasking]:\n",
    "            main_architecture = f\"CNN_LSTM{'_masking' if curr_dataset != audioDatasetMfcc else ''}\"\n",
    "            # Current train function\n",
    "            train_function = train_cnnlstm_with_cross_val\n",
    "            model = MfccLSTM(input_size=20, hidden_size=32, num_classes=len(keys_2), output_size=64)\n",
    "            params_array = [\"images\", \"sequences\"]\n",
    "            description = f\"2 layer CNN (32 and 64 output channels) with final 2 Dense Layers (512 and num_classes) result concatenated with \\n 2 LSTMs (hidden_size=32),  from mfcc with 2 Dense Layers (64 and 16) with a final Lazy Linear layer output of num_classes. Dataset using time shift and masking. {'Original dataset' if original else 'Custom dataset'}. {'Masking Used' if dataset == audioDatasetMfccMasking else 'No Masking'}. Patience={patience}. Random_state={random_state}\"\n",
    "            model_name = f\"model_multiclass_{num_epochs}_{main_architecture}_{curr_day}_{curr_time}{'_custom' if not original and not concat else '_concat' if concat else '_original'}.pth\"\n",
    "            print(f'CURRENT MODEL: {model_name}:')\n",
    "            real_num_epochs, best_val_acc = train_function(curr_dataset, num_epochs, model_name, device, num_classes=len(complete_set if not original or concat else original_set), patience=patience, random_state=random_state)\n",
    "            # PREDICT PART\n",
    "            prediction = predict(test_set, model, params_array, model_name, device)\n",
    "            labels_set = [t[-1] for t in test_set]\n",
    "            final_labels_set = [complete_set[ind] for ind in labels_set]\n",
    "                \n",
    "            # Metrics\n",
    "            accuracy = accuracy_score(final_labels_set, prediction[0])\n",
    "            precision = precision_score(final_labels_set, prediction[0], average='macro')\n",
    "            recall = recall_score(final_labels_set, prediction[0], average='macro')\n",
    "            f1 = sklearn.metrics.f1_score(final_labels_set, prediction[0], average='macro')\n",
    "            \n",
    "            # Save csv data for later comparison\n",
    "            # save_csv(model_name, int(real_num_epochs), description, accuracy, precision, recall, f1)\n",
    "            \n",
    "            # Final results\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(description)\n",
    "            print(f\"Epochs: {num_epochs}\")\n",
    "            print(f\"Accuracy: {accuracy:.3f}\")\n",
    "            print(f\"Precision: {precision:.3f}\")\n",
    "            print(f\"Recall: {recall:.3f}\")\n",
    "            print(f\"F1 Score: {f1:.3f}\")\n",
    "            print(f\"Best val accuracy: {best_val_acc:.3f}\")\n",
    "    elif dataset is audioDatasetFin:\n",
    "        for curr_dataset in [dataset, dataset + audioDatasetFinMasking]:\n",
    "            main_architecture = f\"CoAtNetImp{'_masking' if curr_dataset != audioDatasetFin else ''}\"\n",
    "            # Current train function\n",
    "            train_function = train_coatnet_with_cross_val\n",
    "            model = CoAtNetImp((64, 64), 1, num_blocks, channels, num_classes=len(original_set))\n",
    "            params_array = [\"x\"]\n",
    "            description = f\"Imported CoAtNet model, with 2 Conv layers and then 2 Attention layers followed by a fully connected layer. {'Original dataset' if original else 'Custom dataset'}. {'Masking Used' if dataset == audioDatasetFinMasking else 'No masking'}. Patience={patience}. Random_state={random_state}\"\n",
    "            model_name = f\"model_multiclass_{num_epochs}_{main_architecture}_{curr_day}_{curr_time}{'_custom' if not original and not concat else '_concat' if concat else '_original'}.pth\"\n",
    "            print(f'CURRENT MODEL: {model_name}:')\n",
    "            real_num_epochs, best_val_acc = train_function(curr_dataset, num_epochs, model_name, device, num_classes=len(complete_set if not original or concat else original_set), patience=patience, random_state=random_state)\n",
    "            # PREDICT PART\n",
    "            prediction = np.squeeze(predict(test_set, model, params_array, model_name, device).to_numpy().T)\n",
    "            print(f'Prediction.shape: {prediction.shape}')\n",
    "            labels_set = [t[-1] for t in test_set]\n",
    "            final_labels_set = [complete_set[ind] for ind in labels_set]\n",
    "            print(f'final_labels_set.shape: {np.array(final_labels_set).shape}')\n",
    "                \n",
    "            # # Metrics\n",
    "            # accuracy = accuracy_score(final_labels_set, prediction[0])\n",
    "            # precision = precision_score(final_labels_set, prediction[0], average='macro')\n",
    "            # recall = recall_score(final_labels_set, prediction[0], average='macro')\n",
    "            # f1 = sklearn.metrics.f1_score(final_labels_set, prediction[0], average='macro')\n",
    "            \n",
    "            # Save csv data for later comparison\n",
    "            # save_csv(model_name, int(real_num_epochs), description, accuracy, precision, recall, f1)\n",
    "            \n",
    "            # Final results\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(description)\n",
    "            print(f\"Epochs: {num_epochs}\")\n",
    "            print(f\"Accuracy: {accuracy:.3f}\")\n",
    "            print(f\"Precision: {precision:.3f}\")\n",
    "            print(f\"Recall: {recall:.3f}\")\n",
    "            print(f\"F1 Score: {f1:.3f}\")\n",
    "            print(f\"Best val accuracy: {best_val_acc:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca3565e42e7f87ad",
   "metadata": {},
   "source": [
    "\n",
    "grid_search = GridSearchCV(CoAtNetImp, {})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd43561c586fd9b1",
   "metadata": {},
   "source": [
    "train_final_set, test_set = train_test_split(audioDatasetMfcc, test_size=0.2, random_state=1)\n",
    "\n",
    "model_name = \"model_multiclass_500_CNN_LSTM_masking_2024-09-08.pth\"\n",
    "# model_name = \"model_multiclass_500_CoAtNetImp_2024-09-08.pth\"\n",
    "# model = CoAtNetImp((64, 64), 1, num_blocks, channels, num_classes=len(complete_set))\n",
    "model = MfccLSTM(input_size=20, hidden_size=32, num_classes=len(complete_set), output_size=64)\n",
    "# arg_names = [\"x\"]\n",
    "arg_names = [\"images\", \"sequences\"]\n",
    "prediction = predict(test_set, model, arg_names, model_name, device)\n",
    "labels_set = [t[-1] for t in test_set]\n",
    "final_labels_set = pd.Series([complete_set[ind] for ind in labels_set])\n",
    "print(\"PREDICTION\")\n",
    "print(prediction[0])\n",
    "print(\"FINAL LABELS SET\")\n",
    "print(final_labels_set)\n",
    "# Metrics\n",
    "accuracy = accuracy_score(final_labels_set, prediction[0])\n",
    "precision = precision_score(final_labels_set, prediction[0], average='macro')\n",
    "recall = recall_score(final_labels_set, prediction[0], average='macro')\n",
    "f1 = sklearn.metrics.f1_score(final_labels_set, prediction[0], average='macro')\n",
    "\n",
    "# Save csv data for later comparison\n",
    "# save_csv(model_name, int(real_num_epochs), description, accuracy, precision, recall, f1)\n",
    "\n",
    "# Final results\n",
    "print(f\"Model: {model_name}\")\n",
    "# print(description)\n",
    "# print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n",
    "# print(f\"Best val accuracy: {best_val_acc:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e992282d8cbead9",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for name in [\"model_multiclass_500_CNN_LSTM_2024-09-08.pth\", \"model_multiclass_500_CNN_LSTM_masking_2024-09-08.pth\"]:\n",
    "        \n",
    "    train_final_set, test_set = train_test_split(audioDatasetMfcc, test_size=0.2, random_state=5)\n",
    "    \n",
    "    # model_name = \"model_multiclass_500_CoAtNetImp_masking_2024-09-08.pth\"\n",
    "    model_name = \"model_multiclass_500_CNN_LSTM_2024-09-08.pth\"\n",
    "    # model = CoAtNetImp((64, 64), 1, num_blocks, channels, num_classes=len(keys_2))\n",
    "    model = MfccLSTM(input_size=20, hidden_size=32, num_classes=len(keys_2), output_size=64)\n",
    "    # arg_names = [\"x\"]\n",
    "    arg_names = [\"images\", \"sequences\"]\n",
    "    prediction = predict(test_set, model, arg_names, name, device)\n",
    "    labels_set = [t[-1] for t in test_set]\n",
    "    final_labels_set = [keys_2[ind] for ind in labels_set]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(final_labels_set, prediction[0])\n",
    "    precision = precision_score(final_labels_set, prediction[0], average='macro')\n",
    "    recall = recall_score(final_labels_set, prediction[0], average='macro')\n",
    "    f1 = sklearn.metrics.f1_score(final_labels_set, prediction[0], average='macro')\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(final_labels_set, prediction[0])\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 8})\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "680fb321",
   "metadata": {},
   "source": [
    "# PREDICTION PART\n",
    "\n",
    "# All metrics are calculated from the model with the best validation accuracy\n",
    "# model = MfccLSTM(input_size=20, hidden_size=32, num_classes=21, output_size=64)\n",
    "model = CoAtNetImp((64, 64), 1, num_blocks, channels, num_classes=21)\n",
    "keys_s = '123456789-ABCDEFGHIJ+'\n",
    "\n",
    "# prediction = predict(test_set, model, [\"images\", \"sequences\"],model_name, device)\n",
    "prediction = predict(test_set, model, [\"x\"],model_name, device)\n",
    "# prediction = predict(test_set, model, main_architecture, model_name, random_state)\n",
    "# prediction = predict_mfcc(test_set, model_name, device)\n",
    "labels_set = [t[-1] for t in test_set]\n",
    "final_labels_set = [keys_s[ind] for ind in labels_set]\n",
    "print(list(prediction[0])[15:25])\n",
    "print(final_labels_set[15:25])\n",
    "\n",
    "# Metrics calculation\n",
    "accuracy = accuracy_score(final_labels_set, prediction[0])\n",
    "precision = precision_score(final_labels_set, prediction[0], average='macro')\n",
    "recall = recall_score(final_labels_set, prediction[0], average='macro')\n",
    "f1 = sklearn.metrics.f1_score(final_labels_set, prediction[0], average='macro')\n",
    "\n",
    "# Save in csv file\n",
    "# description = \"2 layer CNN (32 and 64 output channels) with final 2 Dense Layers (512 and num_classes) result concatenated with \\n 2 LSTMs (hidden_size=32),  from mfcc with 2 Dense Layers (64 and 16) with a final Lazy Linear layer output of num_classes. Dataset using time shift and masking. Using now dataset of 40 audio samples \"\n",
    "description = \"Imported CoAtNet model, with 2 Conv layers and then 2 Attention layers followed by a fully connected layer. Patience=10. Testing recently recorded denoised-normalized audio, part 2 to see if it is working. did 36 epochs. 21 keys recorded\"\n",
    "save_csv(model_name, int(real_num_epochs), description, accuracy, precision, recall, f1)\n",
    "\n",
    "# Print results\n",
    "print(\"Final Results!\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(description)\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n",
    "print(f\"Best val accuracy: {best_val_acc:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3179ef25db1fbef0",
   "metadata": {},
   "source": [
    "# prediction = predict(test_set, model, [\"images\", \"sequences\"],model_name, device)\n",
    "# \n",
    "# for ind, pred in enumerate(list(prediction[0])):\n",
    "#     print(f'prediction {ind}: {pred} / label: {final_labels_set[ind]}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57c064ba206085c2",
   "metadata": {},
   "source": [
    "from coatnet import CoAtNet\n",
    "\n",
    "img = torch.rand(16, 3, 64, 64)\n",
    "\n",
    "num_blocks = [2, 2, 3, 5, 2]            # L\n",
    "channels = [64, 96, 192, 384, 768]      # D\n",
    "block_types=['C', 'C', 'T', 'T']        # 'C' for MBConv, 'T' for Transformer\n",
    "\n",
    "net = CoAtNet((64, 64), 1, num_blocks, channels, block_types=block_types)\n",
    "out = net(torch.unsqueeze(audioDatasetFin[0][0], dim=0))\n",
    "print(f'final shape: {out.shape}')\n",
    "print(audioDatasetFin[0][0].shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cda12c04ba1a39d7",
   "metadata": {},
   "source": [
    "import csv\n",
    "    \n",
    "def empty_file(csv_file_path):\n",
    "    # Read the header (first row) of the CSV file\n",
    "    with open(csv_file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Read the first row (header)\n",
    "    \n",
    "    # Write only the header back to the CSV file\n",
    "    with open(csv_file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)  # Wr`ite the header back to the file\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5aa7e7268b306a",
   "metadata": {},
   "source": [
    "# empty_file('model_comparison.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "58c8e875",
   "metadata": {},
   "source": [
    "# Using custom audio\n",
    "\n",
    "The following code adapts the previous working segment to utilize custom audio recorded by the team. Work in progress."
   ]
  },
  {
   "cell_type": "code",
   "id": "23af0b4c",
   "metadata": {},
   "source": [
    "#Using audio from custom-audio to create the test_set\n",
    "keys_t_s='0123'\n",
    "labels = list(keys_t_s)\n",
    "keys_t = [k + '.wav' for k in labels]\n",
    "\n",
    "for key in keys_t:\n",
    "    sample_t, sr_t = librosa.load(f'../Dataset-custom-audio/base-audio/{key}')\n",
    "    print(sr_t)\n",
    "    print(len(isolator(sample_t, sr_t, 1024, 225, 2400, 12000, 0.06)), end=' ')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4879ba55",
   "metadata": {},
   "source": [
    "n_fft = 50 #1024\n",
    "hop_length = 225 #225\n",
    "before = 2400 #2400\n",
    "after = 10000 #12000\n",
    "\n",
    "data_dict_t= {'Key':[], 'File':[]} #for custom audio testing\n",
    "mbp_dataset_t = create_dataset(n_fft, hop_length, before, after, keys_t, custom_audio=True)\n",
    "mbp_dataset_t"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d31b40b02c559d5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3ffae0790160aac2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a529fa2d",
   "metadata": {},
   "source": [
    "audio_samples_t = mbp_dataset_t['File'].values.tolist()\n",
    "labels_t = mbp_dataset_t['Key'].values.tolist()\n",
    "\n",
    "audioDataset_t = np.array(audio_samples_t, dtype = object)\n",
    "print(audio_samples_t[0].shape)\n",
    "mfcc_t = librosa.feature.mfcc(y=audio_samples_t[0], sr=44100) # shape: (n_mfcc, t)\n",
    "print(mfcc_t.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "feddd053",
   "metadata": {},
   "source": [
    "audio_samples_new_t = audio_samples_t.copy() # audio samples CNN\n",
    "\n",
    "for i, sample in enumerate(audio_samples_t):\n",
    "    audio_samples_new_t.append(time_shift(sample))\n",
    "    labels_t.append(labels_t[i])\n",
    "    \n",
    "# convert labels to a numpy array\n",
    "labels_t = np.array(labels_t)\n",
    "print(len(audio_samples_new_t))\n",
    "print(len(labels_t))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a5a0a118",
   "metadata": {},
   "source": [
    "audioDatasetFin_t, audioDatasetMfcc_t = [], []\n",
    "\n",
    "for i in range(len(audio_samples_new_t)):\n",
    "    transformed_sample_t = transform(audio_samples_new_t[i])\n",
    "    transformed_mfcc_t = transform_mfcc(audio_samples_new_t[i])\n",
    "    audioDatasetFin_t.append((transformed_sample_t, labels_t[i]))\n",
    "    audioDatasetMfcc_t.append((transformed_sample_t, transformed_mfcc_t, labels_t[i]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "189bbf34",
   "metadata": {},
   "source": [
    "#Using custom audio:\n",
    "# current random state to split the dataset\n",
    "random_state = 42\n",
    "\n",
    "# values for current run\n",
    "train_final_set, test_set = train_test_split(audioDatasetMfcc_t, test_size=0.2, random_state=random_state)\n",
    "num_epochs = 100\n",
    "main_architecture = \"CNN_LSTM\"\n",
    "currday = datetime.today().strftime('%Y-%m-%d')\n",
    "model_name = f\"model_multiclass_custom_audio_{num_epochs}_{main_architecture}_{currday}.pth\"\n",
    "description = \"2 layer CNN (32 and 64 output channels) with final 2 Dense Layers (512 and num_classes) result concatenated with \\n 2 LSTMs (hidden_size=32),  from mfcc with 2 Dense Layers (64 and 16) with a final Lazy Linear layer output of num_classes. \\n Using custom audio recorded for testing purposes. n_fft = 50\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "45734399",
   "metadata": {},
   "source": [
    "# Training part\n",
    "fold_stats = train_with_cross_validation(train_final_set, num_epochs, model_name, random_state=random_state)\n",
    "max_val = 0\n",
    "real_num_epochs = 0\n",
    "for fold_stat in fold_stats: #using folds instead of LOO\n",
    "    if fold_stat[1] > max_val:\n",
    "        max_val = fold_stat[1]\n",
    "        real_num_epochs = fold_stat[0]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "859fa190",
   "metadata": {},
   "source": [
    "# Prediction part\n",
    "prediction = predict_mfcc(test_set, model_name, device) #using the custom test_set\n",
    "labels_set = [t[2] for t in test_set]\n",
    "print(labels_set)\n",
    "print(prediction[0])\n",
    "final_labels_set = [keys_t_s[ind] for ind in labels_set]\n",
    "\n",
    "# Metrics calculation\n",
    "accuracy = accuracy_score(final_labels_set, prediction[0])\n",
    "precision = precision_score(final_labels_set, prediction[0], average='macro')\n",
    "recall = recall_score(final_labels_set, prediction[0], average='macro')\n",
    "f1 = sklearn.metrics.f1_score(final_labels_set, prediction[0], average='macro')\n",
    "\n",
    "# Save in csv file\n",
    "save_csv(model_name, real_num_epochs, description, accuracy, precision, recall, f1)\n",
    "\n",
    "# Print results\n",
    "print(\"Final Results!\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(description)\n",
    "print(f\"Epochs: {real_num_epochs}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
