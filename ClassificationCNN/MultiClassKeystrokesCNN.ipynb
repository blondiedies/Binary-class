{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T13:56:44.792629Z",
     "start_time": "2025-02-04T13:56:43.184766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose\n",
    "import random\n"
   ],
   "id": "2a122adee9a39c20",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:24:59.245309Z",
     "start_time": "2025-02-04T14:24:58.801648Z"
    }
   },
   "cell_type": "code",
   "source": "sample0, sr0 = librosa.load('../Dataset-for-Binary/base-audio/audio_0.wav')",
   "id": "bfacb9ffeaf90c7e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:00.437264Z",
     "start_time": "2025-02-04T14:25:00.407646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# waveform function for me to not bang my keyboard\n",
    "def disp_waveform(signal, sr=None, color='blue'):\n",
    "    plt.figure(figsize=(7,2))\n",
    "    return librosa.display.waveshow(signal, sr=sr, color=color)"
   ],
   "id": "9ad2373f64ed7deb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:00.941577Z",
     "start_time": "2025-02-04T14:25:00.935333Z"
    }
   },
   "source": [
    "def isolator(signal, sample_rate, n_fft, hop_length, before, after, threshold, show=False):\n",
    "    strokes = []\n",
    "    # -- signal'\n",
    "    if show:\n",
    "        disp_waveform(signal, sr=sample_rate)\n",
    "    fft = librosa.stft(signal, n_fft=n_fft, hop_length=hop_length)\n",
    "    energy = np.abs(np.sum(fft, axis=0)).astype(float)\n",
    "    # norm = np.linalg.norm(energy)\n",
    "    # energy = energy/norm\n",
    "    # -- energy'\n",
    "    if show:\n",
    "        disp_waveform(energy)\n",
    "    threshed = energy > threshold\n",
    "    # -- peaks'\n",
    "    if show:\n",
    "        disp_waveform(threshed.astype(float))\n",
    "    peaks = np.where(threshed == True)[0]\n",
    "    peak_count = len(peaks)\n",
    "    prev_end = sample_rate*0.1*(-1)\n",
    "    # '-- isolating keystrokes'\n",
    "    for i in range(peak_count):\n",
    "        this_peak = peaks[i]\n",
    "        timestamp = (this_peak*hop_length) + n_fft//2\n",
    "        if timestamp > prev_end + (0.1*sample_rate):\n",
    "            keystroke = signal[timestamp-before:timestamp+after]\n",
    "            # strokes.append(torch.tensor(keystroke)[None, :])\n",
    "            # keystroke = transform(keystroke)\n",
    "            strokes.append(keystroke)\n",
    "            if show:\n",
    "                disp_waveform(keystroke, sr=sample_rate)\n",
    "            prev_end = timestamp+after\n",
    "    return strokes"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:18.532410Z",
     "start_time": "2025-02-04T14:25:18.526668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Constants we actually need for the task\n",
    "MBP_AUDIO_DIR = '../Dataset-for-Binary/base-audio/'\n",
    "keys_s = '1234567890QWERTYUIOPASDFGHJKLZXCVBNM'\n",
    "# keys_s = '12'\n",
    "labels = list(keys_s)\n",
    "keys = ['audio_' + k + '.wav' for k in labels]\n",
    "data_dict = {'Key':[], 'File':[]}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ],
   "id": "3c33893a14fb819c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:18.932531Z",
     "start_time": "2025-02-04T14:25:18.929002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataset(n_fft, hop_length, before, after):\n",
    "    for i, File in enumerate(keys):\n",
    "        loc = MBP_AUDIO_DIR + File\n",
    "        samples, sr = librosa.load(loc)\n",
    "        prom = 0.06\n",
    "        step = 0.005\n",
    "        strokes = isolator(samples, sr, n_fft, hop_length, before, after, prom, False )\n",
    "        print(f'File {File} length: {len(strokes)}')\n",
    "        label = [labels[i]]*len(strokes)\n",
    "        data_dict['Key'] += label\n",
    "        data_dict['File'] += strokes\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for l in df['Key']:\n",
    "        if not l in mapper:\n",
    "            mapper[l] = counter\n",
    "            counter += 1\n",
    "    df.replace({'Key': mapper}, inplace = True)\n",
    "\n",
    "    return df"
   ],
   "id": "2fee322473e2bfe1",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:20.077452Z",
     "start_time": "2025-02-04T14:25:19.243409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key in keys_s:\n",
    "    sample, sr = librosa.load(f'../Dataset-for-Binary/base-audio/audio_{key}.wav')\n",
    "    print(len(isolator(sample, sr, 1024, 225, 2400, 12000, 0.06)), end=' ')\n",
    "    "
   ],
   "id": "953e2f0556453dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25 25 25 25 26 25 25 27 26 27 28 28 25 26 25 25 25 25 25 25 25 25 27 26 25 27 25 25 25 26 26 25 25 25 26 "
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:21.301755Z",
     "start_time": "2025-02-04T14:25:20.487082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_fft = 1024\n",
    "hop_length = 225\n",
    "before = 2400\n",
    "after = 12000\n",
    "mbp_dataset = create_dataset(n_fft, hop_length, before, after)\n",
    "mbp_dataset"
   ],
   "id": "f2dd6f724446d4fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File audio_1.wav length: 25\n",
      "File audio_2.wav length: 25\n",
      "File audio_3.wav length: 25\n",
      "File audio_4.wav length: 25\n",
      "File audio_5.wav length: 25\n",
      "File audio_6.wav length: 26\n",
      "File audio_7.wav length: 25\n",
      "File audio_8.wav length: 25\n",
      "File audio_9.wav length: 27\n",
      "File audio_0.wav length: 26\n",
      "File audio_Q.wav length: 27\n",
      "File audio_W.wav length: 28\n",
      "File audio_E.wav length: 28\n",
      "File audio_R.wav length: 25\n",
      "File audio_T.wav length: 26\n",
      "File audio_Y.wav length: 25\n",
      "File audio_U.wav length: 25\n",
      "File audio_I.wav length: 25\n",
      "File audio_O.wav length: 25\n",
      "File audio_P.wav length: 25\n",
      "File audio_A.wav length: 25\n",
      "File audio_S.wav length: 25\n",
      "File audio_D.wav length: 25\n",
      "File audio_F.wav length: 27\n",
      "File audio_G.wav length: 26\n",
      "File audio_H.wav length: 25\n",
      "File audio_J.wav length: 27\n",
      "File audio_K.wav length: 25\n",
      "File audio_L.wav length: 25\n",
      "File audio_Z.wav length: 25\n",
      "File audio_X.wav length: 26\n",
      "File audio_C.wav length: 26\n",
      "File audio_V.wav length: 25\n",
      "File audio_B.wav length: 25\n",
      "File audio_N.wav length: 25\n",
      "File audio_M.wav length: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/8bpx1vc91zq76n48xvq00vkr0000gn/T/ipykernel_41323/2996179166.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace({'Key': mapper}, inplace = True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     Key                                               File\n",
       "0      0  [-0.00017975704, -0.00012727435, -9.371067e-05...\n",
       "1      0  [0.0004975861, 0.00049031794, 0.00055128767, 0...\n",
       "2      0  [0.0003178973, 0.00034715654, 0.00037197635, 0...\n",
       "3      0  [0.0026817801, 0.0026667325, 0.0026979204, 0.0...\n",
       "4      0  [0.006475482, 0.006330982, 0.0053669773, 0.003...\n",
       "..   ...                                                ...\n",
       "916   35  [-0.250816, -0.25290227, -0.2548398, -0.256657...\n",
       "917   35  [0.13746458, 0.13331993, 0.12892574, 0.1242145...\n",
       "918   35  [0.0017171799, 0.0016756048, 0.0016776036, 0.0...\n",
       "919   35  [-0.00014814897, -0.00018149195, -0.0002237717...\n",
       "920   35  [0.0002516488, 0.00018340952, 0.00015876105, 0...\n",
       "\n",
       "[921 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.00017975704, -0.00012727435, -9.371067e-05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.0004975861, 0.00049031794, 0.00055128767, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.0003178973, 0.00034715654, 0.00037197635, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.0026817801, 0.0026667325, 0.0026979204, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.006475482, 0.006330982, 0.0053669773, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>35</td>\n",
       "      <td>[-0.250816, -0.25290227, -0.2548398, -0.256657...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>35</td>\n",
       "      <td>[0.13746458, 0.13331993, 0.12892574, 0.1242145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>35</td>\n",
       "      <td>[0.0017171799, 0.0016756048, 0.0016776036, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>35</td>\n",
       "      <td>[-0.00014814897, -0.00018149195, -0.0002237717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>35</td>\n",
       "      <td>[0.0002516488, 0.00018340952, 0.00015876105, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>921 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:23.746684Z",
     "start_time": "2025-02-04T14:25:23.744738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio_samples = mbp_dataset['File'].values.tolist()\n",
    "labels = mbp_dataset['Key'].values.tolist()\n",
    "\n",
    "audioDataset = np.array(audio_samples, dtype = object)\n",
    "# labels = np.array(labels)"
   ],
   "id": "c89e46a8b42021c0",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:24.312835Z",
     "start_time": "2025-02-04T14:25:24.309511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TimeShifting():\n",
    "    def __call__(self, samples):\n",
    "#       samples_shape = samples.shape\n",
    "        samples = samples.flatten()\n",
    "        \n",
    "        shift = int(len(samples) * 0.4) #Max shift (0.4)\n",
    "        random_shift = random.randint(0, shift) #Random number between 0 and 0.4*len(samples)\n",
    "        data_roll = np.roll(samples, random_shift)\n",
    "        return data_roll"
   ],
   "id": "39d2cd83eaab0466",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:25.037241Z",
     "start_time": "2025-02-04T14:25:25.034519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def time_shift(samples):\n",
    "    samples = samples.flatten()\n",
    "    shift = int(len(samples) * 0.4) #Max shift (0.4)\n",
    "    random_shift = random.randint(0, shift) #Random number between 0 and 0.4*len(samples)\n",
    "    data_roll = np.roll(samples, random_shift)\n",
    "    return data_roll"
   ],
   "id": "ac3598b8c7bcd285",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:25.486534Z",
     "start_time": "2025-02-04T14:25:25.428530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "class ToMelSpectrogram:\n",
    "    def __init__(self, audio_length=14400):\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        if len(samples) > self.audio_length:\n",
    "            samples = samples[:self.audio_length]\n",
    "        elif len(samples) < self.audio_length:\n",
    "            samples = np.pad(samples, (0, self.audio_length - len(samples)), mode='constant')\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=samples, sr=44100, n_mels=64, n_fft=1024, hop_length=225)\n",
    "        mel_spec_resized = resize(mel_spec, (64, 64), anti_aliasing=True)\n",
    "        mel_spec_resized = np.expand_dims(mel_spec_resized, axis=0)\n",
    "        return torch.tensor(mel_spec_resized)\n",
    "\n",
    "\n",
    "class ToMelSpectrogramMfcc:\n",
    "    def __init__(self, audio_length=14400):\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        if len(samples) > self.audio_length:\n",
    "            samples = samples[:self.audio_length]\n",
    "        elif len(samples) < self.audio_length:\n",
    "            samples = np.pad(samples, (0, self.audio_length - len(samples)), mode='constant')\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=samples, sr=44100, n_mels=64, n_fft=n_fft, hop_length=hop_length)\n",
    "        mel_spec = librosa.feature.mfcc(S=librosa.power_to_db(mel_spec))\n",
    "        mel_spec_resized = resize(mel_spec, (64, 64), anti_aliasing=True)\n",
    "        mel_spec_resized = np.expand_dims(mel_spec_resized, axis=0)\n",
    "\n",
    "        return torch.tensor(mel_spec_resized)\n"
   ],
   "id": "4a429af86b3206df",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:26.024028Z",
     "start_time": "2025-02-04T14:25:26.019745Z"
    }
   },
   "cell_type": "code",
   "source": "transform = Compose([ToMelSpectrogram()])",
   "id": "f019fc2bc0e25204",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:26.522659Z",
     "start_time": "2025-02-04T14:25:26.490775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio_samples_new = audio_samples.copy()\n",
    "\n",
    "for i, sample in enumerate(audio_samples):\n",
    "    audio_samples_new.append(time_shift(sample))\n",
    "    labels.append(labels[i])\n",
    "\n",
    "# convert labels to a numpy array\n",
    "labels = np.array(labels)\n",
    "print(len(audio_samples_new))\n",
    "print(len(labels))"
   ],
   "id": "44b3b3ca9f37f4c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842\n",
      "1842\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:31.992372Z",
     "start_time": "2025-02-04T14:25:26.931571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audioDatasetFin = []\n",
    "\n",
    "for i in range(len(audio_samples_new)):\n",
    "    transformed_sample = transform(audio_samples_new[i])\n",
    "    audioDatasetFin.append((transformed_sample, labels[i]))"
   ],
   "id": "be9e929216f37f07",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:32.030987Z",
     "start_time": "2025-02-04T14:25:31.999274Z"
    }
   },
   "cell_type": "code",
   "source": "len(audioDatasetFin)",
   "id": "94d1788d46fe597f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1842"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:32.037615Z",
     "start_time": "2025-02-04T14:25:32.032581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=36):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.LazyLinear(512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 14 * 14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "876758bd07bc26b7",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:25:53.615938Z",
     "start_time": "2025-02-04T14:25:53.608199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "def train(dataset, num_epochs, model_path, leave_one_out=False):\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "    \n",
    "    if leave_one_out:\n",
    "        train_set, val_set = dataset[:-1], [dataset[-1]]\n",
    "    else:\n",
    "        train_set, val_set = train_test_split(dataset, test_size=0.15)\n",
    "    train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=16, shuffle=True)\n",
    "    \n",
    "    model = CNN()\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.BCELoss()\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        correct_train = 0  # correct training examples\n",
    "        total_train = 0 # total training examples\n",
    "        tic = time.perf_counter()\n",
    "        \n",
    "        # print(f'---- EPOCH {epoch} ----')\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted_train = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted_train == labels).sum().item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        toc = time.perf_counter()\n",
    "        time_taken = toc - tic\n",
    "        \n",
    "        epoch_train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Evaluation of the model\n",
    "        model.eval()\n",
    "        total, correct = 0, 0\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # print(\"VALIDATION\")\n",
    "            # print(f'outputs: {outputs}')\n",
    "            # print(f'predicted: {predicted}')\n",
    "            # print(f'labels: {labels}')\n",
    "            \n",
    "        val_accuracy = correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} / Val Accuracy: {val_accuracy:.4f} Iter Time: {time_taken:.2f}s\")\n",
    "        \n",
    "        if epoch == num_epochs - 1 and epoch != 0:\n",
    "            plt.plot(range(epoch+1), train_accuracies, label='Training Accuracy', color=\"blue\")\n",
    "            plt.plot(range(epoch+1), val_accuracies, label='Validation Accuracy', color=\"orange\")\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title(f'Training vs Validation Accuracy')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        print()\n",
    "    torch.save(model.state_dict(), model_path)"
   ],
   "id": "9e1ab811f0d1abfe",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T14:26:13.467901Z",
     "start_time": "2025-02-04T14:26:11.004688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(audioDatasetFin[0][0].shape)\n",
    "\n",
    "train(audioDatasetFin, 200, \"model_multiclass_cnn_12_08_24.pth\", leave_one_out=True)"
   ],
   "id": "7407a641de41b299",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64])\n",
      "Epoch [1/200], Train Loss: 3.3674, Train Accuracy: 0.0809 / Val Accuracy: 0.0000 Iter Time: 1.34s\n",
      "\n",
      "Epoch [2/200], Train Loss: 2.6379, Train Accuracy: 0.2091 / Val Accuracy: 0.0000 Iter Time: 0.91s\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(audioDatasetFin[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m----> 3\u001B[0m train(audioDatasetFin, \u001B[38;5;241m200\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_multiclass_cnn_12_08_24.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m, leave_one_out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[24], line 37\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(dataset, num_epochs, model_path, leave_one_out)\u001B[0m\n\u001B[1;32m     35\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[1;32m     36\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m---> 37\u001B[0m epoch_train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m inputs\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     39\u001B[0m _, predicted_train \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs\u001B[38;5;241m.\u001B[39mdata, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     40\u001B[0m total_train \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7e64a35722ee3aef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
