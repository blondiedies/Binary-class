{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:20.023503Z",
     "start_time": "2024-08-19T19:13:18.002407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose\n",
    "import random\n"
   ],
   "id": "2a122adee9a39c20",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:20.027070Z",
     "start_time": "2024-08-19T19:13:20.024436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rnn = nn.LSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ],
   "id": "8dfb7e66099184ef",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:20.028585Z",
     "start_time": "2024-08-19T19:13:20.027476Z"
    }
   },
   "cell_type": "code",
   "source": "# sample0, sr0 = librosa.load('Keystroke-Datasets/MBPWavs/0.wav')",
   "id": "bfacb9ffeaf90c7e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:20.030683Z",
     "start_time": "2024-08-19T19:13:20.029105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# waveform function for me to not bang my keyboard\n",
    "def disp_waveform(signal, sr=None, color='blue'):\n",
    "    plt.figure(figsize=(7,2))\n",
    "    return librosa.display.waveshow(signal, sr=sr, color=color)"
   ],
   "id": "9ad2373f64ed7deb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:20.034181Z",
     "start_time": "2024-08-19T19:13:20.031828Z"
    }
   },
   "source": [
    "def isolator(signal, sample_rate, n_fft, hop_length, before, after, threshold, show=False):\n",
    "    strokes = []\n",
    "    # -- signal'\n",
    "    if show:\n",
    "        disp_waveform(signal, sr=sample_rate)\n",
    "    fft = librosa.stft(signal, n_fft=n_fft, hop_length=hop_length)\n",
    "    energy = np.abs(np.sum(fft, axis=0)).astype(float)\n",
    "    # norm = np.linalg.norm(energy)\n",
    "    # energy = energy/norm\n",
    "    # -- energy'\n",
    "    if show:\n",
    "        disp_waveform(energy)\n",
    "    threshed = energy > threshold\n",
    "    # -- peaks'\n",
    "    if show:\n",
    "        disp_waveform(threshed.astype(float))\n",
    "    peaks = np.where(threshed == True)[0]\n",
    "    peak_count = len(peaks)\n",
    "    prev_end = sample_rate*0.1*(-1)\n",
    "    # '-- isolating keystrokes'\n",
    "    for i in range(peak_count):\n",
    "        this_peak = peaks[i]\n",
    "        timestamp = (this_peak*hop_length) + n_fft//2\n",
    "        if timestamp > prev_end + (0.1*sample_rate):\n",
    "            keystroke = signal[timestamp-before:timestamp+after]\n",
    "            # strokes.append(torch.tensor(keystroke)[None, :])\n",
    "            # keystroke = transform(keystroke)\n",
    "            strokes.append(keystroke)\n",
    "            if show:\n",
    "                disp_waveform(keystroke, sr=sample_rate)\n",
    "            prev_end = timestamp+after\n",
    "    return strokes"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:20.052468Z",
     "start_time": "2024-08-19T19:13:20.034640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Constants we actually need for the task\n",
    "MBP_AUDIO_DIR = '../Dataset-for-Binary/base-audio/'\n",
    "keys_s = '1234567890QWERTYUIOPASDFGHJKLZXCVBNM'\n",
    "# keys_s = '12'\n",
    "labels = list(keys_s)\n",
    "keys = ['audio_' + k + '.wav' for k in labels]\n",
    "data_dict = {'Key':[], 'File':[]}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ],
   "id": "3c33893a14fb819c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:20.687325Z",
     "start_time": "2024-08-19T19:13:20.683860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataset(n_fft, hop_length, before, after):\n",
    "    for i, File in enumerate(keys):\n",
    "        loc = MBP_AUDIO_DIR + File\n",
    "        samples, sr = librosa.load(loc)\n",
    "        prom = 0.06\n",
    "        step = 0.005\n",
    "        strokes = isolator(samples, sr, n_fft, hop_length, before, after, prom, False )\n",
    "        print(f'File {File} length: {len(strokes)}')\n",
    "        label = [labels[i]]*len(strokes)\n",
    "        data_dict['Key'] += label\n",
    "        data_dict['File'] += strokes\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for l in df['Key']:\n",
    "        if not l in mapper:\n",
    "            mapper[l] = counter\n",
    "            counter += 1\n",
    "    df.replace({'Key': mapper}, inplace = True)\n",
    "\n",
    "    return df"
   ],
   "id": "2fee322473e2bfe1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:22.459152Z",
     "start_time": "2024-08-19T19:13:21.215589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key in keys_s:\n",
    "    sample, sr = librosa.load(f'../Dataset-for-Binary/base-audio/audio_{key}.wav')\n",
    "    print(sr)\n",
    "    print(len(isolator(sample, sr, 1024, 225, 2400, 12000, 0.06)), end=' ')\n",
    "    "
   ],
   "id": "953e2f0556453dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "26 22050\n",
      "25 22050\n",
      "25 22050\n",
      "27 22050\n",
      "26 22050\n",
      "27 22050\n",
      "28 22050\n",
      "28 22050\n",
      "25 22050\n",
      "26 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "27 22050\n",
      "26 22050\n",
      "25 22050\n",
      "27 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "26 22050\n",
      "26 22050\n",
      "25 22050\n",
      "25 22050\n",
      "25 22050\n",
      "26 "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:23.683632Z",
     "start_time": "2024-08-19T19:13:23.064298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_fft = 1024\n",
    "hop_length = 225\n",
    "before = 2400\n",
    "after = 12000\n",
    "mbp_dataset = create_dataset(n_fft, hop_length, before, after)\n",
    "mbp_dataset"
   ],
   "id": "f2dd6f724446d4fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File audio_1.wav length: 25\n",
      "File audio_2.wav length: 25\n",
      "File audio_3.wav length: 25\n",
      "File audio_4.wav length: 25\n",
      "File audio_5.wav length: 25\n",
      "File audio_6.wav length: 26\n",
      "File audio_7.wav length: 25\n",
      "File audio_8.wav length: 25\n",
      "File audio_9.wav length: 27\n",
      "File audio_0.wav length: 26\n",
      "File audio_Q.wav length: 27\n",
      "File audio_W.wav length: 28\n",
      "File audio_E.wav length: 28\n",
      "File audio_R.wav length: 25\n",
      "File audio_T.wav length: 26\n",
      "File audio_Y.wav length: 25\n",
      "File audio_U.wav length: 25\n",
      "File audio_I.wav length: 25\n",
      "File audio_O.wav length: 25\n",
      "File audio_P.wav length: 25\n",
      "File audio_A.wav length: 25\n",
      "File audio_S.wav length: 25\n",
      "File audio_D.wav length: 25\n",
      "File audio_F.wav length: 27\n",
      "File audio_G.wav length: 26\n",
      "File audio_H.wav length: 25\n",
      "File audio_J.wav length: 27\n",
      "File audio_K.wav length: 25\n",
      "File audio_L.wav length: 25\n",
      "File audio_Z.wav length: 25\n",
      "File audio_X.wav length: 26\n",
      "File audio_C.wav length: 26\n",
      "File audio_V.wav length: 25\n",
      "File audio_B.wav length: 25\n",
      "File audio_N.wav length: 25\n",
      "File audio_M.wav length: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/8bpx1vc91zq76n48xvq00vkr0000gn/T/ipykernel_41610/2996179166.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace({'Key': mapper}, inplace = True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     Key                                               File\n",
       "0      0  [-0.00017975704, -0.00012727435, -9.371067e-05...\n",
       "1      0  [0.0004975861, 0.00049031794, 0.00055128767, 0...\n",
       "2      0  [0.0003178973, 0.00034715654, 0.00037197635, 0...\n",
       "3      0  [0.0026817801, 0.0026667325, 0.0026979204, 0.0...\n",
       "4      0  [0.006475482, 0.006330982, 0.0053669773, 0.003...\n",
       "..   ...                                                ...\n",
       "916   35  [-0.250816, -0.25290227, -0.2548398, -0.256657...\n",
       "917   35  [0.13746458, 0.13331993, 0.12892574, 0.1242145...\n",
       "918   35  [0.0017171799, 0.0016756048, 0.0016776036, 0.0...\n",
       "919   35  [-0.00014814897, -0.00018149195, -0.0002237717...\n",
       "920   35  [0.0002516488, 0.00018340952, 0.00015876105, 0...\n",
       "\n",
       "[921 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.00017975704, -0.00012727435, -9.371067e-05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.0004975861, 0.00049031794, 0.00055128767, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.0003178973, 0.00034715654, 0.00037197635, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.0026817801, 0.0026667325, 0.0026979204, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.006475482, 0.006330982, 0.0053669773, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>35</td>\n",
       "      <td>[-0.250816, -0.25290227, -0.2548398, -0.256657...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>35</td>\n",
       "      <td>[0.13746458, 0.13331993, 0.12892574, 0.1242145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>35</td>\n",
       "      <td>[0.0017171799, 0.0016756048, 0.0016776036, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>35</td>\n",
       "      <td>[-0.00014814897, -0.00018149195, -0.0002237717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>35</td>\n",
       "      <td>[0.0002516488, 0.00018340952, 0.00015876105, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>921 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:25.836956Z",
     "start_time": "2024-08-19T19:13:25.814495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio_samples = mbp_dataset['File'].values.tolist()\n",
    "labels = mbp_dataset['Key'].values.tolist()\n",
    "\n",
    "audioDataset = np.array(audio_samples, dtype = object)\n",
    "print(audio_samples[0].shape)\n",
    "mfcc = librosa.feature.mfcc(y=audio_samples[0], sr=44100) # shape: (n_mfcc, t)\n",
    "print(mfcc.shape)\n",
    "# labels = np.array(labels)"
   ],
   "id": "c89e46a8b42021c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14400,)\n",
      "(20, 29)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:26.658428Z",
     "start_time": "2024-08-19T19:13:26.655366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TimeShifting():\n",
    "    def __call__(self, samples):\n",
    "#       samples_shape = samples.shape\n",
    "        samples = samples.flatten()\n",
    "        \n",
    "        shift = int(len(samples) * 0.4) #Max shift (0.4)\n",
    "        random_shift = random.randint(0, shift) #Random number between 0 and 0.4*len(samples)\n",
    "        data_roll = np.roll(samples, random_shift)\n",
    "        return data_roll"
   ],
   "id": "39d2cd83eaab0466",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:27.605351Z",
     "start_time": "2024-08-19T19:13:27.602293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def time_shift(samples):\n",
    "    samples = samples.flatten()\n",
    "    shift = int(len(samples) * 0.4) #Max shift (0.4)\n",
    "    random_shift = random.randint(0, shift) #Random number between 0 and 0.4*len(samples)\n",
    "    data_roll = np.roll(samples, random_shift)\n",
    "    return data_roll"
   ],
   "id": "ac3598b8c7bcd285",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:28.485735Z",
     "start_time": "2024-08-19T19:13:28.447637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "class ToMelSpectrogram:\n",
    "    def __init__(self, audio_length=14400):\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        if len(samples) > self.audio_length:\n",
    "            samples = samples[:self.audio_length]\n",
    "        elif len(samples) < self.audio_length:\n",
    "            samples = np.pad(samples, (0, self.audio_length - len(samples)), mode='constant')\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=samples, sr=44100, n_mels=64, n_fft=1024, hop_length=225)\n",
    "        mel_spec_resized = resize(mel_spec, (64, 64), anti_aliasing=True)\n",
    "        mel_spec_resized = np.expand_dims(mel_spec_resized, axis=0)\n",
    "        return torch.tensor(mel_spec_resized)\n",
    "\n",
    "\n",
    "class ToMelSpectrogramMfcc:\n",
    "    def __init__(self, audio_length=14400):\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        if len(samples) > self.audio_length:\n",
    "            samples = samples[:self.audio_length]\n",
    "        elif len(samples) < self.audio_length:\n",
    "            samples = np.pad(samples, (0, self.audio_length - len(samples)), mode='constant')\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=samples, sr=44100, n_mels=64, n_fft=n_fft, hop_length=hop_length)\n",
    "        mel_spec = librosa.feature.mfcc(S=librosa.power_to_db(mel_spec))\n",
    "        mel_spec_resized = resize(mel_spec, (64, 64), anti_aliasing=True)\n",
    "        mel_spec_resized = np.expand_dims(mel_spec_resized, axis=0)\n",
    "\n",
    "        return torch.tensor(mel_spec_resized)\n",
    "\n",
    "\n",
    "class ToMfcc:\n",
    "    def __init__(self, audio_length=14400):\n",
    "        self.audio_length = audio_length\n",
    "\n",
    "    def __call__(self, samples):\n",
    "        if len(samples) > self.audio_length:\n",
    "            samples = samples[:self.audio_length]\n",
    "        elif len(samples) < self.audio_length:\n",
    "            samples = np.pad(samples, (0, self.audio_length - len(samples)), mode='constant')\n",
    "        \n",
    "        mfcc_spec = librosa.feature.mfcc(y=samples, sr=44100)\n",
    "        mfcc_spec = np.transpose(mfcc_spec)\n",
    "        return torch.tensor(mfcc_spec)\n"
   ],
   "id": "4a429af86b3206df",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:29.207293Z",
     "start_time": "2024-08-19T19:13:29.205218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = Compose([ToMelSpectrogram()])\n",
    "transform_mfcc = Compose([ToMfcc()])"
   ],
   "id": "f019fc2bc0e25204",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:30.001603Z",
     "start_time": "2024-08-19T19:13:29.980568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audio_samples_new = audio_samples.copy() # audio samples CNN\n",
    "\n",
    "for i, sample in enumerate(audio_samples):\n",
    "    audio_samples_new.append(time_shift(sample))\n",
    "    labels.append(labels[i])\n",
    "    \n",
    "# convert labels to a numpy array\n",
    "labels = np.array(labels)\n",
    "print(len(audio_samples_new))\n",
    "print(len(labels))"
   ],
   "id": "44b3b3ca9f37f4c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842\n",
      "1842\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:41.416895Z",
     "start_time": "2024-08-19T19:13:30.940121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "audioDatasetFin, audioDatasetMfcc = [], []\n",
    "\n",
    "for i in range(len(audio_samples_new)):\n",
    "    transformed_sample = transform(audio_samples_new[i])\n",
    "    transformed_mfcc = transform_mfcc(audio_samples_new[i])\n",
    "    audioDatasetFin.append((transformed_sample, labels[i]))\n",
    "    audioDatasetMfcc.append((transformed_sample, transformed_mfcc, labels[i]))"
   ],
   "id": "be9e929216f37f07",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:41.483847Z",
     "start_time": "2024-08-19T19:13:41.428740Z"
    }
   },
   "cell_type": "code",
   "source": "len(audioDatasetFin)",
   "id": "94d1788d46fe597f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1842"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:41.510789Z",
     "start_time": "2024-08-19T19:13:41.487169Z"
    }
   },
   "cell_type": "code",
   "source": "audioDatasetMfcc[0][0].shape",
   "id": "cf735153a104afec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:41.537700Z",
     "start_time": "2024-08-19T19:13:41.522097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "class MfccLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.2, num_classes=36):\n",
    "        super(MfccLSTM, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.LazyLinear(64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        \n",
    "        # self.lstm = nn.Sequential(\n",
    "        #     nn.LSTM(input_size, hidden_size, batch_first=True),\n",
    "        #     nn.Dropout(dropout),\n",
    "        #     nn.LSTM(hidden_size, hidden_size, batch_first=True),\n",
    "        #     nn.Dropout(dropout),\n",
    "        #     nn.LazyLinear(256),\n",
    "        #     nn.Linear(256, output_size)\n",
    "        # )\n",
    "        \n",
    "        self.fc3 = nn.LazyLinear(128)\n",
    "        self.final_lstm = nn.LSTM(1, 64, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.LazyLinear(num_classes)\n",
    "    \n",
    "    def forward(self, image_input, sequence_input):\n",
    "        # must return shape (batch_size, num_classes) \n",
    "        # batch_size: right now is 16\n",
    "        # num_classes: right now is 36\n",
    "        x1 = self.conv(image_input)\n",
    "        # print(f'output of convolutional part: {x1.shape[1:]}')\n",
    "        # print(\"after conv layer1\")\n",
    "        # print('got here x1')\n",
    "        # must return shape (batch_size, output_size)\n",
    "        # output_size: should be 36\n",
    "        # # # print(sequence_input.shape)\n",
    "        out1, _ = self.lstm(sequence_input)\n",
    "        out1_dp = self.dropout(out1)\n",
    "        # print(f'output of first lstm: {out1_dp.shape[1:]}')\n",
    "        out2, _ = self.lstm2(out1_dp[:, -1, :])\n",
    "        out2_dp = self.dropout(out2)\n",
    "        # print(f'output of second lstm: {out2_dp.shape[1:]}')\n",
    "        x2 = self.fc2(self.fc1(out2_dp))\n",
    "        x3 = torch.cat((x1, x2), 1)\n",
    "        # print(f'output of concatenation: {x3.shape[1:]}')\n",
    "        # x4 = self.fc3(x3)\n",
    "        # final_out, _ = self.final_lstm(torch.unsqueeze(x4, dim=2))\n",
    "        # print(f'output of final lstm: {final_out[:, -1, :].shape[1:]}')\n",
    "        # x = self.fc(final_out[:, -1, :])\n",
    "        # print(f'output of final linear layer: {x.shape[1:]}')\n",
    "        # print()\n",
    "        x = self.fc(x3)\n",
    "        return x\n",
    "    "
   ],
   "id": "21c53f1c392a2eb2",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:41.543721Z",
     "start_time": "2024-08-19T19:13:41.539593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=36):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.LazyLinear(512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 14 * 14)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "876758bd07bc26b7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:13:41.555122Z",
     "start_time": "2024-08-19T19:13:41.545159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "def train(dataset, num_epochs, model_path, leave_one_out=False):\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "    \n",
    "    if leave_one_out:\n",
    "        train_set, val_set = dataset[:-1], [dataset[-1]]\n",
    "    else:\n",
    "        train_set, val_set = train_test_split(dataset, test_size=0.15)\n",
    "    train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=16, shuffle=True)\n",
    "    \n",
    "    # print(\"Before initialization of the model\")\n",
    "    model = MfccLSTM(input_size=20, hidden_size=32, num_classes=36, output_size=64)\n",
    "    # print(\"After initialization of the model\")\n",
    "    model = model.to(device)\n",
    "    # print(\"Model to GPU\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.BCELoss()\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        correct_train = 0  # correct training examples\n",
    "        total_train = 0 # total training examples\n",
    "        tic = time.perf_counter()\n",
    "        \n",
    "        # print(f'---- EPOCH {epoch} ----')\n",
    "        # print(\"Before entering the inner loop\")\n",
    "        for images, sequences, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, sequences)\n",
    "            # time.sleep(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_train_loss += loss.item() * images.size(0)\n",
    "\n",
    "            _, predicted_train = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted_train == labels).sum().item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        toc = time.perf_counter()\n",
    "        time_taken = toc - tic\n",
    "        \n",
    "        epoch_train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Evaluation of the model\n",
    "        model.eval()\n",
    "        total, correct = 0, 0\n",
    "        for images, sequences, labels  in val_loader:\n",
    "            images = images.to(device)\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images, sequences)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = correct / total\n",
    "        print(f'correct: {correct}, total: {total}')\n",
    "        print(f'val_accuracy: {val_accuracy:.4f}')\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} / Val Accuracy: {val_accuracy:.4f} Iter Time: {time_taken:.2f}s\")\n",
    "        \n",
    "        if epoch == num_epochs - 1 and epoch != 0:\n",
    "            plt.plot(range(epoch+1), train_accuracies, label='Training Accuracy', color=\"blue\")\n",
    "            plt.plot(range(epoch+1), val_accuracies, label='Validation Accuracy', color=\"orange\")\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title(f'Training vs Validation Accuracy')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        print()\n",
    "    torch.save(model.state_dict(), model_path)"
   ],
   "id": "9e1ab811f0d1abfe",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:14:45.089372Z",
     "start_time": "2024-08-19T19:13:44.462936Z"
    }
   },
   "cell_type": "code",
   "source": "train(audioDatasetMfcc, 500, \"model_multiclass_200_cnnlstm_finallstm_19_08_24.pth\", leave_one_out=True)",
   "id": "7407a641de41b299",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 0, total: 1\n",
      "val_accuracy: 0.0000\n",
      "Epoch [1/500], Train Loss: 3.4188, Train Accuracy: 0.0641 / Val Accuracy: 0.0000 Iter Time: 6.47s\n",
      "\n",
      "correct: 0, total: 1\n",
      "val_accuracy: 0.0000\n",
      "Epoch [2/500], Train Loss: 2.7472, Train Accuracy: 0.1999 / Val Accuracy: 0.0000 Iter Time: 1.13s\n",
      "\n",
      "correct: 0, total: 1\n",
      "val_accuracy: 0.0000\n",
      "Epoch [3/500], Train Loss: 2.2076, Train Accuracy: 0.3292 / Val Accuracy: 0.0000 Iter Time: 1.12s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [4/500], Train Loss: 1.8928, Train Accuracy: 0.4014 / Val Accuracy: 1.0000 Iter Time: 1.12s\n",
      "\n",
      "correct: 0, total: 1\n",
      "val_accuracy: 0.0000\n",
      "Epoch [5/500], Train Loss: 1.5944, Train Accuracy: 0.5084 / Val Accuracy: 0.0000 Iter Time: 1.24s\n",
      "\n",
      "correct: 0, total: 1\n",
      "val_accuracy: 0.0000\n",
      "Epoch [6/500], Train Loss: 1.3415, Train Accuracy: 0.5714 / Val Accuracy: 0.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 0, total: 1\n",
      "val_accuracy: 0.0000\n",
      "Epoch [7/500], Train Loss: 1.0964, Train Accuracy: 0.6583 / Val Accuracy: 0.0000 Iter Time: 1.28s\n",
      "\n",
      "correct: 0, total: 1\n",
      "val_accuracy: 0.0000\n",
      "Epoch [8/500], Train Loss: 0.9541, Train Accuracy: 0.6920 / Val Accuracy: 0.0000 Iter Time: 1.38s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [9/500], Train Loss: 0.8299, Train Accuracy: 0.7409 / Val Accuracy: 1.0000 Iter Time: 1.19s\n",
      "\n",
      "correct: 0, total: 1\n",
      "val_accuracy: 0.0000\n",
      "Epoch [10/500], Train Loss: 0.7707, Train Accuracy: 0.7512 / Val Accuracy: 0.0000 Iter Time: 1.13s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [11/500], Train Loss: 0.6783, Train Accuracy: 0.7898 / Val Accuracy: 1.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [12/500], Train Loss: 0.5656, Train Accuracy: 0.8153 / Val Accuracy: 1.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 0, total: 1\n",
      "val_accuracy: 0.0000\n",
      "Epoch [13/500], Train Loss: 0.5630, Train Accuracy: 0.8218 / Val Accuracy: 0.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [14/500], Train Loss: 0.5657, Train Accuracy: 0.8142 / Val Accuracy: 1.0000 Iter Time: 1.13s\n",
      "\n",
      "correct: 0, total: 1\n",
      "val_accuracy: 0.0000\n",
      "Epoch [15/500], Train Loss: 0.4570, Train Accuracy: 0.8544 / Val Accuracy: 0.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [16/500], Train Loss: 0.4093, Train Accuracy: 0.8707 / Val Accuracy: 1.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [17/500], Train Loss: 0.3878, Train Accuracy: 0.8691 / Val Accuracy: 1.0000 Iter Time: 1.17s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [18/500], Train Loss: 0.3906, Train Accuracy: 0.8778 / Val Accuracy: 1.0000 Iter Time: 1.15s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [19/500], Train Loss: 0.3396, Train Accuracy: 0.8979 / Val Accuracy: 1.0000 Iter Time: 1.15s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [20/500], Train Loss: 0.3145, Train Accuracy: 0.9082 / Val Accuracy: 1.0000 Iter Time: 1.24s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [21/500], Train Loss: 0.2501, Train Accuracy: 0.9131 / Val Accuracy: 1.0000 Iter Time: 1.16s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [22/500], Train Loss: 0.3474, Train Accuracy: 0.8946 / Val Accuracy: 1.0000 Iter Time: 1.25s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [23/500], Train Loss: 0.5200, Train Accuracy: 0.8571 / Val Accuracy: 1.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [24/500], Train Loss: 0.2302, Train Accuracy: 0.9402 / Val Accuracy: 1.0000 Iter Time: 1.20s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [25/500], Train Loss: 0.3486, Train Accuracy: 0.9022 / Val Accuracy: 1.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 0, total: 1\n",
      "val_accuracy: 0.0000\n",
      "Epoch [26/500], Train Loss: 0.2565, Train Accuracy: 0.9229 / Val Accuracy: 0.0000 Iter Time: 1.15s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [27/500], Train Loss: 0.1792, Train Accuracy: 0.9468 / Val Accuracy: 1.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [28/500], Train Loss: 0.2014, Train Accuracy: 0.9441 / Val Accuracy: 1.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [29/500], Train Loss: 0.1470, Train Accuracy: 0.9538 / Val Accuracy: 1.0000 Iter Time: 1.16s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [30/500], Train Loss: 0.1521, Train Accuracy: 0.9565 / Val Accuracy: 1.0000 Iter Time: 1.18s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [31/500], Train Loss: 0.1338, Train Accuracy: 0.9614 / Val Accuracy: 1.0000 Iter Time: 1.18s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [32/500], Train Loss: 0.1672, Train Accuracy: 0.9544 / Val Accuracy: 1.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [33/500], Train Loss: 0.1889, Train Accuracy: 0.9408 / Val Accuracy: 1.0000 Iter Time: 1.16s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [34/500], Train Loss: 0.1494, Train Accuracy: 0.9571 / Val Accuracy: 1.0000 Iter Time: 1.15s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [35/500], Train Loss: 0.1106, Train Accuracy: 0.9696 / Val Accuracy: 1.0000 Iter Time: 1.20s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [36/500], Train Loss: 0.2038, Train Accuracy: 0.9397 / Val Accuracy: 1.0000 Iter Time: 1.15s\n",
      "\n",
      "correct: 0, total: 1\n",
      "val_accuracy: 0.0000\n",
      "Epoch [37/500], Train Loss: 0.1331, Train Accuracy: 0.9620 / Val Accuracy: 0.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [38/500], Train Loss: 0.2065, Train Accuracy: 0.9468 / Val Accuracy: 1.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [39/500], Train Loss: 0.1288, Train Accuracy: 0.9631 / Val Accuracy: 1.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [40/500], Train Loss: 0.1350, Train Accuracy: 0.9598 / Val Accuracy: 1.0000 Iter Time: 1.14s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [41/500], Train Loss: 0.1270, Train Accuracy: 0.9690 / Val Accuracy: 1.0000 Iter Time: 1.13s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [42/500], Train Loss: 0.1326, Train Accuracy: 0.9663 / Val Accuracy: 1.0000 Iter Time: 1.15s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [43/500], Train Loss: 0.1010, Train Accuracy: 0.9734 / Val Accuracy: 1.0000 Iter Time: 1.24s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [44/500], Train Loss: 0.0717, Train Accuracy: 0.9794 / Val Accuracy: 1.0000 Iter Time: 1.20s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [45/500], Train Loss: 0.0539, Train Accuracy: 0.9870 / Val Accuracy: 1.0000 Iter Time: 1.33s\n",
      "\n",
      "correct: 1, total: 1\n",
      "val_accuracy: 1.0000\n",
      "Epoch [46/500], Train Loss: 0.0803, Train Accuracy: 0.9766 / Val Accuracy: 1.0000 Iter Time: 1.17s\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train(audioDatasetMfcc, \u001B[38;5;241m500\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_multiclass_200_cnnlstm_finallstm_19_08_24.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m, leave_one_out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[21], line 52\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(dataset, num_epochs, model_path, leave_one_out)\u001B[0m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n\u001B[1;32m     51\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 52\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     54\u001B[0m toc \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     55\u001B[0m time_taken \u001B[38;5;241m=\u001B[39m toc \u001B[38;5;241m-\u001B[39m tic\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    381\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    382\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    383\u001B[0m             )\n\u001B[0;32m--> 385\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    388\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:166\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    155\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    157\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    158\u001B[0m         group,\n\u001B[1;32m    159\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    163\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    164\u001B[0m         state_steps)\n\u001B[0;32m--> 166\u001B[0m     adam(\n\u001B[1;32m    167\u001B[0m         params_with_grad,\n\u001B[1;32m    168\u001B[0m         grads,\n\u001B[1;32m    169\u001B[0m         exp_avgs,\n\u001B[1;32m    170\u001B[0m         exp_avg_sqs,\n\u001B[1;32m    171\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    172\u001B[0m         state_steps,\n\u001B[1;32m    173\u001B[0m         amsgrad\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mamsgrad\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    174\u001B[0m         has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    175\u001B[0m         beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    176\u001B[0m         beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    177\u001B[0m         lr\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    178\u001B[0m         weight_decay\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    179\u001B[0m         eps\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    180\u001B[0m         maximize\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    181\u001B[0m         foreach\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mforeach\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    182\u001B[0m         capturable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    183\u001B[0m         differentiable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    184\u001B[0m         fused\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfused\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    185\u001B[0m         grad_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_scale\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    186\u001B[0m         found_inf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    187\u001B[0m     )\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:316\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    314\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 316\u001B[0m func(params,\n\u001B[1;32m    317\u001B[0m      grads,\n\u001B[1;32m    318\u001B[0m      exp_avgs,\n\u001B[1;32m    319\u001B[0m      exp_avg_sqs,\n\u001B[1;32m    320\u001B[0m      max_exp_avg_sqs,\n\u001B[1;32m    321\u001B[0m      state_steps,\n\u001B[1;32m    322\u001B[0m      amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[1;32m    323\u001B[0m      has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    324\u001B[0m      beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    325\u001B[0m      beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    326\u001B[0m      lr\u001B[38;5;241m=\u001B[39mlr,\n\u001B[1;32m    327\u001B[0m      weight_decay\u001B[38;5;241m=\u001B[39mweight_decay,\n\u001B[1;32m    328\u001B[0m      eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m    329\u001B[0m      maximize\u001B[38;5;241m=\u001B[39mmaximize,\n\u001B[1;32m    330\u001B[0m      capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[1;32m    331\u001B[0m      differentiable\u001B[38;5;241m=\u001B[39mdifferentiable,\n\u001B[1;32m    332\u001B[0m      grad_scale\u001B[38;5;241m=\u001B[39mgrad_scale,\n\u001B[1;32m    333\u001B[0m      found_inf\u001B[38;5;241m=\u001B[39mfound_inf)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:439\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    437\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (max_exp_avg_sqs[i]\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 439\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (exp_avg_sq\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    441\u001B[0m     param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n\u001B[1;32m    443\u001B[0m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "20fb3c1c29961f85",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
